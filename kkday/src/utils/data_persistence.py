"""
KKDAY ë°ì´í„° ì˜ì†ì„± ê´€ë¦¬ ì‹œìŠ¤í…œ (KLOOK ë°©ì‹ ì ìš©)
- URL ìˆ˜ì§‘ ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ ì €ì¥
- 2ë‹¨ê³„ ë¶„ë¦¬ ì‹¤í–‰ ìƒíƒœ ì¶”ì 
- ë©”íƒ€ë°ì´í„° ë° í†µê³„ ì •ë³´ ê´€ë¦¬
"""

import json
import os
from datetime import datetime
from typing import Dict, List, Any, Optional
import hashlib

from ..config import get_city_code, get_city_info

class KKdayDataPersistence:
    """KKDAY ë°ì´í„° ì˜ì†ì„± ê´€ë¦¬ í´ë˜ìŠ¤ (KLOOK ë°©ì‹ êµ¬ì¡°)"""

    def __init__(self, base_dir: str = None):
        if base_dir is None:
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
            current_file = os.path.abspath(__file__)
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_file)))
            self.base_dir = project_root
        else:
            self.base_dir = base_dir

        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.base_dir, exist_ok=True)

    def save_url_collection_data(self, city_name: str, tab: str, url_data: List[Dict],
                                collection_info: Dict) -> str:
        """
        URL ìˆ˜ì§‘ ë°ì´í„°ë¥¼ KLOOK ë°©ì‹ JSON í˜•íƒœë¡œ ì €ì¥

        Args:
            city_name: ë„ì‹œëª… (ì˜ˆ: "ì‚¿í¬ë¡œ")
            tab: íƒ­ êµ¬ë¶„ (ì˜ˆ: "ì „ì²´", "íˆ¬ì–´", "ì•¡í‹°ë¹„í‹°")
            url_data: ìˆ˜ì§‘ëœ URL ë¦¬ìŠ¤íŠ¸
            collection_info: ìˆ˜ì§‘ ë©”íƒ€ë°ì´í„°

        Returns:
            str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """

        # íŒŒì¼ëª… ìƒì„± (KLOOK ë°©ì‹)
        filename = f"kkday_urls_data_{city_name}_{tab}.json"
        filepath = os.path.join(self.base_dir, filename)

        # KLOOK ìŠ¤íƒ€ì¼ ë°ì´í„° êµ¬ì¡°
        data_structure = {
            "collection_info": {
                "city": city_name,
                "tab": tab,
                "timestamp": datetime.now().isoformat(),
                "target_products": collection_info.get("target_products", len(url_data)),
                "max_pages": collection_info.get("max_pages", 10),
                "platform": "kkday"
            },
            "url_rank_mapping": [],
            "collection_stats": {
                "total_urls_found": len(url_data),
                "total_pages_processed": collection_info.get("pages_processed", 0),
                "collection_success": True,
                "duplicate_count": 0,
                "new_count": len(url_data)
            }
        }

        # URL ë°ì´í„° êµ¬ì¡°í™” (KLOOK ë°©ì‹)
        duplicate_count = 0
        for i, url_info in enumerate(url_data, 1):
            if isinstance(url_info, str):
                # ë‹¨ìˆœ URL ë¬¸ìì—´ì¸ ê²½ìš°
                url_entry = {
                    "rank": i,
                    "url": url_info,
                    "page": 1,
                    "page_index": i,
                    "collected_at": datetime.now().isoformat(),
                    "is_duplicate": False
                }
            elif isinstance(url_info, dict):
                # ì´ë¯¸ êµ¬ì¡°í™”ëœ ë°ì´í„°ì¸ ê²½ìš°
                url_entry = {
                    "rank": url_info.get("rank", i),
                    "url": url_info.get("url", ""),
                    "page": url_info.get("page", 1),
                    "page_index": url_info.get("page_index", i),
                    "collected_at": url_info.get("collected_at", datetime.now().isoformat()),
                    "is_duplicate": url_info.get("is_duplicate", False)
                }

                if url_entry["is_duplicate"]:
                    duplicate_count += 1

            data_structure["url_rank_mapping"].append(url_entry)

        # í†µê³„ ì—…ë°ì´íŠ¸
        data_structure["collection_stats"]["duplicate_count"] = duplicate_count
        data_structure["collection_stats"]["new_count"] = len(url_data) - duplicate_count

        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data_structure, f, ensure_ascii=False, indent=2)

        print(f"âœ… KKDAY URL ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}")
        print(f"   ğŸ“Š ì´ {len(url_data)}ê°œ URL, ì¤‘ë³µ {duplicate_count}ê°œ, ì‹ ê·œ {len(url_data) - duplicate_count}ê°œ")

        return filepath

    def save_status_data(self, city_name: str, tab: str, stage1_data: Dict = None,
                        stage2_data: Dict = None) -> str:
        """
        ì‹¤í–‰ ìƒíƒœ ë°ì´í„°ë¥¼ KLOOK ë°©ì‹ìœ¼ë¡œ ì €ì¥

        Args:
            city_name: ë„ì‹œëª…
            tab: íƒ­ êµ¬ë¶„
            stage1_data: Stage 1 ì‹¤í–‰ ê²°ê³¼
            stage2_data: Stage 2 ì‹¤í–‰ ê²°ê³¼

        Returns:
            str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """

        filename = f"kkday_status_{city_name}_{tab}.json"
        filepath = os.path.join(self.base_dir, filename)

        # ê¸°ì¡´ ìƒíƒœ ë°ì´í„° ë¡œë“œ (ìˆë‹¤ë©´)
        status_data = {}
        if os.path.exists(filepath):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    status_data = json.load(f)
            except:
                pass

        # KLOOK ìŠ¤íƒ€ì¼ ìƒíƒœ êµ¬ì¡°
        if not status_data:
            status_data = {
                "city": city_name,
                "tab": tab,
                "platform": "kkday",
                "stage1": {
                    "status": "pending",
                    "timestamp": None,
                    "data": None
                },
                "stage2": {
                    "status": "pending",
                    "timestamp": None,
                    "data": None
                },
                "last_updated": datetime.now().isoformat()
            }

        # Stage 1 ìƒíƒœ ì—…ë°ì´íŠ¸
        if stage1_data:
            status_data["stage1"] = {
                "status": stage1_data.get("status", "success"),
                "timestamp": datetime.now().isoformat(),
                "data": {
                    "url_count": stage1_data.get("url_count", 0),
                    "file_path": f"kkday_urls_data_{city_name}_{tab}.json",
                    "new_count": stage1_data.get("new_count", 0)
                }
            }

        # Stage 2 ìƒíƒœ ì—…ë°ì´íŠ¸
        if stage2_data:
            status_data["stage2"] = {
                "status": stage2_data.get("status", "success"),
                "timestamp": datetime.now().isoformat(),
                "data": stage2_data.get("data", {})
            }

        status_data["last_updated"] = datetime.now().isoformat()

        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(status_data, f, ensure_ascii=False, indent=2)

        print(f"âœ… KKDAY ìƒíƒœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}")

        return filepath

    def load_url_collection_data(self, city_name: str, tab: str = "ì „ì²´") -> Optional[Dict]:
        """
        ì €ì¥ëœ URL ìˆ˜ì§‘ ë°ì´í„° ë¡œë“œ

        Args:
            city_name: ë„ì‹œëª…
            tab: íƒ­ êµ¬ë¶„

        Returns:
            Dict: URL ìˆ˜ì§‘ ë°ì´í„° ë˜ëŠ” None
        """

        filename = f"kkday_urls_data_{city_name}_{tab}.json"
        filepath = os.path.join(self.base_dir, filename)

        if not os.path.exists(filepath):
            print(f"âš ï¸ URL ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
            return None

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)

            print(f"âœ… KKDAY URL ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {filename}")
            print(f"   ğŸ“Š {data['collection_stats']['total_urls_found']}ê°œ URL ë¡œë“œë¨")

            return data

        except Exception as e:
            print(f"âŒ URL ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def load_status_data(self, city_name: str, tab: str = "ì „ì²´") -> Optional[Dict]:
        """
        ì €ì¥ëœ ìƒíƒœ ë°ì´í„° ë¡œë“œ

        Args:
            city_name: ë„ì‹œëª…
            tab: íƒ­ êµ¬ë¶„

        Returns:
            Dict: ìƒíƒœ ë°ì´í„° ë˜ëŠ” None
        """

        filename = f"kkday_status_{city_name}_{tab}.json"
        filepath = os.path.join(self.base_dir, filename)

        if not os.path.exists(filepath):
            print(f"âš ï¸ ìƒíƒœ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
            return None

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)

            print(f"âœ… KKDAY ìƒíƒœ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {filename}")

            return data

        except Exception as e:
            print(f"âŒ ìƒíƒœ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def get_urls_for_stage2(self, city_name: str, tab: str = "ì „ì²´") -> List[str]:
        """
        Stage 2ì—ì„œ ì‚¬ìš©í•  URL ëª©ë¡ ë°˜í™˜

        Args:
            city_name: ë„ì‹œëª…
            tab: íƒ­ êµ¬ë¶„

        Returns:
            List[str]: URL ëª©ë¡
        """

        url_data = self.load_url_collection_data(city_name, tab)
        if not url_data:
            return []

        # ì¤‘ë³µì´ ì•„ë‹Œ URLë§Œ ì¶”ì¶œ
        urls = []
        for url_entry in url_data.get("url_rank_mapping", []):
            if not url_entry.get("is_duplicate", False):
                urls.append(url_entry["url"])

        print(f"âœ… Stage 2ìš© URL {len(urls)}ê°œ ì¤€ë¹„ ì™„ë£Œ")

        return urls

    def check_all_cities_status(self) -> Dict[str, Dict]:
        """
        ëª¨ë“  ë„ì‹œì˜ ì‹¤í–‰ ìƒíƒœ í™•ì¸ (ìš´ì˜ ëª¨ë‹ˆí„°ë§ìš©)

        Returns:
            Dict: ë„ì‹œë³„ ìƒíƒœ ì •ë³´
        """

        status_files = []
        for file in os.listdir(self.base_dir):
            if file.startswith("kkday_status_") and file.endswith(".json"):
                status_files.append(file)

        all_status = {}
        for status_file in status_files:
            try:
                with open(os.path.join(self.base_dir, status_file), 'r', encoding='utf-8') as f:
                    status_data = json.load(f)

                city = status_data.get("city", "unknown")
                tab = status_data.get("tab", "ì „ì²´")
                key = f"{city}_{tab}"

                all_status[key] = {
                    "stage1_status": status_data["stage1"]["status"],
                    "stage2_status": status_data["stage2"]["status"],
                    "last_updated": status_data["last_updated"]
                }

            except:
                continue

        return all_status

print("âœ… KKDAY ë°ì´í„° ì˜ì†ì„± ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ (KLOOK ë°©ì‹ ì ìš©)")