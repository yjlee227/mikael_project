{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373fb224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# 기존 시스템 함수 import (정확한 함수명 사용)\n",
    "from src.config import (CONFIG, UNIFIED_CITY_INFO, is_url_processed_fast,\n",
    "                       mark_url_processed_fast)\n",
    "from src.utils.file_handler import (\n",
    "    create_product_data_structure,\n",
    "    save_to_csv_klook,\n",
    "    get_dual_image_urls_klook,\n",
    "    download_dual_images_klook,\n",
    "    get_next_product_number,\n",
    "    auto_create_country_csv_after_crawling,\n",
    "    get_csv_stats\n",
    ")\n",
    "from src.scraper.parsers import extract_all_product_data\n",
    "from src.scraper.driver_manager import setup_driver\n",
    "\n",
    "# 크롤링 설정\n",
    "CITY_NAME = \"도쿄\"  # 수정 필요시 변경\n",
    "BATCH_SIZE = 50     # 진행률 표시 간격\n",
    "REQUEST_DELAY = 1   # 요청 간 대기시간(초)\n",
    "\n",
    "print(f\"KLOOK Sitemap 크롤러 준비\")\n",
    "print(f\"   대상 도시: {CITY_NAME}\")\n",
    "print(f\"   시작 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: URL 파일 처리 함수들\n",
    "\n",
    "def load_sitemap_urls(city_name):\n",
    "    \"\"\"저장된 sitemap URL 파일에서 URL 목록 로드\"\"\"\n",
    "    filename = f\"sitemap_urls/{city_name}_urls.txt\"\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"URL 파일을 찾을 수 없습니다: {filename}\")\n",
    "        print(\"해결 방법:\")\n",
    "        print(\"   1. KLOOK_Sitemap_Collector.ipynb를 먼저 실행하세요\")\n",
    "        print(\"   2. 도시명이 정확한지 확인하세요\")\n",
    "        return []\n",
    "\n",
    "    urls = []\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                url = line.strip()\n",
    "                if url:\n",
    "                    if url.startswith('https://www.klook.com/activity/'):\n",
    "                        urls.append(url)\n",
    "                    else:\n",
    "                        print(f\"잘못된 URL 형식 (라인 {line_num}): {url}\")\n",
    "\n",
    "        print(f\"URL 파일 로드 성공:\")\n",
    "        print(f\"   파일: {filename}\")\n",
    "        print(f\"   총 URL: {len(urls):,}개\")\n",
    "        return urls\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"URL 파일 읽기 실패: {e}\")\n",
    "        return []\n",
    "\n",
    "def filter_new_urls_only(urls, city_name):\n",
    "    \"\"\"중복 체크하여 신규 URL만 반환 (기존 페이지네이션 수집 데이터와 비교)\"\"\"\n",
    "    if not urls:\n",
    "        return []\n",
    "\n",
    "    print(f\"중복 검사 시작: {len(urls):,}개 URL\")\n",
    "\n",
    "    new_urls = []\n",
    "    duplicate_count = 0\n",
    "\n",
    "    for i, url in enumerate(urls, 1):\n",
    "        # 진행률 표시 (1000개마다)\n",
    "        if i % 1000 == 0:\n",
    "            progress = (i / len(urls)) * 100\n",
    "            print(f\"   진행: {i:,}/{len(urls):,} ({progress:.1f}%) - 신규: {len(new_urls):,}개\")\n",
    "\n",
    "        if not is_url_processed_fast(url, city_name):\n",
    "            new_urls.append(url)\n",
    "        else:\n",
    "            duplicate_count += 1\n",
    "\n",
    "    print(f\"중복 검사 완료:\")\n",
    "    print(f\"   신규 URL: {len(new_urls):,}개\")\n",
    "    print(f\"   중복 제외: {duplicate_count:,}개\")\n",
    "    print(f\"   신규 비율: {len(new_urls)/len(urls)*100:.1f}%\")\n",
    "\n",
    "    return new_urls\n",
    "\n",
    "def validate_sitemap_setup(city_name):\n",
    "    \"\"\"크롤링 시작 전 환경 검증\"\"\"\n",
    "    issues = []\n",
    "\n",
    "    # URL 파일 존재 확인\n",
    "    url_file = f\"sitemap_urls/{city_name}_urls.txt\"\n",
    "    if not os.path.exists(url_file):\n",
    "        issues.append(f\"URL 파일 없음: {url_file}\")\n",
    "\n",
    "    # 디렉토리 구조 확인\n",
    "    required_dirs = [\"hash_index\", \"data\", \"klook_img\"]\n",
    "    for dir_name in required_dirs:\n",
    "        if not os.path.exists(dir_name):\n",
    "            issues.append(f\"필수 디렉토리 없음: {dir_name}\")\n",
    "\n",
    "    # 도시 정보 확인\n",
    "    if city_name not in UNIFIED_CITY_INFO:\n",
    "        issues.append(f\"지원되지 않는 도시: {city_name}\")\n",
    "\n",
    "    if issues:\n",
    "        print(\"환경 검증 실패:\")\n",
    "        for issue in issues:\n",
    "            print(f\"   • {issue}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"환경 검증 통과\")\n",
    "        return True\n",
    "\n",
    "print(\"URL 파일 처리 함수 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ecfc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 크롤링 실행 함수들\n",
    "\n",
    "def create_sitemap_product_data(city_name, product_number, rank=None):\n",
    "    \"\"\"Sitemap 방식임을 표시하는 데이터 구조 생성\"\"\"\n",
    "    data = create_product_data_structure(city_name, product_number, rank)\n",
    "    data['수집방식'] = 'sitemap'  # 구분 컬럼 추가\n",
    "    return data\n",
    "\n",
    "def crawl_single_sitemap_url(driver, url, city_name, product_number, current, total):\n",
    "    \"\"\"단일 URL 크롤링 및 데이터 저장 (연속성 유지)\"\"\"\n",
    "    try:\n",
    "        print(f\"[{current:,}/{total:,}] #{product_number} 크롤링...\")\n",
    "        print(f\"   URL: {url}\")\n",
    "\n",
    "        # 페이지 방문\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # 기본 데이터 구조 (sitemap 표시 포함)\n",
    "        product_data = create_sitemap_product_data(city_name, product_number, current)\n",
    "        product_data['URL'] = url\n",
    "\n",
    "        # 상품 데이터 추출 (정확한 함수명 사용)\n",
    "        extracted_data = extract_all_product_data(driver, url)\n",
    "\n",
    "        if extracted_data:\n",
    "            # 데이터 병합\n",
    "            product_data.update(extracted_data)\n",
    "\n",
    "            # 기존 CSV에 추가 저장 (연속성 유지)\n",
    "            if save_to_csv_klook(product_data, city_name):\n",
    "                print(f\"   상품 #{product_number} CSV 저장 완료\")\n",
    "\n",
    "                # 이미지 다운로드 (연속성 유지)\n",
    "                try:\n",
    "                    image_urls = get_dual_image_urls_klook(driver)\n",
    "                    if image_urls:\n",
    "                        download_dual_images_klook(image_urls, product_number, city_name)\n",
    "                        print(f\"   이미지 다운로드 완료\")\n",
    "                except Exception as img_e:\n",
    "                    print(f\"   이미지 다운로드 실패: {img_e}\")\n",
    "\n",
    "                # 처리 완료 표시\n",
    "                mark_url_processed_fast(url, city_name, product_number, current)\n",
    "\n",
    "                return True, product_data.get('상품명', 'Unknown')\n",
    "        else:\n",
    "            print(f\"   상품 데이터 추출 실패\")\n",
    "            return False, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   크롤링 실패: {e}\")\n",
    "        return False, None\n",
    "\n",
    "def safe_crawl_with_retry(driver, url, city_name, product_number, current, total, max_retries=2):\n",
    "    \"\"\"재시도 로직이 있는 안전한 크롤링\"\"\"\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            if attempt > 0:\n",
    "                print(f\"   재시도 {attempt}/{max_retries}\")\n",
    "                time.sleep(3)  # 재시도시 더 긴 대기\n",
    "\n",
    "            success, product_name = crawl_single_sitemap_url(\n",
    "                driver, url, city_name, product_number, current, total\n",
    "            )\n",
    "\n",
    "            if success:\n",
    "                return True, product_name\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries:\n",
    "                print(f\"   최종 실패: {e}\")\n",
    "                return False, None\n",
    "            else:\n",
    "                print(f\"   시도 {attempt + 1} 실패: {e}\")\n",
    "\n",
    "    return False, None\n",
    "\n",
    "print(\"크롤링 실행 함수 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b1a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: 진행률 및 통계 함수들\n",
    "\n",
    "def show_progress_summary(current, total, success_count, failed_urls, start_time, city_name):\n",
    "    \"\"\"상세한 진행률 및 통계 표시\"\"\"\n",
    "    progress = (current / total) * 100\n",
    "    elapsed = time.time() - start_time\n",
    "    avg_time = elapsed / current if current > 0 else 0\n",
    "    remaining = (total - current) * avg_time\n",
    "\n",
    "    print(f\"\\n진행 상황 ({datetime.now().strftime('%H:%M:%S')})\")\n",
    "    print(f\"   도시: {city_name}\")\n",
    "    print(f\"   진행률: {current:,}/{total:,} ({progress:.1f}%)\")\n",
    "    print(f\"   성공: {success_count:,}개\")\n",
    "    print(f\"   실패: {len(failed_urls):,}개\")\n",
    "    print(f\"   소요시간: {elapsed/60:.1f}분\")\n",
    "    print(f\"   예상 잔여: {remaining/60:.1f}분\")\n",
    "    if current > 0:\n",
    "        print(f\"   성공률: {success_count/current*100:.1f}%\")\n",
    "        print(f\"   평균 속도: {avg_time:.1f}초/URL\")\n",
    "\n",
    "def save_crawling_log(city_name, total_urls, success_count, failed_urls, start_time, end_time):\n",
    "    \"\"\"크롤링 결과 로그 저장\"\"\"\n",
    "    log_data = {\n",
    "        \"도시\": city_name,\n",
    "        \"시작시간\": datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"종료시간\": datetime.fromtimestamp(end_time).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"소요시간\": f\"{(end_time - start_time)/60:.1f}분\",\n",
    "        \"총_URL\": total_urls,\n",
    "        \"성공\": success_count,\n",
    "        \"실패\": len(failed_urls),\n",
    "        \"성공률\": f\"{success_count/total_urls*100:.1f}%\",\n",
    "        \"수집방식\": \"sitemap\"\n",
    "    }\n",
    "\n",
    "    # 로그 파일 저장\n",
    "    log_filename = f\"crawling_logs/{city_name}_sitemap_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "    os.makedirs(\"crawling_logs\", exist_ok=True)\n",
    "\n",
    "    with open(log_filename, 'w', encoding='utf-8') as f:\n",
    "        for key, value in log_data.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "        if failed_urls:\n",
    "            f.write(f\"\\n실패한 URL 목록:\\n\")\n",
    "            for url in failed_urls:\n",
    "                f.write(f\"  - {url}\\n\")\n",
    "\n",
    "    print(f\"크롤링 로그 저장: {log_filename}\")\n",
    "    return log_data\n",
    "\n",
    "print(\"진행률 및 통계 함수 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5168af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: 메인 크롤링 실행\n",
    "\n",
    "def main_sitemap_crawling(city_name):\n",
    "    \"\"\"Sitemap 기반 크롤링 메인 함수\"\"\"\n",
    "    start_time = time.time()\n",
    "    driver = None\n",
    "    success_count = 0\n",
    "    failed_urls = []\n",
    "    successful_products = []\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n'{city_name}' Sitemap 크롤링 시작\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # 1. 환경 검증\n",
    "        if not validate_sitemap_setup(city_name):\n",
    "            return\n",
    "\n",
    "        # 2. 저장된 URL 불러오기\n",
    "        print(f\"\\n1단계: URL 파일 로드\")\n",
    "        sitemap_urls = load_sitemap_urls(city_name)\n",
    "        if not sitemap_urls:\n",
    "            return\n",
    "\n",
    "        # 3. 기존 수집 데이터와 중복 체크하여 제외\n",
    "        print(f\"\\n2단계: 중복 검사\")\n",
    "        new_urls = filter_new_urls_only(sitemap_urls, city_name)\n",
    "\n",
    "        if not new_urls:\n",
    "            print(\"\\n모든 URL이 이미 처리되었습니다!\")\n",
    "            print(\"새로운 URL을 수집하려면 KLOOK_Sitemap_Collector.ipynb를 다시 실행하세요.\")\n",
    "            return\n",
    "\n",
    "        # 4. 드라이버 설정\n",
    "        print(f\"\\n3단계: 브라우저 드라이버 설정\")\n",
    "        driver = setup_driver(city_name)\n",
    "        print(f\"드라이버 준비 완료\")\n",
    "\n",
    "        # 5. 신규 URL 순차 크롤링 (연속성 유지)\n",
    "        print(f\"\\n4단계: 크롤링 실행\")\n",
    "        print(f\"   처리 대상: {len(new_urls):,}개 신규 URL\")\n",
    "        print(f\"   예상 소요: {len(new_urls) * REQUEST_DELAY / 60:.1f}분\")\n",
    "\n",
    "        for i, url in enumerate(new_urls, 1):\n",
    "            # 번호 연속성 유지 (기존 CSV 번호 이어서)\n",
    "            product_number = get_next_product_number(city_name)\n",
    "\n",
    "            # 안전한 크롤링 (재시도 포함)\n",
    "            success, product_name = safe_crawl_with_retry(\n",
    "                driver, url, city_name, product_number, i, len(new_urls)\n",
    "            )\n",
    "\n",
    "            if success:\n",
    "                success_count += 1\n",
    "                successful_products.append({\n",
    "                    'number': product_number,\n",
    "                    'name': product_name,\n",
    "                    'url': url\n",
    "                })\n",
    "            else:\n",
    "                failed_urls.append(url)\n",
    "\n",
    "            # 진행률 표시\n",
    "            if i % BATCH_SIZE == 0 or i == len(new_urls):\n",
    "                show_progress_summary(i, len(new_urls), success_count, failed_urls, start_time, city_name)\n",
    "\n",
    "            # 요청 간격 (서버 부하 방지)\n",
    "            if i < len(new_urls):  # 마지막이 아닐때만\n",
    "                time.sleep(REQUEST_DELAY)\n",
    "\n",
    "        # 6. 최종 결과\n",
    "        end_time = time.time()\n",
    "        print(f\"\\n'{city_name}' Sitemap 크롤링 완료!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"최종 통계:\")\n",
    "        print(f\"   총 소요시간: {(end_time - start_time)/60:.1f}분\")\n",
    "        print(f\"   처리 URL: {len(new_urls):,}개\")\n",
    "        print(f\"   성공: {success_count:,}개\")\n",
    "        print(f\"   실패: {len(failed_urls):,}개\")\n",
    "        print(f\"   성공률: {success_count/len(new_urls)*100:.1f}%\")\n",
    "\n",
    "        if successful_products:\n",
    "            print(f\"\\n수집 성공한 상품 (최근 5개):\")\n",
    "            for product in successful_products[-5:]:\n",
    "                print(f\"   #{product['number']}: {product['name']}\")\n",
    "\n",
    "        # 7. 크롤링 로그 저장\n",
    "        save_crawling_log(city_name, len(new_urls), success_count, failed_urls, start_time, end_time)\n",
    "\n",
    "        # 8. 국가별 통합 CSV 자동 생성\n",
    "        if success_count > 0:\n",
    "            print(f\"\\n5단계: 국가별 통합 CSV 생성\")\n",
    "            auto_create_country_csv_after_crawling(city_name)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n사용자에 의해 중단됨 (현재까지 {success_count}개 성공)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n크롤링 중 치명적 오류: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # 드라이버 정리 (안전한 종료)\n",
    "        if driver:\n",
    "            print(f\"\\n브라우저 드라이버 종료 중...\")\n",
    "            try:\n",
    "                driver.quit()\n",
    "                print(f\"드라이버 종료 완료\")\n",
    "            except Exception as e:\n",
    "                print(f\"드라이버 종료 중 오류: {e}\")\n",
    "\n",
    "# 실행\n",
    "print(f\"크롤링 시작 준비 완료\")\n",
    "main_sitemap_crawling(CITY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faaf9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: 결과 확인 및 통계\n",
    "\n",
    "def show_final_statistics(city_name):\n",
    "    \"\"\"최종 수집 데이터 통계 표시\"\"\"\n",
    "    print(f\"\\n'{city_name}' 최종 데이터 현황\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # CSV 통계\n",
    "    csv_stats = get_csv_stats(city_name)\n",
    "    if csv_stats and 'error' not in csv_stats:\n",
    "        print(f\"CSV 파일 정보:\")\n",
    "        print(f\"   파일: {csv_stats['file_path']}\")\n",
    "        print(f\"   총 상품: {csv_stats['total_products']:,}개\")\n",
    "        print(f\"   고유 상품: {csv_stats['unique_hashes']:,}개\")\n",
    "        print(f\"   파일 크기: {csv_stats['file_size']:,} bytes ({csv_stats['file_size']/1024/1024:.1f}MB)\")\n",
    "\n",
    "        # sitemap 방식으로 수집된 데이터 확인\n",
    "        try:\n",
    "            import csv\n",
    "            sitemap_count = 0\n",
    "            pagination_count = 0\n",
    "\n",
    "            with open(csv_stats['file_path'], 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                for row in reader:\n",
    "                    collection_method = row.get('수집방식', 'pagination')\n",
    "                    if collection_method == 'sitemap':\n",
    "                        sitemap_count += 1\n",
    "                    else:\n",
    "                        pagination_count += 1\n",
    "\n",
    "            print(f\"\\n수집 방식별 통계:\")\n",
    "            print(f\"   페이지네이션: {pagination_count:,}개\")\n",
    "            print(f\"   Sitemap: {sitemap_count:,}개\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   방식별 통계 확인 실패: {e}\")\n",
    "    else:\n",
    "        print(f\"CSV 파일 통계 확인 실패\")\n",
    "\n",
    "    # 이미지 파일 확인\n",
    "    from src.utils.file_handler import get_image_stats\n",
    "    img_stats = get_image_stats(city_name)\n",
    "    if img_stats and 'error' not in img_stats:\n",
    "        print(f\"\\n이미지 파일 정보:\")\n",
    "        print(f\"   디렉토리: {img_stats['directory']}\")\n",
    "        print(f\"   총 이미지: {img_stats['total_images']:,}개\")\n",
    "        print(f\"   총 크기: {img_stats['total_size']/1024/1024:.1f}MB\")\n",
    "\n",
    "def show_next_steps():\n",
    "    \"\"\"다음 단계 안내\"\"\"\n",
    "    print(f\"\\n다음 단계 안내\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"완료된 작업:\")\n",
    "    print(\"   • Sitemap URL 수집 완료\")\n",
    "    print(\"   • 중복 검사 및 신규 URL 크롤링 완료\")\n",
    "    print(\"   • 기존 CSV에 데이터 추가 (연속성 유지)\")\n",
    "    print(\"   • 이미지 다운로드 완료\")\n",
    "    print(\"   • 국가별 통합 CSV 생성 완료\")\n",
    "\n",
    "    print(f\"\\n추가 가능한 작업:\")\n",
    "    print(\"   1. 다른 도시 크롤링:\")\n",
    "    print(\"      → Cell 1에서 CITY_NAME 변경 후 재실행\")\n",
    "    print(\"   2. 정기 업데이트:\")\n",
    "    print(\"      → KLOOK_Sitemap_Collector.ipynb → 이 노트북 순서로 실행\")\n",
    "    print(\"   3. 데이터 품질 검증:\")\n",
    "    print(\"      → 수집된 데이터의 완성도 확인\")\n",
    "\n",
    "# 최종 결과 확인 실행\n",
    "show_final_statistics(CITY_NAME)\n",
    "show_next_steps()\n",
    "\n",
    "print(f\"\\nKLOOK Sitemap 크롤링 시스템 완료\")\n",
    "print(f\"완료 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
