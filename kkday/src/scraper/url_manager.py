"""
URL ìˆ˜ì§‘ ë° ê´€ë¦¬ ì‹œìŠ¤í…œ
- KKday URL íŒ¨í„´ ê²€ì¦ ë° ìˆ˜ì§‘
- URL ì¤‘ë³µ ë°©ì§€ ì‹œìŠ¤í…œ
- URL ìƒíƒœ ê´€ë¦¬ ë° ì¶”ì 
"""

import os
import re
import hashlib
import json
import time
import random
from datetime import datetime
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

from ..config import CONFIG, get_city_code, is_url_processed_fast, mark_url_processed_fast, SELENIUM_AVAILABLE, get_random_user_agent 

# ì¡°ê±´ë¶€ import (sitemap ê¸°ëŠ¥ìš©)
try:
    import requests
    from bs4 import BeautifulSoup
    REQUESTS_AVAILABLE = True
except ImportError:
    print("âš ï¸ requests/beautifulsoup4ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Sitemap ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
    REQUESTS_AVAILABLE = False

if SELENIUM_AVAILABLE:
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException

# =============================================================================
# KKday URL íŒ¨í„´ ë° ê²€ì¦ ì‹œìŠ¤í…œ
# =============================================================================

def is_valid_kkday_url(url):
    """KKday URL ìœ íš¨ì„± ê²€ì‚¬"""
    if not url or not isinstance(url, str):
        return False
    # KKday ë„ë©”ì¸ ì²´í¬
    kkday_domains = [
        'kkday.com/ko',
        'www.kkday.com/ko',
        'm.kkday.com'
    ]
    parsed = urlparse(url)
    domain_valid = any(domain in parsed.netloc.lower() + parsed.path.lower() for domain in kkday_domains)
    if not domain_valid:
        return False

    # KKday URL íŒ¨í„´ ì²´í¬ (ìƒí’ˆ ìƒì„¸ + ëª©ë¡ í˜ì´ì§€) 
    product_patterns = [
        # ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ íŒ¨í„´
        r'/ko/product/\d+$',                           # /ko/product/287674
        r'/ko/product/\d+-[\w\-]+$',                   # /ko/product/146520-krabi-kayaking
        r'/ko/product/\d+-[\w\-]+-[\w\-]+$',           # /ko/product/138349-speedboat-transfer-service
        r'/ko/product/\d+-[\w\-]+-[\w\-]+-[\w\-]+',    # ë” ê¸´ í˜•íƒœë“¤
        r'/product/\d+',                               # ì–¸ì–´ ìƒëµëœ í˜•íƒœ
        # ìƒí’ˆ ëª©ë¡ í˜ì´ì§€ íŒ¨í„´
        r'/ko/product/productlist/[%\w\-]+',           #/ko/product/productlist/%EB%B0%A9%EC%BD%95
        r'/ko/product/productlist$',                   # /ko/product/productlist
        r'/product/productlist',                       # ì–¸ì–´ ìƒëµëœ í˜•íƒœ
    ]    
    
    path_valid = any(re.search(pattern, url) for pattern in product_patterns)
    return path_valid

def normalize_kkday_url(url):
    """KKday URL ì •ê·œí™”"""
    if not url:
        return url
    try:
        parsed = urlparse(url)
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì •ë¦¬ (ì¶”ì ìš© íŒŒë¼ë¯¸í„° ì œê±°)
        query_params = parse_qs(parsed.query)
        # ìœ ì§€í•  íŒŒë¼ë¯¸í„°ë§Œ ì„ íƒ (KKday ê¸°ì¤€)
        keep_params = ['lang', 'currency', 'city', 'page', 'date']
        cleaned_params = {k: v for k, v in query_params.items() if k in keep_params}
        # URL ì¬êµ¬ì„±
        cleaned_query = urlencode(cleaned_params, doseq=True)
        cleaned_url = urlunparse((
            parsed.scheme,
            parsed.netloc,
            parsed.path,
            parsed.params,
            cleaned_query,
            ''  # fragment ì œê±°
        ))
        return cleaned_url
    except Exception:
        return url

def extract_product_id(url):
    """URLì—ì„œ product ID ì¶”ì¶œ"""
    if not url:
        return None
    # ìˆ«ì ID íŒ¨í„´ ë§¤ì¹­ (KKday ê¸°ì¤€)
    id_patterns = [
        r'/ko/product/(\d+)',              # /ko/product/287674
        r'/ko/product/(\d+)-[\w\-]+',      # /ko/product/146520-krabi-kayaking
        r'/product/(\d+)',                 # /product/287674 (ì–¸ì–´ ìƒëµ)
        r'/product/(\d+)-[\w\-]+',         # /product/146520-krabi-kayaking
    ]
    for pattern in id_patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return None

# =============================================================================
# URL ìˆ˜ì§‘ ì‹œìŠ¤í…œ
# =============================================================================

def collect_urls_from_page(driver, city_name):
    """í˜„ì¬ í˜ì´ì§€ì—ì„œ KKday URL ìˆ˜ì§‘"""
    print("ğŸ”— í˜ì´ì§€ì—ì„œ URL ìˆ˜ì§‘ ì¤‘...")
    
    if not SELENIUM_AVAILABLE:
        print("âš ï¸ Seleniumì´ ì—†ì–´ URL ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []
    
    try:
        # KKday ìƒí’ˆ URLì„ ì°¾ëŠ” CSS ì„ íƒìë“¤
        url_selectors = [
            "a[href*='/ko/product/']",              # KKday ìƒí’ˆ ë§í¬
            ".product-card a",                      # ìƒí’ˆ ì¹´ë“œ (ê³µí†µ)
            ".product-list-main__product-card-2 a", # KKday ì…€ë ‰í„° ë¬¸ì„œ ê¸°ì¤€
            ".gtm-prod-card-element a",             # KKday ì…€ë ‰í„° ë¬¸ì„œ ê¸°ì¤€
            ".card a[href*='kkday']",               # kkday ë„ë©”ì¸ í¬í•¨
            ".item a[href*='/ko/product/']",        # KKday ìƒí’ˆ ë§í¬ (ë‹¤ë¥¸ ì»¨í…Œì´ë„ˆ)
        ]    
        
        found_urls = []      # ìˆœì„œ ìœ ì§€ë¥¼ ìœ„í•œ list
        seen_urls = set()    # ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•œ set
        
        for selector in url_selectors:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                
                for element in elements:
                    try:
                        href = element.get_attribute("href")
                        if href and is_valid_kkday_url(href):           
                            normalized_url = normalize_kkday_url(href)  
                            if normalized_url not in seen_urls:
                                found_urls.append(normalized_url)
                                seen_urls.add(normalized_url)                 
                    except:
                        continue
                        
            except Exception:
                continue
        
        print(f"  âœ… ìˆ˜ì§‘ëœ URL: {len(found_urls)}ê°œ")
        return found_urls
        
    except Exception as e:
        print(f"  âš ï¸ URL ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

def get_pagination_urls(driver, max_pages=5):
    """í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•œ URL ìˆ˜ì§‘"""
    print(f"ğŸ“„ í˜ì´ì§€ë„¤ì´ì…˜ URL ìˆ˜ì§‘ ì¤‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)...")
    
    all_urls = []        # ìˆœì„œ ìœ ì§€ë¥¼ ìœ„í•œ list
    seen_urls = set()    # ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•œ set
    current_page = 1

    while current_page <= max_pages:
        print(f"  ğŸ“„ {current_page}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")

        # í˜„ì¬ í˜ì´ì§€ì—ì„œ URL ìˆ˜ì§‘
        page_urls = collect_urls_from_page(driver, "")

        if not page_urls:
            print("  âš ï¸ URLì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìˆ˜ì§‘ ì¤‘ë‹¨")
            break

        for url in page_urls:
            if url not in seen_urls:
                all_urls.append(url)
                seen_urls.add(url)

        # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
        if current_page < max_pages:
            if not go_to_next_page(driver):
                print("  â„¹ï¸ ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ìŒ")
                break

        current_page += 1
        time.sleep(2)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°

    print(f"âœ… ì´ ìˆ˜ì§‘ëœ URL: {len(all_urls)}ê°œ")
    return all_urls
    
def go_to_next_page(driver):
    """KKday ìˆ«ì í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ë‹¤ìŒ í˜ì´ì§€ ì´ë™"""
    if not SELENIUM_AVAILABLE:
        return False

    # KKday í˜ì´ì§€ë„¤ì´ì…˜ ì…€ë ‰í„° (ë¬¸ì„œ ê¸°ë°˜)
    kkday_pagination_selectors = [
        ".pagination .a-page a",
        ".pagination .fa-angle-right",
        "#productListApp > div > div > main > ul.pagination a",
        ".pagination li a",
        ".pagination a",
        "ul.pagination li a"
    ]

    print("  â¡ï¸ KKday ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")

    try:
        # 1ë‹¨ê³„: í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
        current_page_num = get_current_page_number(driver)
        next_page_num = current_page_num + 1
        print(f"    ğŸ“„ í˜„ì¬: {current_page_num}í˜ì´ì§€ â†’ ë‹¤ìŒ: {next_page_num}í˜ì´ì§€")

        # 2ë‹¨ê³„: KKday í˜ì´ì§€ë„¤ì´ì…˜ ë§í¬ í´ë¦­ ì‹œë„
        for selector in kkday_pagination_selectors:
            try:
                # ëª¨ë“  í˜ì´ì§€ ë§í¬ ì°¾ê¸°
                page_links = driver.find_elements(By.CSS_SELECTOR, selector)

                for link in page_links:
                    try:
                        link_text = link.text.strip()
                        link_href = link.get_attribute("href") or ""

                        # ë‹¤ìŒ í˜ì´ì§€ ë²ˆí˜¸ì™€ ë§¤ì¹˜ë˜ëŠ” ë§í¬ ì°¾ê¸°
                        if (link_text == str(next_page_num) or
                            f"page={next_page_num}" in link_href or
                            f"pageNumber={next_page_num}" in link_href):

                            if link.is_enabled() and link.is_displayed():
                                # í´ë¦­ ì „ URL ì €ì¥
                                old_url = driver.current_url

                                # ë§í¬ í´ë¦­
                                driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", link)
                                time.sleep(1)
                                link.click()

                                # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° ë° ë³€í™” í™•ì¸
                                time.sleep(3)
                                new_url = driver.current_url

                                if new_url != old_url:
                                    print(f"    âœ… {next_page_num}í˜ì´ì§€ ì´ë™ ì„±ê³µ")
                                    return True

                    except Exception:
                        continue

            except Exception:
                continue

        # 3ë‹¨ê³„: URL ì§ì ‘ ë³€ê²½ (í´ë°±)
        print("    ğŸ”„ ë§í¬ í´ë¦­ ì‹¤íŒ¨ - URL ì§ì ‘ ë³€ê²½ ì‹œë„")
        return go_to_next_page_by_url(driver, next_page_num)

    except Exception as e:
        print(f"  âŒ í˜ì´ì§€ë„¤ì´ì…˜ ì´ë™ ì‹¤íŒ¨: {e}")
        return False


def get_current_page_number(driver):
    """í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ"""
    current_page_selectors = [
        ".pagination .active",
        ".pagination .current",
        ".a-page.current",
        "[aria-current='page']"
    ]

    try:
        # í™œì„± í˜ì´ì§€ ìš”ì†Œì—ì„œ ë²ˆí˜¸ ì¶”ì¶œ
        for selector in current_page_selectors:
            try:
                current_element = driver.find_element(By.CSS_SELECTOR, selector)
                page_text = current_element.text.strip()

                if page_text.isdigit():
                    return int(page_text)

            except Exception:
                continue

        # URLì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
        current_url = driver.current_url
        if 'page=' in current_url:
            import re
            match = re.search(r'page=(\d+)', current_url)
            if match:
                return int(match.group(1))

        # ê¸°ë³¸ê°’: 1í˜ì´ì§€
        return 1

    except Exception:
        return 1


def go_to_next_page_by_url(driver, next_page_num):
    """URL ì§ì ‘ ë³€ê²½ìœ¼ë¡œ ë‹¤ìŒ í˜ì´ì§€ ì´ë™"""
    try:
        current_url = driver.current_url

        if 'page=' in current_url:
            import re
            new_url = re.sub(r'page=\d+', f'page={next_page_num}', current_url)
        else:
            separator = '&' if '?' in current_url else '?'
            new_url = current_url + f'{separator}page={next_page_num}'

        print(f"    ğŸ”— URL ì§ì ‘ ì´ë™: page={next_page_num}")
        driver.get(new_url)
        time.sleep(4)

        # ì´ë™ ì„±ê³µ í™•ì¸
        final_url = driver.current_url
        if f'page={next_page_num}' in final_url or str(next_page_num) in final_url:
            print(f"    âœ… URL ì§ì ‘ ì´ë™ ì„±ê³µ")
            return True
        else:
            print(f"    âŒ URL ì§ì ‘ ì´ë™ ì‹¤íŒ¨")
            return False

    except Exception as e:
        print(f"    âŒ URL ë³€ê²½ ì‹¤íŒ¨: {e}")
        return False

# =============================================================================
# URL ìƒíƒœ ê´€ë¦¬ ì‹œìŠ¤í…œ
# =============================================================================

def is_url_already_processed(url, city_name):
    """URL ì²˜ë¦¬ ì—¬ë¶€ í™•ì¸"""
    # hashlib ì‹œìŠ¤í…œ ìš°ì„  ì‚¬ìš©
    if CONFIG.get("USE_HASH_SYSTEM", True):
        return is_url_processed_fast(url, city_name)
    
    # í´ë°±: CSV ê¸°ë°˜ í™•ì¸ (ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ í˜¸í™˜)
    try:
        from ..utils.file_handler import get_csv_stats
        
        stats = get_csv_stats(city_name)
        if isinstance(stats, dict) and 'error' not in stats:
            # CSVì—ì„œ URL í™•ì¸ ë¡œì§ (ê°„ë‹¨í™”)
            return False  # ì¼ë‹¨ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ê°€ì •
            
    except Exception:
        pass
    
    return False

def mark_url_as_processed(url, city_name, product_number=None, rank=None):
    """URLì„ ì²˜ë¦¬ ì™„ë£Œë¡œ í‘œì‹œ"""
    # hashlib ì‹œìŠ¤í…œ ìš°ì„  ì‚¬ìš©
    if CONFIG.get("USE_HASH_SYSTEM", True):
        return mark_url_processed_fast(url, city_name, product_number, rank)
    
    return True

def get_unprocessed_urls(url_list, city_name):
    """ì²˜ë¦¬ë˜ì§€ ì•Šì€ URL ëª©ë¡ ë°˜í™˜"""
    unprocessed = []
    
    for url in url_list:
        if not is_url_already_processed(url, city_name):
            unprocessed.append(url)
    
    print(f"ğŸ“Š ì „ì²´ URL: {len(url_list)}ê°œ, ë¯¸ì²˜ë¦¬ URL: {len(unprocessed)}ê°œ")
    return unprocessed

def save_urls_to_file(urls, city_name, filename_suffix="collected"):
    """URL ëª©ë¡ì„ íŒŒì¼ë¡œ ì €ì¥"""
    try:
        city_code = get_city_code(city_name)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        save_dir = "url_logs"
        os.makedirs(save_dir, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„±
        filename = f"{city_code}_{filename_suffix}_{timestamp}.txt"
        filepath = os.path.join(save_dir, filename)
        
        # URL ì €ì¥
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"# {city_name} URL ëª©ë¡\n")
            f.write(f"# ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# ì´ URL ê°œìˆ˜: {len(urls)}\n\n")
            
            for i, url in enumerate(urls, 1):
                f.write(f"{i}. {url}\n")
        
        print(f"âœ… URL ëª©ë¡ ì €ì¥ ì™„ë£Œ: {filepath}")
        return filepath
        
    except Exception as e:
        print(f"âš ï¸ URL ëª©ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")
        return None

def load_urls_from_file(filepath):
    """íŒŒì¼ì—ì„œ URL ëª©ë¡ ë¡œë“œ"""
    try:
        urls = []
        
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # ì£¼ì„ê³¼ ë¹ˆ ì¤„ ê±´ë„ˆë›°ê¸°
                if line and not line.startswith('#'):
                    # ë²ˆí˜¸ ì œê±° (ì˜ˆ: "1. https://...")
                    if '. ' in line:
                        url = line.split('. ', 1)[1]
                    else:
                        url = line
                    
                    if is_valid_kkday_url(url):
                        urls.append(url)
        
        print(f"âœ… URL ëª©ë¡ ë¡œë“œ ì™„ë£Œ: {len(urls)}ê°œ")
        return urls
        
    except Exception as e:
        print(f"âš ï¸ URL ëª©ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

# =============================================================================
# í†µí•© URL ìˆ˜ì§‘ ì‹œìŠ¤í…œ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±)
# =============================================================================

def execute_comprehensive_url_collection(driver, city_name, max_pages=3):
    """ì¢…í•©ì ì¸ URL ìˆ˜ì§‘ ì‹œìŠ¤í…œ"""
    print(f"ğŸš€ {city_name} ì¢…í•© URL ìˆ˜ì§‘ ì‹œì‘...")
    
    try:
        # í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•œ URL ìˆ˜ì§‘
        all_urls = get_pagination_urls(driver, max_pages)
        
        # ì¤‘ë³µ ì œê±° ë° ê²€ì¦
        valid_urls = []
        for url in all_urls:
            if is_valid_kkday_url(url):
                normalized = normalize_kkday_url(url)
                valid_urls.append(normalized)
        
        # ì¤‘ë³µ ì œê±°
        unique_urls = []
        seen_urls = set()
        for url in valid_urls:
            if url not in seen_urls:
                unique_urls.append(url)
                seen_urls.add(url)
        
        # ë¯¸ì²˜ë¦¬ URLë§Œ í•„í„°ë§
        unprocessed_urls = get_unprocessed_urls(unique_urls, city_name)
        
        # URL ì €ì¥
        if unprocessed_urls:
            save_urls_to_file(unprocessed_urls, city_name)
        
        print(f"âœ… URL ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(unique_urls)}ê°œ, ë¯¸ì²˜ë¦¬ {len(unprocessed_urls)}ê°œ")
        return unprocessed_urls
        
    except Exception as e:
        print(f"âŒ URL ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

# =============================================================================
# ğŸ—ºï¸ Sitemap ê¸°ë°˜ URL ìˆ˜ì§‘ (í˜ì´ì§€ë„¤ì´ì…˜ ë³´ì™„ìš©)
# =============================================================================

def collect_urls_from_sitemap(city_name, exclude_urls=None, limit=1000):
    """Sitemapì—ì„œ KKday URL ìˆ˜ì§‘ (ì¤‘ë³µ ì œì™¸)"""
    if not REQUESTS_AVAILABLE:
        print("âŒ requests/beautifulsoup4ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    
    print(f"ğŸ—ºï¸ '{city_name}' Sitemap URL ìˆ˜ì§‘ ì‹œì‘...")
    
    exclude_set = set(exclude_urls or [])
    print(f"   ğŸš« ì œì™¸í•  URL: {len(exclude_set)}ê°œ")
    
    # KKday sitemap URLë“¤
    sitemap_urls = [
        "https://www.kkday.com/sitemap.xml",
        "https://www.kkday.com/sitemap-products.xml",
        "https://www.kkday.com/sitemap-ko.xml",
        f"https://www.kkday.com/sitemap-{city_name.lower()}.xml"
    ]
       
    collected_urls = []
    
    for sitemap_url in sitemap_urls:
        try:
            print(f"  ğŸ“‹ Sitemap ì²˜ë¦¬ ì¤‘: {sitemap_url}")
            
            response = requests.get(sitemap_url, timeout=30, headers={
                'User-Agent': get_random_user_agent()
            })
            
            if response.status_code == 200:
                # XML íŒŒì‹±
                soup = BeautifulSoup(response.content, 'xml')
                
                # URL ì¶”ì¶œ
                urls = soup.find_all('url')
                for url_element in urls:
                    loc = url_element.find('loc')
                    if loc and loc.text:
                        url = loc.text.strip()
                        
                        # KKday ìƒí’ˆ URL í•„í„°ë§
                        if is_valid_kkday_url(url):
                            normalized_url = normalize_kkday_url(url)
                            
                            # ì¤‘ë³µ ì²´í¬ (ì´ë¯¸ ìˆ˜ì§‘í•œ URL ì œì™¸)
                            if normalized_url not in exclude_set and normalized_url not in collected_urls:
                                # ë„ì‹œëª…ê³¼ ê´€ë ¨ëœ URL ìš°ì„  (ì„ íƒì )
                                if city_name.lower() in url.lower() or len(collected_urls) < limit:
                                    collected_urls.append(normalized_url)
                                    
                                    if len(collected_urls) >= limit:
                                        break
                
                print(f"    âœ… ìƒˆë¡œìš´ URL {len([u for u in collected_urls if u not in exclude_set])}ê°œ ë°œê²¬")
                
            else:
                print(f"    âš ï¸ Sitemap ì ‘ê·¼ ì‹¤íŒ¨: HTTP {response.status_code}")
                
        except Exception as e:
            print(f"    âŒ Sitemap ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue
    
    # ìµœì¢… ì¤‘ë³µ ì œê±° (ì•ˆì „ì¥ì¹˜)
    final_urls = [url for url in collected_urls if url not in exclude_set]
    
    print(f"ğŸ‰ Sitemap ìˆ˜ì§‘ ì™„ë£Œ: {len(final_urls)}ê°œ ìƒˆë¡œìš´ URL")
    return final_urls[:limit]

def save_urls_to_collection(urls, city_name, collection_type="sitemap"):
    """URL ì»¬ë ‰ì…˜ì„ íŒŒì¼ë¡œ ì €ì¥"""
    try:
        city_code = get_city_code(city_name)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        save_dir = "url_collections"
        os.makedirs(save_dir, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„±
        filename = f"{city_code}_{collection_type}_{timestamp}.json"
        filepath = os.path.join(save_dir, filename)
        
        # ì»¬ë ‰ì…˜ ë°ì´í„° êµ¬ì„±
        collection_data = {
            "city_name": city_name,
            "city_code": city_code,
            "collection_type": collection_type,
            "collected_at": datetime.now().isoformat(),
            "total_urls": len(urls),
            "urls": urls
        }
        
        # JSON ì €ì¥
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(collection_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… URL ì»¬ë ‰ì…˜ ì €ì¥ ì™„ë£Œ: {filepath}")
        return True
        
    except Exception as e:
        print(f"âš ï¸ URL ì»¬ë ‰ì…˜ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

print("âœ… url_manager.py ë¡œë“œ ì™„ë£Œ: KKday URL ê´€ë¦¬ ì‹œìŠ¤í…œ ì¤€ë¹„!")
print("   ğŸ—ºï¸ Sitemap ê¸°ëŠ¥ ì¶”ê°€:")
print("   - collect_urls_from_sitemap(): KKday ì¤‘ë³µ ì œì™¸ Sitemap ìˆ˜ì§‘")
print("   - save_urls_to_collection(): URL ì»¬ë ‰ì…˜ ì €ì¥")