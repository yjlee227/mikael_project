# detail_scraper.py
import sys
import os
import time
import random

# Add src to path to find the crawler modules
sys.path.append('./src')
sys.path.append('.')

try:
    from src.scraper.crawler import KKdayCrawler
    print("âœ… KKday ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ KKday ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    raise

# ===== ğŸ¯ ì‚¬ìš©ì ì„¤ì • ì˜ì—­ =====
CITY_NAME = "ë°©ì½•"
INPUT_FILE = "kkday_urls.txt"

print("="*70)
print("ğŸš€ KKday ìƒì„¸ ì •ë³´ ìŠ¤í¬ë ˆì´í¼ ì‹œì‘")
print("="*70)
print(f"   ğŸ™ï¸ ë„ì‹œ: {CITY_NAME}")
print(f"   ğŸ’¾ ì…ë ¥ íŒŒì¼: {INPUT_FILE}")

# 1. URL íŒŒì¼ ì½ê¸°
if not os.path.exists(INPUT_FILE):
    print(f"âŒ URL íŒŒì¼ '{INPUT_FILE}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ğŸ’¡ ë¨¼ì € url_collector.pyë¥¼ ì‹¤í–‰í•˜ì—¬ URLì„ ìˆ˜ì§‘í•˜ì„¸ìš”.")
    sys.exit()

with open(INPUT_FILE, 'r', encoding='utf-8') as f:
    urls_to_scrape = [line.strip() for line in f if line.strip()]

if not urls_to_scrape:
    print("âš ï¸ URL íŒŒì¼ì— ìˆ˜ì§‘í•  URLì´ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit()

print(f"âœ… '{INPUT_FILE}'ì—ì„œ {len(urls_to_scrape)}ê°œì˜ URLì„ ì½ì—ˆìŠµë‹ˆë‹¤.")

crawler = None
try:
    # 2. í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    print("\nğŸ—ï¸ KKday í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”...")
    crawler = KKdayCrawler(city_name=CITY_NAME)
    if not crawler.initialize():
        raise Exception("í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨")

    # 3. ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰
    print("\nğŸ“¦ ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")
    success = crawler.crawl_products_batch(urls_to_scrape)

    # 4. [ì¶”ê°€] êµ­ê°€ë³„ í†µí•© CSV ìë™ ìƒì„±
    if success:
        from src.utils.file_handler import auto_create_country_csv_after_crawling
        auto_create_country_csv_after_crawling(CITY_NAME)

except Exception as e:
    print(f"âŒ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
finally:
    if crawler and crawler.driver:
        print("\nğŸŒ ë“œë¼ì´ë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        crawler.driver.quit()
    print("ğŸ ìƒì„¸ ì •ë³´ ìŠ¤í¬ë ˆì´í¼ ì¢…ë£Œ")
