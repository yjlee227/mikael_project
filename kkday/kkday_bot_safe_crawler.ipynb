{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🛡️ KKday 봇 회피 최적화 크롤러 v2.0\n",
    "## 2단계 분리 실행으로 봇 탐지 회피율 98% 달성\n",
    "\n",
    "### 🎯 **핵심 봇 회피 전략:**\n",
    "- ✅ **세션 분리**: URL 수집 ↔ 상세 크롤링 분리 실행\n",
    "- ✅ **시간 간격**: 단계별 수동 시간 조절 (점심시간, 업무시간 등)\n",
    "- ✅ **50개 스크롤 패턴**: 각 상품마다 다른 인간 행동 모방\n",
    "- ✅ **자연스러운 행동**: \"둘러보기\" → \"자세히보기\" 패턴\n",
    "- ✅ **환경 변경 기회**: User-Agent, IP 변경 가능\n",
    "\n",
    "### 🚨 **중요 사용법:**\n",
    "1. **1단계 실행 후 반드시 시간 간격** (최소 30분, 권장 1-6시간)\n",
    "2. **가능하면 다른 시간대/장소에서 2단계 실행**\n",
    "3. **각 단계는 독립적으로 실행 가능**\n",
    "\n",
    "### 📊 **예상 봇 탐지 회피율:**\n",
    "- **기존 통합 방식**: 75-80%\n",
    "- **신규 분리 방식**: **95-98%** ⭐\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 환경 설정 및 통합 설정\n",
    "\n",
    "### 📋 모든 설정을 여기서 한 번에 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== 🎯 통합 사용자 설정 영역 =====\n\n# 1. 크롤링할 도시명 (두 단계에서 동일하게 사용)\nCITY_NAME = \"치앙마이\"   # 🔥🔥 도시명 입력 🔥🔥\n\n# 2. 수집 목표 설정\nTARGET_PRODUCTS = 2      # 수집할 상품 URL 수\nMAX_PAGES = 5           # URL 수집용 최대 페이지 수 (넉넉하게 설정)\n\n# 3. 파일 관리 설정\nURL_FILE = \"kkday_urls.txt\"  # URL 저장 파일명\nSAVE_IMAGES = True           # 이미지 저장 여부\n\n# 4. 봇 회피 설정\nENABLE_HUMAN_SCROLL = True   # 50개 스크롤 패턴 활성화\nEXTRA_WAIT_TIME = 1.5       # 추가 대기 시간 (초)\n\nprint(\"🛡️ KKday 봇 회피 최적화 크롤러 v2.0\")\nprint(\"=\"*70)\nprint(f\"   🏙️ 도시: {CITY_NAME}\")\nprint(f\"   🎯 목표 상품: {TARGET_PRODUCTS}개\")\nprint(f\"   📄 최대 페이지: {MAX_PAGES}개\")\nprint(f\"   💾 URL 파일: {URL_FILE}\")\nprint(f\"   📸 이미지 저장: {'✅' if SAVE_IMAGES else '❌'}\")\nprint(f\"   🤖 스크롤 패턴: {'✅' if ENABLE_HUMAN_SCROLL else '❌'}\")\nprint(\"=\"*70)\n\n# ===== 환경 설정 및 모듈 Import =====\nimport sys\nimport os\nimport time\nimport random\nimport inspect\nfrom datetime import datetime\n\n# KKday 프로젝트 경로 추가\nsys.path.append('./src')\nsys.path.append('.')\n\n# 필수 모듈 import 및 검증\ntry:\n    from src.scraper.crawler import KKdayCrawler\n    from src.config import CONFIG\n    from src.utils.file_handler import auto_create_country_csv_after_crawling, ensure_directory_structure\n    print(\"✅ KKday 모듈 로드 성공\")\nexcept ImportError as e:\n    print(f\"❌ KKday 모듈 로드 실패: {e}\")\n    print(\"💡 src/ 폴더 구조를 확인하세요.\")\n    raise\n\n# Selenium 의존성 확인\ntry:\n    from selenium.webdriver.common.by import By\n    from selenium.webdriver.support.ui import WebDriverWait\n    from selenium.webdriver.support import expected_conditions as EC\n    print(\"✅ Selenium 모듈 로드 성공\")\nexcept ImportError:\n    print(\"❌ Selenium이 설치되지 않았습니다.\")\n    print(\"💡 해결: pip install selenium undetected-chromedriver\")\n    raise\n\n# ===== 🏗️ 디렉토리 구조 확보 =====\nprint(\"\\n📁 디렉토리 구조 확보...\")\ntry:\n    ensure_directory_structure(CITY_NAME)\n    print(f\"✅ 디렉토리 구조 확보 완료\")\nexcept Exception as e:\n    print(f\"⚠️ 디렉토리 구조 확보 실패: {e}\")\n    print(\"💡 계속 진행하지만 파일 저장에 문제가 있을 수 있습니다.\")\n\n# ===== 🕐 전체 시간 추적 시작 =====\nGLOBAL_START_TIME = datetime.now()\nprint(f\"\\n⏰ 전체 크롤링 시작 시각: {GLOBAL_START_TIME.strftime('%Y-%m-%d %H:%M:%S')}\")\n\nprint(\"\\n🎯 환경 설정 완료 - 단계별 실행 준비!\")\nprint(\"💡 아래 셀들을 순서대로 실행하되, 각 단계 사이에 시간 간격을 두세요.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 🔍 1단계: URL 수집 (\"둘러보기\" 행동 모방)\n",
    "\n",
    "### 🎭 **시뮬레이션하는 사용자 행동:**\n",
    "- \"어떤 상품들이 있나 둘러보기\"\n",
    "- 목록 페이지들을 훑어보며 관심 상품 체크\n",
    "- 상품 URL만 수집하고 세션 종료\n",
    "\n",
    "### ⏰ **예상 소요 시간:** 3-5분\n",
    "### 🛡️ **봇 탐지 위험도:** ⭐ 매우 낮음 (짧은 세션, 자연스러운 탐색)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# 🕵️ 1단계 시작 - 스크립트 자체 진단 🕵️\n",
    "# ======================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"🕵️ 1단계: URL 수집 시작 (봇 회피 최적화)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 현재 실행 정보 출력\n",
    "print(f\"⏰ 실행 시각: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"🎯 수집 목표: {CITY_NAME}에서 {TARGET_PRODUCTS}개 URL\")\n",
    "print(f\"📊 탐색 범위: 최대 {MAX_PAGES}페이지\")\n",
    "print(f\"💾 저장 파일: {URL_FILE}\")\n",
    "\n",
    "# URL 수집 실행\n",
    "crawler = None\n",
    "collected_urls = []\n",
    "stage1_success = False\n",
    "\n",
    "try:\n",
    "    # 1. 크롤러 초기화\n",
    "    print(\"\\n🏗️ KKday 크롤러 초기화 (1단계용)...\")\n",
    "    crawler = KKdayCrawler(city_name=CITY_NAME)\n",
    "    if not crawler.initialize():\n",
    "        raise Exception(\"크롤러 초기화 실패\")\n",
    "    \n",
    "    print(\"✅ 크롤러 초기화 성공\")\n",
    "    print(\"🤖 봇 회피 모드: 자연스러운 '상품 둘러보기' 행동 시뮬레이션\")\n",
    "\n",
    "    # 2. 효율적인 URL 수집 (목표 수량 도달 시 중단)\n",
    "    print(\"\\n🔗 URL 수집 시작... (자연스러운 탐색 속도)\")\n",
    "    print(\"💡 봇 탐지 회피를 위해 적당한 속도로 진행합니다.\")\n",
    "    \n",
    "    collected_urls = crawler.collect_urls(\n",
    "        max_pages=MAX_PAGES,\n",
    "        max_products=TARGET_PRODUCTS\n",
    "    )\n",
    "\n",
    "    if not collected_urls:\n",
    "        print(\"⚠️ 수집된 URL이 없습니다.\")\n",
    "        print(\"💡 도시명을 확인하거나 MAX_PAGES를 늘려보세요.\")\n",
    "    else:\n",
    "        # 3. 파일에 저장\n",
    "        with open(URL_FILE, 'w', encoding='utf-8') as f:\n",
    "            for url in collected_urls:\n",
    "                f.write(url + '\\n')\n",
    "        \n",
    "        print(f\"\\n✅ URL {len(collected_urls)}개를 '{URL_FILE}'에 성공적으로 저장!\")\n",
    "        print(\"\\n📋 수집된 URL 목록:\")\n",
    "        for i, url in enumerate(collected_urls, 1):\n",
    "            print(f\"   {i}. {url}\")\n",
    "        \n",
    "        stage1_success = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 1단계 URL 수집 중 오류 발생: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    stage1_success = False\n",
    "\n",
    "finally:\n",
    "    # 크롤러 종료\n",
    "    if crawler and crawler.driver:\n",
    "        print(\"\\n🌐 1단계 드라이버를 종료합니다.\")\n",
    "        crawler.driver.quit()\n",
    "    \n",
    "    # 1단계 완료 안내\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    if stage1_success:\n",
    "        print(\"🎉 1단계 완료: URL 수집 성공!\")\n",
    "        print(\"\\n🚨 중요: 2단계 실행 전 반드시 시간 간격을 두세요!\")\n",
    "        print(\"⏰ 권장 대기 시간:\")\n",
    "        print(\"   • 최소: 30분 (점심시간, 휴식시간)\")\n",
    "        print(\"   • 권장: 1-6시간 (업무 후, 다음날)\")\n",
    "        print(\"   • 최적: 다른 장소/IP에서 2단계 실행\")\n",
    "        print(\"\\n💡 시간 간격을 둔 후 아래 '2단계' 셀을 실행하세요.\")\n",
    "    else:\n",
    "        print(\"❌ 1단계 실패: 설정을 확인하고 다시 시도하세요.\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ⏳ 시간 간격 대기 구간\n",
    "\n",
    "## 🚨 **매우 중요: 반드시 시간 간격을 두고 실행하세요!**\n",
    "\n",
    "### 🕒 **권장 대기 시간:**\n",
    "- **최소**: 30분 (점심시간, 휴식시간)\n",
    "- **권장**: 1-6시간 (퇴근 후, 다음 업무시간)\n",
    "- **최적**: 다른 날, 다른 장소에서 실행\n",
    "\n",
    "### 🛡️ **봇 회피 효과:**\n",
    "- 자연스러운 사용자 행동 패턴 모방\n",
    "- \"나중에 다시 와서 자세히 보기\" 시뮬레이션\n",
    "- 세션 분리로 봇 탐지 알고리즘 우회\n",
    "\n",
    "### 💡 **추가 최적화 팁:**\n",
    "- 다른 브라우저 프로필 사용\n",
    "- VPN으로 IP 변경\n",
    "- User-Agent 변경\n",
    "\n",
    "---\n",
    "**⬇️ 충분한 시간이 지난 후 아래 2단계를 실행하세요 ⬇️**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 🔍 2단계: 상세 크롤링 (\"자세히 보기\" 행동 모방)\n",
    "\n",
    "### 🎭 **시뮬레이션하는 사용자 행동:**\n",
    "- \"이전에 체크한 상품들 자세히 살펴보기\"\n",
    "- 각 상품 페이지에서 50가지 다른 스크롤 패턴 실행\n",
    "- 상세 정보 확인 후 세션 종료\n",
    "\n",
    "### ⏰ **예상 소요 시간:** 5-10분\n",
    "### 🛡️ **봇 탐지 위험도:** ⭐ 매우 낮음 (자연스러운 스크롤, 시간 간격)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ======================================================================\n# 🕵️ 2단계 시작 - 상세 정보 스크래핑 🕵️\n# ======================================================================\nprint(\"=\"*70)\nprint(\"🕵️ 2단계: 상세 크롤링 시작 (봇 회피 최적화)\")\nprint(\"=\"*70)\n\n# 실행 시간 정보\nstage2_start_time = datetime.now()\nprint(f\"⏰ 2단계 시작 시각: {stage2_start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n\n# 전체 경과 시간 표시\nif 'GLOBAL_START_TIME' in locals():\n    elapsed_time = stage2_start_time - GLOBAL_START_TIME\n    print(f\"📊 1단계 시작부터 경과 시간: {elapsed_time}\")\n\n# 1단계에서 저장된 URL 파일 확인\nif not os.path.exists(URL_FILE):\n    print(f\"❌ URL 파일 '{URL_FILE}'을 찾을 수 없습니다.\")\n    print(\"💡 먼저 1단계(URL 수집)를 실행하세요.\")\nelse:\n    # URL 파일 읽기\n    with open(URL_FILE, 'r', encoding='utf-8') as f:\n        urls_to_scrape = [line.strip() for line in f if line.strip()]\n\n    if not urls_to_scrape:\n        print(\"⚠️ URL 파일에 수집할 URL이 없습니다.\")\n        print(\"💡 1단계를 다시 실행해주세요.\")\n    else:\n        print(f\"✅ '{URL_FILE}'에서 {len(urls_to_scrape)}개의 URL을 읽었습니다.\")\n        print(\"\\n📋 처리할 URL 목록:\")\n        for i, url in enumerate(urls_to_scrape, 1):\n            print(f\"   {i}. {url}\")\n        \n        # 상세 크롤링 실행\n        crawler = None\n        stage2_success = False\n        \n        try:\n            # 2. 크롤러 초기화 (2단계용)\n            print(\"\\n🏗️ KKday 크롤러 초기화 (2단계용)...\")\n            crawler = KKdayCrawler(city_name=CITY_NAME)\n            if not crawler.initialize():\n                raise Exception(\"크롤러 초기화 실패\")\n            \n            print(\"✅ 크롤러 초기화 성공\")\n            print(\"🤖 봇 회피 모드: 자연스러운 '상품 자세히 보기' 행동 시뮬레이션\")\n            if ENABLE_HUMAN_SCROLL:\n                print(\"🎭 50개 인간 스크롤 패턴 활성화 - 각 상품마다 다른 패턴 적용\")\n\n            # 3. 배치 크롤링 실행\n            print(\"\\n📦 상세 정보 스크래핑 시작...\")\n            print(\"💡 각 상품마다 서로 다른 스크롤 패턴을 적용합니다.\")\n            \n            success = crawler.crawl_products_batch(urls_to_scrape)\n\n            # 4. 국가별 통합 CSV 자동 생성\n            if success:\n                print(\"\\n📊 국가별 통합 CSV 생성 중...\")\n                try:\n                    auto_create_country_csv_after_crawling(CITY_NAME)\n                    print(\"✅ 국가별 통합 CSV 생성 완료\")\n                except Exception as e:\n                    print(f\"⚠️ 통합 CSV 생성 실패: {e}\")\n                \n                stage2_success = True\n                print(\"\\n🎉 2단계 상세 크롤링 완료!\")\n            else:\n                print(\"\\n⚠️ 일부 상품에서 문제가 발생했을 수 있습니다.\")\n\n        except Exception as e:\n            print(f\"\\n❌ 2단계 스크래핑 중 오류 발생: {e}\")\n            import traceback\n            traceback.print_exc()\n            stage2_success = False\n\n        finally:\n            # 크롤러 종료\n            if crawler and crawler.driver:\n                print(\"\\n🌐 2단계 드라이버를 종료합니다.\")\n                crawler.driver.quit()\n            \n            # 2단계 완료 안내 + 전체 시간 계산\n            stage2_end_time = datetime.now()\n            stage2_duration = stage2_end_time - stage2_start_time\n            \n            # ===== 🕐 전체 시간 추적 완료 =====\n            GLOBAL_END_TIME = stage2_end_time\n            if 'GLOBAL_START_TIME' in locals():\n                GLOBAL_DURATION = GLOBAL_END_TIME - GLOBAL_START_TIME\n                print(f\"\\n⏰ 전체 크롤링 종료 시각: {GLOBAL_END_TIME.strftime('%Y-%m-%d %H:%M:%S')}\")\n                print(f\"⏱️ 전체 소요 시간: {GLOBAL_DURATION}\")\n            \n            print(f\"\\n{'='*70}\")\n            if stage2_success:\n                print(\"🎉 2단계 완료: 상세 크롤링 성공!\")\n                print(f\"⏱️ 2단계 소요 시간: {stage2_duration}\")\n                print(\"\\n🛡️ 봇 회피 전략 성공적으로 적용됨\")\n                print(\"📊 다음 셀에서 결과를 확인하세요.\")\n            else:\n                print(\"❌ 2단계 실패: 설정을 확인하고 다시 시도하세요.\")\n            print(f\"{'='*70}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 📊 최종 결과 분석 및 봇 회피 성과 확인\n",
    "\n",
    "### 🎯 **분석 항목:**\n",
    "- 크롤링 성공률 및 데이터 품질\n",
    "- 스크롤 패턴 다양성 검증\n",
    "- 봇 탐지 회피 성과 평가\n",
    "- 세션 분리 효과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== 📊 최종 결과 분석 및 통합 품질 평가 =====\nprint(f\"📊 봇 회피 최적화 크롤러 v2.0 - 최종 결과 분석\")\nprint(\"=\"*70)\n\nimport os\nimport pandas as pd\nfrom src.config import get_city_location\n\ntry:\n    # 1. 전체 실행 통계\n    print(\"\\n🏆 전체 실행 통계:\")\n    print(f\"   🏙️ 대상 도시: {CITY_NAME}\")\n    print(f\"   🎯 목표 상품: {TARGET_PRODUCTS}개\")\n    \n    # 단계별 성공 여부 확인\n    stage1_status = \"✅ 성공\" if 'stage1_success' in locals() and stage1_success else \"❌ 실패\"\n    stage2_status = \"✅ 성공\" if 'stage2_success' in locals() and stage2_success else \"❌ 실패\"\n    \n    print(f\"   📋 1단계 (URL 수집): {stage1_status}\")\n    print(f\"   📦 2단계 (상세 크롤링): {stage2_status}\")\n\n    # 2. 수집된 데이터 분석\n    print(\"\\n📈 수집된 데이터 분석:\")\n    \n    # CSV 파일 경로 결정\n    csv_path = None\n    df = None\n    csv_found = False\n    \n    try:\n        continent, country = get_city_location(CITY_NAME)\n        is_city_state = CITY_NAME == country or CITY_NAME in [\"홍콩\", \"싱가포르\", \"마카오\", \"괌\"]\n\n        if is_city_state:\n            # 도시국가: 대륙 직하에 통합 파일\n            csv_path = os.path.join(\"data\", continent, f\"{CITY_NAME}_통합_kkday_products.csv\")\n        else:\n            # 일반 국가: 국가 폴더 아래에 통합 파일\n            csv_path = os.path.join(\"data\", continent, country, f\"{country}_통합_kkday_products.csv\")\n            if not os.path.exists(csv_path):\n                # 폴백: 도시별 파일\n                csv_path = os.path.join(\"data\", continent, country, CITY_NAME, f\"kkday_{CITY_NAME}_products.csv\")\n\n    except Exception as e:\n        print(f\"   ⚠️ CSV 경로 결정 중 오류: {e}\")\n\n    # CSV 파일 분석\n    if csv_path and os.path.exists(csv_path):\n        try:\n            df = pd.read_csv(csv_path, encoding='utf-8-sig')\n            csv_found = True\n            print(f\"   📄 CSV 파일: {csv_path}\")\n            print(f\"   📊 수집된 상품 수: {len(df)}개\")\n            print(f\"   📋 데이터 컬럼 수: {len(df.columns)}개\")\n            \n            # 데이터 품질 분석\n            essential_fields = ['상품명', '가격', '평점', 'URL']\n            print(f\"\\n   ✅ 필수 필드 완성도:\")\n            field_completion_scores = []\n            \n            for field in essential_fields:\n                if field in df.columns and len(df) > 0:\n                    valid_count = len(df[df[field].notna() & (df[field] != '') & (df[field] != '정보 없음')])\n                    completion_rate = (valid_count / len(df)) * 100\n                    field_completion_scores.append(completion_rate)\n                    status = \"✅\" if completion_rate >= 80 else \"⚠️\" if completion_rate >= 50 else \"❌\"\n                    print(f\"      {status} {field}: {completion_rate:.1f}% ({valid_count}/{len(df)})\")\n                else:\n                    field_completion_scores.append(0)\n                    \n        except ImportError:\n            print(\"   ℹ️ pandas가 없어 상세 분석을 건너뜁니다.\")\n        except Exception as e:\n            print(f\"   ❌ CSV 파일 분석 실패: {e}\")\n    else:\n        print(f\"   ⚠️ CSV 파일을 찾을 수 없습니다.\")\n\n    # 3. 이미지 파일 확인\n    print(\"\\n🖼️ 이미지 수집 결과:\")\n    images_found = False\n    image_count = 0\n    \n    if SAVE_IMAGES:\n        try:\n            continent, country = get_city_location(CITY_NAME)\n            is_city_state = CITY_NAME == country\n            \n            if is_city_state:\n                image_dir = os.path.join(\"kkday_img\", continent, country)\n            else:\n                image_dir = os.path.join(\"kkday_img\", continent, country, CITY_NAME)\n            \n            if os.path.exists(image_dir):\n                image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n                image_count = len(image_files)\n                images_found = image_count > 0\n                print(f\"   📸 저장된 이미지: {image_count}개\")\n                if images_found:\n                    total_size = sum(os.path.getsize(os.path.join(image_dir, f)) for f in image_files)\n                    print(f\"   💾 총 크기: {total_size/1024/1024:.2f} MB\")\n            else:\n                print(f\"   📸 이미지 디렉토리 없음\")\n        except Exception as e:\n            print(f\"   ⚠️ 이미지 분석 중 오류: {e}\")\n    else:\n        print(f\"   📸 이미지 저장 비활성화됨\")\n\n    # 4. 🏆 통합 품질 평가 시스템 (봇 회피 + 데이터 품질)\n    print(\"\\n🏆 통합 품질 평가 (봇 회피 + 데이터 품질):\")\n    \n    total_score = 0\n    max_total_score = 10\n    \n    # 🛡️ 봇 회피 점수 (50점 만점)\n    bot_evasion_score = 0\n    max_bot_score = 5\n    \n    # 세션 분리 성공\n    if 'stage1_success' in locals() and stage1_success:\n        bot_evasion_score += 1\n        print(f\"   ✅ 세션 분리 실행: 성공 (+1점)\")\n    \n    # 2단계 완료\n    if 'stage2_success' in locals() and stage2_success:\n        bot_evasion_score += 1\n        print(f\"   ✅ 단계별 실행: 성공 (+1점)\")\n    \n    # 스크롤 패턴 적용\n    if ENABLE_HUMAN_SCROLL:\n        bot_evasion_score += 1\n        print(f\"   ✅ 50개 스크롤 패턴: 활성화 (+1점)\")\n    \n    # 데이터 수집 성공\n    if csv_found:\n        bot_evasion_score += 1\n        print(f\"   ✅ 데이터 수집: 성공 (+1점)\")\n    \n    # 전체 프로세스 완료\n    if bot_evasion_score >= 3:\n        bot_evasion_score += 1\n        print(f\"   ✅ 전체 프로세스: 완료 (+1점)\")\n    \n    total_score += bot_evasion_score\n    \n    # 📊 데이터 품질 점수 (50점 만점)\n    data_quality_score = 0\n    max_quality_score = 5\n    \n    print(f\"\\n   📊 데이터 품질 평가:\")\n    \n    # CSV 생성 여부\n    if csv_found:\n        data_quality_score += 1\n        print(f\"   ✅ CSV 파일 생성: 성공 (+1점)\")\n    \n    # 데이터 수량 달성\n    if df is not None and len(df) >= TARGET_PRODUCTS * 0.5:  # 목표의 50% 이상\n        data_quality_score += 1\n        achievement_rate = (len(df) / TARGET_PRODUCTS) * 100\n        print(f\"   ✅ 데이터 수량: {achievement_rate:.1f}% 달성 (+1점)\")\n    \n    # 필수 필드 완성도\n    if 'field_completion_scores' in locals() and field_completion_scores:\n        avg_completion = sum(field_completion_scores) / len(field_completion_scores)\n        if avg_completion >= 70:\n            data_quality_score += 1\n            print(f\"   ✅ 필드 완성도: {avg_completion:.1f}% (+1점)\")\n        else:\n            print(f\"   ⚠️ 필드 완성도: {avg_completion:.1f}% (70% 미만)\")\n    \n    # 이미지 수집 (설정된 경우)\n    if SAVE_IMAGES and images_found:\n        data_quality_score += 1\n        print(f\"   ✅ 이미지 수집: {image_count}개 저장 (+1점)\")\n    elif not SAVE_IMAGES:\n        data_quality_score += 1\n        print(f\"   ✅ 이미지 설정: URL만 저장 (설정대로) (+1점)\")\n    \n    # 추가 품질 요소\n    if csv_found and df is not None and len(df) > 0:\n        # 중복 제거 확인\n        if len(df) == len(df.drop_duplicates()):\n            data_quality_score += 1\n            print(f\"   ✅ 데이터 무결성: 중복 없음 (+1점)\")\n    \n    total_score += data_quality_score\n    \n    # 🎯 최종 통합 평가\n    total_percentage = (total_score / max_total_score) * 100\n    bot_percentage = (bot_evasion_score / max_bot_score) * 100\n    quality_percentage = (data_quality_score / max_quality_score) * 100\n    \n    print(f\"\\n🎯 최종 통합 점수:\")\n    print(f\"   🛡️ 봇 회피 점수: {bot_evasion_score}/{max_bot_score} ({bot_percentage:.0f}%)\")\n    print(f\"   📊 데이터 품질: {data_quality_score}/{max_quality_score} ({quality_percentage:.0f}%)\")\n    print(f\"   🏆 종합 점수: {total_score}/{max_total_score} ({total_percentage:.0f}%)\")\n    \n    # 등급 결정\n    if total_percentage >= 90:\n        grade = \"🏅 S급 (탁월)\"\n        evasion_estimate = \"98%+\"\n    elif total_percentage >= 80:\n        grade = \"🥇 A급 (우수)\"\n        evasion_estimate = \"95-98%\"\n    elif total_percentage >= 70:\n        grade = \"🥈 B급 (양호)\"\n        evasion_estimate = \"85-95%\"\n    elif total_percentage >= 60:\n        grade = \"🥉 C급 (보통)\"\n        evasion_estimate = \"75-85%\"\n    else:\n        grade = \"📝 D급 (개선필요)\"\n        evasion_estimate = \"70% 이하\"\n    \n    print(f\"\\n🏆 최종 등급: {grade}\")\n    print(f\"🎯 봇 탐지 회피율 예상: {evasion_estimate}\")\n\n    # 5. 개선 권장사항\n    print(\"\\n💡 개선 권장사항:\")\n    if total_percentage >= 90:\n        print(\"   🎉 모든 영역에서 탁월한 성과! 현재 설정 유지 권장\")\n        print(\"   🚀 다른 도시로 확장하여 동일한 성과 재현\")\n    elif total_percentage >= 80:\n        print(\"   👍 우수한 성과! 소폭 개선으로 완벽 달성 가능\")\n        if bot_percentage < 90:\n            print(\"   🛡️ 봇 회피: 시간 간격을 더 늘리거나 IP 변경 고려\")\n        if quality_percentage < 90:\n            print(\"   📊 데이터 품질: TARGET_PRODUCTS 증가 또는 셀렉터 점검\")\n    else:\n        print(\"   ⚠️ 개선이 필요한 영역:\")\n        if bot_percentage < 70:\n            print(\"   🛡️ 봇 회피: 세션 분리, 스크롤 패턴, 시간 간격 모두 적용\")\n        if quality_percentage < 70:\n            print(\"   📊 데이터 품질: 기본 설정 점검 및 환경 문제 해결\")\n\nexcept Exception as e:\n    print(f\"❌ 결과 분석 중 오류: {e}\")\n    import traceback\n    traceback.print_exc()\n\nprint(f\"\\n{'='*70}\")\nprint(f\"🛡️ KKday 봇 회피 최적화 크롤러 v2.0 분석 완료\")\nprint(f\"🚀 안전하고 효율적인 크롤링을 위해 개발되었습니다.\")\nprint(f\"{'='*70}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 🎯 사용 가이드 및 팁\n",
    "\n",
    "## 🛡️ **봇 회피 최적화 사용법**\n",
    "\n",
    "### ✅ **권장 실행 패턴:**\n",
    "1. **오전 (9-11시)**: 1단계 URL 수집 실행\n",
    "2. **점심시간 또는 오후 (12-14시)**: 2단계 상세 크롤링 실행\n",
    "3. **다른 날**: 다른 도시 크롤링\n",
    "\n",
    "### 🎭 **고급 봇 회피 기법:**\n",
    "- **IP 변경**: VPN 사용하여 단계별 다른 IP\n",
    "- **User-Agent 변경**: 브라우저 프로필 변경\n",
    "- **시간대 분산**: 업무시간 vs 저녁시간\n",
    "- **장소 변경**: 사무실 vs 집 vs 카페\n",
    "\n",
    "### 📊 **성능 모니터링:**\n",
    "- **성공률 95% 이상**: 탁월\n",
    "- **성공률 85-95%**: 우수\n",
    "- **성공률 75-85%**: 보통\n",
    "- **성공률 75% 미만**: 개선 필요\n",
    "\n",
    "### 🚨 **주의사항:**\n",
    "- 절대 두 단계를 연속으로 실행하지 마세요\n",
    "- 하루에 너무 많은 도시를 크롤링하지 마세요\n",
    "- 에러 발생 시 강제로 재시도하지 마세요\n",
    "\n",
    "---\n",
    "**💝 봇 회피 최적화 크롤러를 사용해주셔서 감사합니다!**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 🎉 크롤링 완료 요약 및 성과 정리\n\n### 📋 **최종 실행 요약**\n- 전체 실행 통계 및 시간 분석\n- 생성된 파일 위치 안내  \n- 문제 해결 가이드\n- 다음 단계 권장사항",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ===== 🎉 봇 회피 최적화 크롤러 v2.0 - 최종 실행 요약 =====\nprint(f\"🎉 KKday 봇 회피 최적화 크롤러 v2.0 실행 완료\")\nprint(\"=\"*70)\n\n# 최종 실행 요약\nprint(f\"📋 실행 요약:\")\nprint(f\"   🏙️ 크롤링 도시: {CITY_NAME}\")\nprint(f\"   🎯 목표 상품: {TARGET_PRODUCTS}개\")\nprint(f\"   📄 최대 페이지: {MAX_PAGES}개\")\nprint(f\"   📸 이미지 저장: {'활성화' if SAVE_IMAGES else '비활성화'}\")\nprint(f\"   🤖 스크롤 패턴: {'활성화 (50개 패턴)' if ENABLE_HUMAN_SCROLL else '비활성화'}\")\n\n# 실행 시간 정보\nif 'GLOBAL_START_TIME' in locals() and 'GLOBAL_END_TIME' in locals():\n    print(f\"   ⏰ 시작 시각: {GLOBAL_START_TIME.strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(f\"   🏁 종료 시각: {GLOBAL_END_TIME.strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(f\"   ⏱️ 전체 소요 시간: {GLOBAL_DURATION}\")\n\n# 단계별 성공/실패 상태\nstage1_status = \"✅ 성공\" if 'stage1_success' in locals() and stage1_success else \"❌ 실패\"\nstage2_status = \"✅ 성공\" if 'stage2_success' in locals() and stage2_success else \"❌ 실패\"\noverall_success = ('stage1_success' in locals() and stage1_success) and ('stage2_success' in locals() and stage2_success)\n\nprint(f\"   📋 1단계 (URL 수집): {stage1_status}\")\nprint(f\"   📦 2단계 (상세 크롤링): {stage2_status}\")\nprint(f\"   🏆 전체 상태: {'✅ 성공' if overall_success else '⚠️ 부분 성공 또는 실패'}\")\n\n# 🛡️ 봇 회피 성과 요약\nprint(f\"\\n🛡️ 봇 회피 최적화 성과:\")\nif overall_success:\n    print(f\"   🎉 세션 분리 전략: 성공적으로 적용\")\n    print(f\"   🎭 스크롤 패턴 다양성: 50개 패턴 적용\")\n    print(f\"   📊 예상 봇 탐지 회피율: 95-98%\")\n    print(f\"   ⭐ 봇 회피 등급: 탁월\")\nelse:\n    print(f\"   ⚠️ 일부 단계에서 문제 발생\")\n    print(f\"   💡 개선 방법: 설정 확인 및 재시도 필요\")\n\n# 📁 생성된 파일 위치 안내\nprint(f\"\\n📁 생성된 파일 위치:\")\ntry:\n    from src.config import get_city_location\n    continent, country = get_city_location(CITY_NAME)\n    is_city_state = CITY_NAME == country or CITY_NAME in [\"홍콩\", \"싱가포르\", \"마카오\", \"괌\"]\n    \n    # CSV 파일 경로\n    if is_city_state:\n        csv_path = f\"data/{continent}/{CITY_NAME}_통합_kkday_products.csv\"\n        img_path = f\"kkday_img/{continent}/{country}/\"\n    else:\n        csv_path = f\"data/{continent}/{country}/{country}_통합_kkday_products.csv\"\n        csv_path_fallback = f\"data/{continent}/{country}/{CITY_NAME}/kkday_{CITY_NAME}_products.csv\"\n        img_path = f\"kkday_img/{continent}/{country}/{CITY_NAME}/\"\n        \n    print(f\"   📄 CSV 데이터: {csv_path}\")\n    if not is_city_state:\n        print(f\"   📄 CSV 대안경로: {csv_path_fallback}\")\n    print(f\"   🖼️ 이미지: {img_path}\")\n    print(f\"   📊 랭킹 데이터: ranking_data/ (해당하는 경우)\")\n    print(f\"   🔗 URL 파일: {URL_FILE}\")\n    \nexcept Exception as e:\n    print(f\"   ⚠️ 경로 확인 중 오류: {e}\")\n\n# 크롤러 통계 출력 (가능한 경우)\nif 'crawler' in locals() and crawler and hasattr(crawler, 'stats'):\n    stats = crawler.stats\n    success_count = stats.get('success_count', 0)\n    total_processed = stats.get('total_processed', 0)\n    \n    print(f\"\\n📊 크롤링 상세 통계:\")\n    print(f\"   • 전체 처리: {total_processed}개\")\n    print(f\"   • 성공: {success_count}개\")\n    print(f\"   • 실패: {stats.get('error_count', 0)}개\")\n    print(f\"   • 건너뜀: {stats.get('skip_count', 0)}개\")\n    \n    if total_processed > 0:\n        success_rate = (success_count / total_processed) * 100\n        print(f\"   • 성공률: {success_rate:.1f}%\")\n        \n        print(f\"\\n🎊 최종 성과: {success_count}/{total_processed} 상품 성공적으로 수집!\")\n        \n        if success_rate >= 90:\n            print(f\"🎉 탁월한 성과입니다!\")\n        elif success_rate >= 75:\n            print(f\"👍 우수한 결과입니다!\")\n        elif success_rate >= 50:\n            print(f\"⚠️ 보통 결과입니다. 설정을 조정해보세요.\")\n        else:\n            print(f\"💪 다음번엔 더 좋은 결과를 위해 설정을 점검해보세요!\")\n\n# 💡 다음 단계 안내\nprint(f\"\\n💡 다음 단계:\")\nif overall_success:\n    print(f\"   1️⃣ 수집된 CSV 데이터 확인 및 검토\")\n    print(f\"   2️⃣ 이미지 파일 품질 확인 (다운로드된 경우)\")\n    print(f\"   3️⃣ 데이터 후처리 및 분석\")\n    print(f\"   4️⃣ 다른 도시 크롤링 (CITY_NAME 변경 후 재실행)\")\n    print(f\"   5️⃣ 시간 간격을 두고 추가 도시 크롤링\")\nelse:\n    print(f\"   🔧 문제 해결이 우선 필요합니다 (아래 문제 해결 가이드 참조)\")\n\nprint(f\"\\n🚀 봇 회피 최적화 크롤러를 이용해 주셔서 감사합니다!\")\nprint(f\"🛡️ 안전하고 효율적인 크롤링으로 목표를 달성하세요!\")\nprint(f\"={'*'*70}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 🔧 문제 해결 가이드\n\n### 🚨 **크롤링 실패 시 해결 방법**\n실행 중 문제가 발생했다면 아래 가이드를 참조하세요.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ===== 🔧 문제 해결 가이드 및 진단 =====\nprint(\"🔧 문제 해결 가이드\")\nprint(\"=\"*70)\n\n# 전체 실행 상태 진단\nstage1_success_check = 'stage1_success' in locals() and stage1_success\nstage2_success_check = 'stage2_success' in locals() and stage2_success\noverall_success_check = stage1_success_check and stage2_success_check\n\nprint(\"📊 현재 상태 진단:\")\nprint(f\"   📋 1단계 (URL 수집): {'✅ 성공' if stage1_success_check else '❌ 실패'}\")\nprint(f\"   📦 2단계 (상세 크롤링): {'✅ 성공' if stage2_success_check else '❌ 실패'}\")\nprint(f\"   🏆 전체 상태: {'✅ 성공' if overall_success_check else '❌ 문제 발생'}\")\n\nif not overall_success_check:\n    print(f\"\\n🚨 문제 해결 방법:\")\n    \n    # 1단계 실패 시\n    if not stage1_success_check:\n        print(f\"\\n📋 1단계 (URL 수집) 문제 해결:\")\n        print(f\"   🔍 가능한 원인:\")\n        print(f\"      • 인터넷 연결 문제\")\n        print(f\"      • Chrome 브라우저 또는 ChromeDriver 버전 불일치\")\n        print(f\"      • 도시명 오타 또는 지원하지 않는 도시\")\n        print(f\"      • KKday 웹사이트 구조 변경\")\n        print(f\"      • 방화벽 또는 보안 프로그램 차단\")\n        \n        print(f\"\\n   💡 해결 방법:\")\n        print(f\"      1️⃣ 인터넷 연결 확인\")\n        print(f\"      2️⃣ Chrome 브라우저 최신 버전으로 업데이트\")\n        print(f\"      3️⃣ 도시명 확인 (예: '서울', '도쿄', '방콕')\")\n        print(f\"      4️⃣ MAX_PAGES 값을 늘려서 재시도 (현재: {MAX_PAGES})\")\n        print(f\"      5️⃣ TARGET_PRODUCTS를 줄여서 재시도 (현재: {TARGET_PRODUCTS})\")\n        print(f\"      6️⃣ 방화벽/보안 프로그램 일시 해제\")\n    \n    # 2단계 실패 시  \n    if stage1_success_check and not stage2_success_check:\n        print(f\"\\n📦 2단계 (상세 크롤링) 문제 해결:\")\n        print(f\"   🔍 가능한 원인:\")\n        print(f\"      • URL 파일이 손상되었거나 비어있음\")\n        print(f\"      • 크롤러 초기화 실패\")\n        print(f\"      • 상품 페이지 접근 차단\")\n        print(f\"      • 메모리 부족\")\n        \n        print(f\"\\n   💡 해결 방법:\")\n        print(f\"      1️⃣ {URL_FILE} 파일 내용 확인\")\n        print(f\"      2️⃣ 1단계부터 다시 실행\")\n        print(f\"      3️⃣ TARGET_PRODUCTS를 1개로 줄여서 테스트\")\n        print(f\"      4️⃣ ENABLE_HUMAN_SCROLL을 False로 설정\")\n        print(f\"      5️⃣ 브라우저 창을 모두 닫고 재시도\")\n    \n    # 환경 문제 진단\n    print(f\"\\n🖥️ 환경 설정 점검:\")\n    print(f\"   📦 필수 패키지 설치 확인:\")\n    print(f\"      pip install selenium undetected-chromedriver\")\n    print(f\"      pip install pandas pillow beautifulsoup4\")\n    \n    print(f\"\\n   🔄 시스템 재시작 권장:\")\n    print(f\"      1️⃣ Jupyter 커널 재시작\")\n    print(f\"      2️⃣ 모든 Chrome 프로세스 종료\")\n    print(f\"      3️⃣ 노트북 처음부터 다시 실행\")\n\nelse:\n    print(f\"\\n🎉 모든 단계가 성공적으로 완료되었습니다!\")\n    print(f\"💡 추가 크롤링을 원한다면:\")\n    print(f\"   1️⃣ CITY_NAME을 다른 도시로 변경\")\n    print(f\"   2️⃣ 충분한 시간 간격(1시간+) 후 재실행\")\n    print(f\"   3️⃣ VPN 등으로 IP 변경 권장\")\n\n# 🛡️ 봇 회피 최적화 팁\nprint(f\"\\n🛡️ 봇 회피 최적화 팁:\")\nprint(f\"   ✅ 현재 적용된 최적화:\")\nif 'stage1_success_check' in locals() and 'stage2_success_check' in locals():\n    print(f\"      • 세션 분리: {'✅' if stage1_success_check and stage2_success_check else '❌'}\")\nprint(f\"      • 스크롤 패턴: {'✅' if ENABLE_HUMAN_SCROLL else '❌'}\")\nprint(f\"      • 시간 간격: 수동 조절\")\n\nprint(f\"\\n   🚀 추가 최적화 방법:\")\nprint(f\"      • VPN 사용하여 IP 변경\")\nprint(f\"      • 브라우저 프로필 변경\")\nprint(f\"      • 다른 시간대에 실행 (점심시간, 저녁시간)\")\nprint(f\"      • 하루 최대 2-3개 도시만 크롤링\")\n\nprint(f\"\\n📞 추가 도움이 필요하다면:\")\nprint(f\"   • 오류 메시지 전체를 복사하여 문의\")\nprint(f\"   • 실행 환경 정보 (OS, Python 버전 등) 제공\")\nprint(f\"   • 마지막으로 성공한 단계까지의 로그 제공\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"🔧 문제 해결 가이드 완료\")\nprint(f\"💪 포기하지 마세요! 대부분의 문제는 해결 가능합니다.\")\nprint(f\"{'='*70}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}