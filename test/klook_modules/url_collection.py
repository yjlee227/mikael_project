"""
ğŸš€ ê·¸ë£¹ 8: KLOOK URL ìˆ˜ì§‘ ì‹œìŠ¤í…œ
- í˜ì´ì§€ë„¤ì´ì…˜ ê¸°ë°˜ URL ìˆ˜ì§‘
- Sitemap ë° ë¸Œë¼ìš°ì € ìˆ˜ì§‘ í†µí•©
- ë‹¤ì–‘í•œ ìˆ˜ì§‘ ì „ëµ ì§€ì›
"""

import os
import time
import random
import json
import re
from datetime import datetime
from urllib.parse import urlparse, urljoin, parse_qs, urlunparse

# ì¡°ê±´ë¶€ import
try:
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
    SELENIUM_AVAILABLE = True
except ImportError:
    print("âš ï¸ Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. URL ìˆ˜ì§‘ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
    SELENIUM_AVAILABLE = False

try:
    import requests
    from bs4 import BeautifulSoup
    REQUESTS_AVAILABLE = True
except ImportError:
    print("âš ï¸ requests ë˜ëŠ” beautifulsoup4ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Sitemap ìˆ˜ì§‘ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
    REQUESTS_AVAILABLE = False

# config ëª¨ë“ˆì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë“¤ import
from .config import CONFIG, get_city_code, get_city_info
from .url_manager import is_valid_klook_url, normalize_klook_url, save_urls_to_collection

# =============================================================================
# ğŸ” í˜ì´ì§€ë„¤ì´ì…˜ ê¸°ë°˜ URL ìˆ˜ì§‘
# =============================================================================

def collect_urls_with_pagination(driver, city_name, max_pages=10, strategy="smart"):
    """ğŸ” í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•œ ì²´ê³„ì  URL ìˆ˜ì§‘"""
    if not SELENIUM_AVAILABLE:
        return []
    
    print(f"ğŸ” '{city_name}' í˜ì´ì§€ë„¤ì´ì…˜ URL ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
    print(f"ğŸ“Š ì „ëµ: {strategy}")
    
    collected_urls = []
    current_page = 1
    
    while current_page <= max_pages:
        print(f"\nğŸ“„ í˜ì´ì§€ {current_page}/{max_pages} ìˆ˜ì§‘ ì¤‘...")
        
        try:
            # í˜„ì¬ í˜ì´ì§€ì—ì„œ URL ìˆ˜ì§‘ (ì¢Œí‘œ ê¸°ë°˜ ì •ë ¬)
            # KLOOK í˜ì´ì§€ì—ì„œëŠ” í˜ì´ì§€ë‹¹ 15ê°œ ì •ë„ì´ë¯€ë¡œ ì¶©ë¶„íˆ ìˆ˜ì§‘
            # ì„¤ì •ê°’ ì‚¬ìš© (ì „ì—­ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
            collect_limit = globals().get('MAX_COLLECT_LIMIT', 50)
            page_urls = collect_urls_from_current_page_by_coordinates(driver, limit=collect_limit)
            
            if not page_urls:
                print(f"    âš ï¸ í˜ì´ì§€ {current_page}ì—ì„œ URLì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                if strategy == "strict":
                    break  # strict ëª¨ë“œì—ì„œëŠ” ë¹ˆ í˜ì´ì§€ ë°œê²¬ì‹œ ì¤‘ë‹¨
                else:
                    current_page += 1
                    continue
            
            # ìœ íš¨í•œ KLOOK URLë§Œ í•„í„°ë§
            valid_urls = [url for url in page_urls if is_valid_klook_url(url)]
            collected_urls.extend(valid_urls)
            
            print(f"    âœ… í˜ì´ì§€ {current_page}: {len(valid_urls)}ê°œ URL ìˆ˜ì§‘")
            
            # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
            if current_page < max_pages:
                next_success = navigate_to_next_page(driver, current_page)
                if not next_success:
                    print(f"    âš ï¸ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ ì‹¤íŒ¨ - ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break
            
            current_page += 1
            
            # ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€ê¸°
            time.sleep(random.uniform(
                CONFIG.get("MEDIUM_MIN_DELAY", 2),
                CONFIG.get("MEDIUM_MAX_DELAY", 4)
            ))
            
        except Exception as e:
            print(f"    âŒ í˜ì´ì§€ {current_page} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            if strategy == "strict":
                break
            current_page += 1
            continue
    
    # ì¤‘ë³µ ì œê±°
    unique_urls = list(set(collected_urls))
    print(f"\nğŸ‰ í˜ì´ì§€ë„¤ì´ì…˜ ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"   ğŸ“Š ì´ {current_page - 1}í˜ì´ì§€ ì²˜ë¦¬")
    print(f"   ğŸ”— ìˆ˜ì§‘ëœ URL: {len(unique_urls)}ê°œ (ì¤‘ë³µ ì œê±° í›„)")
    
    return unique_urls

def collect_urls_from_current_page_by_coordinates(driver, limit=100):
    """í˜„ì¬ í˜ì´ì§€ì—ì„œ KLOOK URL ìˆ˜ì§‘ (ì¢Œí‘œ ê¸°ë°˜ ì •ë ¬ë¡œ ì‹œê°ì  ìˆœì„œ ë³´ì¥)"""
    if not SELENIUM_AVAILABLE:
        return []
    
    print(f"      ğŸ“ ì¢Œí‘œ ê¸°ë°˜ URL ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {limit}ê°œ)")
    
    # ë™ì  ë¡œë”© ëŒ€ê¸°
    try:
        print(f"      â±ï¸ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        WebDriverWait(driver, 10).until(
            lambda d: len(d.find_elements(By.CSS_SELECTOR, "a[href*='/activity/']")) > 0
        )
        print(f"      âœ… ë¡œë”© ì™„ë£Œ")
        time.sleep(3)  # ì¶©ë¶„í•œ ëŒ€ê¸°
    except Exception as e:
        print(f"      âš ï¸ ë¡œë”© ëŒ€ê¸° ì‹¤íŒ¨: {e}")
        time.sleep(2)
    
    # í˜ì´ì§€ ìŠ¤í¬ë¡¤ë¡œ ëª¨ë“  ìƒí’ˆ ë¡œë”©
    try:
        print(f"      ğŸ“œ ì „ì²´ ìƒí’ˆ ë¡œë”©ì„ ìœ„í•œ ìŠ¤í¬ë¡¤...")
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(1)
    except Exception as e:
        print(f"      âš ï¸ ìŠ¤í¬ë¡¤ ì‹¤íŒ¨: {e}")
    
    # ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¡œ ëª¨ë“  activity URL ìˆ˜ì§‘
    selectors = [
        "a[href*='/activity/']",
        ".result-card-list a[href*='/activity/']", 
        ".search-result-list a[href*='/activity/']",
        "[data-testid*='product'] a[href*='/activity/']",
        ".product-card a[href*='/activity/']"
    ]
    
    all_elements_with_coords = []
    
    for selector in selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            print(f"      ğŸ” '{selector}': {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
            
            for element in elements:
                try:
                    href = element.get_attribute('href')
                    if href and is_valid_klook_url(href):
                        normalized_url = normalize_klook_url(href)
                        
                        # ì¤‘ë³µ ì²´í¬
                        if any(item['url'] == normalized_url for item in all_elements_with_coords):
                            continue
                        
                        # ìš”ì†Œê°€ í™”ë©´ì— ë³´ì´ëŠ”ì§€ í™•ì¸
                        if element.is_displayed():
                            location = element.location
                            y_coord = location.get('y', 0)
                            x_coord = location.get('x', 0)
                            
                            # í™”ë©´ì— ì‹¤ì œë¡œ í‘œì‹œëœ ìš”ì†Œë§Œ ì„ íƒ
                            if y_coord > 0:
                                all_elements_with_coords.append({
                                    'url': normalized_url,
                                    'y': y_coord,
                                    'x': x_coord,
                                    'element': element
                                })
                        
                except Exception:
                    continue
                    
        except Exception as e:
            print(f"      âš ï¸ ì…€ë ‰í„° '{selector}' ì‹¤íŒ¨: {e}")
            continue
    
    # ì¢Œí‘œë¡œ ì •ë ¬: Yì¢Œí‘œ ìš°ì„  (ìœ„â†’ì•„ë˜), ê°™ìœ¼ë©´ Xì¢Œí‘œ (ì™¼ìª½â†’ì˜¤ë¥¸ìª½)
    all_elements_with_coords.sort(key=lambda item: (item['y'], item['x']))
    
    # ì •ë ¬ëœ ìˆœì„œëŒ€ë¡œ URL ìˆ˜ì§‘
    collected_urls = []
    for i, item in enumerate(all_elements_with_coords[:limit]):
        collected_urls.append(item['url'])
        url_name = item['url'].split('/')[-1].replace('-', ' ')[:40]
        print(f"        ğŸ“ {i+1}ìœ„: Y={item['y']:4d} | {url_name}")
    
    print(f"      âœ… ì¢Œí‘œ ê¸°ë°˜ ì •ë ¬ ì™„ë£Œ: {len(collected_urls)}ê°œ URL")
    return collected_urls

def collect_urls_from_current_page(driver, limit=100):
    """í˜„ì¬ í˜ì´ì§€ì—ì„œ KLOOK URL ìˆ˜ì§‘ (í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ)"""
    if not SELENIUM_AVAILABLE:
        return []
    
    collected_urls = []
    
    # KLOOK ìƒí’ˆ URL ì…€ë ‰í„°ë“¤ (í˜ì´ì§€ ìˆœì„œ ë³´ì¥)
    # ì‹¤ì œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì •í™•í•œ ì…€ë ‰í„°
    url_selectors = [
        # ğŸ¯ KLOOK 2024 êµ¬ì¡° ê¸°ë°˜ - result-card-list ë‚´ë¶€ ìˆœì„œ ë³´ì¥
        ".result-card-list a[href*='/activity/']",
        ".search-result-list a[href*='/activity/']",
        
        # ê¸°ì¡´ KLOOK ìµœì í™” ì…€ë ‰í„°ë“¤ (ë°±ì—…)
        "[data-testid*='product'] a[href*='/activity/']",
        ".product-card a[href*='/activity/']",
        ".activity-card a[href*='/activity/']",
        "[class*='card'] a[href*='/activity/']",
        
        # ìˆœì„œ ë³´ì¥ ì…€ë ‰í„°ë“¤ (DOM íŠ¸ë¦¬ ìˆœì„œëŒ€ë¡œ)
        "div[class*='product'] a[href*='/activity/'], div[class*='item'] a[href*='/activity/'], div[class*='card'] a[href*='/activity/']",
        
        # ì¼ë°˜ì ì¸ KLOOK íŒ¨í„´ (ìˆœì„œ ì¤‘ìš”)
        "a[href*='/activity/']",
        "a[href*='/ko/activity/']",
        "a[href*='/en/activity/']",
        
        # ë°±ì—… ì…€ë ‰í„°ë“¤
        "a[href*='klook.com'][href*='activity']",
        "[class*='product'] a",
        "[class*='item'] a",
        ".list-item a"
    ]
    
    print(f"      ğŸ“ í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ URL ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {limit}ê°œ)")
    
    # ë™ì  ë¡œë”© ëŒ€ê¸° (KLOOK í˜ì´ì§€ ìµœì í™” - result-card-list ê¸°ë°˜)
    try:
        print(f"      â±ï¸ í˜ì´ì§€ ë™ì  ë¡œë”© ëŒ€ê¸° ì¤‘...")
        WebDriverWait(driver, 10).until(
            lambda d: len(d.find_elements(By.CSS_SELECTOR, ".result-card-list a[href*='/activity/']")) > 0
        )
        print(f"      âœ… ë™ì  ë¡œë”© ì™„ë£Œ")
        # ì¶”ê°€ ì•ˆì •í™” ëŒ€ê¸°
        time.sleep(2)
    except Exception as e:
        print(f"      âš ï¸ ë™ì  ë¡œë”© ëŒ€ê¸° ì‹¤íŒ¨: {e}")
        # ë°±ì—… ëŒ€ê¸° - ì¼ë°˜ ì…€ë ‰í„°ë¡œ ì¬ì‹œë„
        try:
            WebDriverWait(driver, 5).until(
                lambda d: len(d.find_elements(By.CSS_SELECTOR, "a[href*='/activity/']")) > 0
            )
            print(f"      âœ… ë°±ì—… ë™ì  ë¡œë”© ì™„ë£Œ")
            time.sleep(2)
        except Exception as backup_e:
            print(f"      âš ï¸ ë°±ì—… ë™ì  ë¡œë”©ë„ ì‹¤íŒ¨: {backup_e}")
    
    for selector in url_selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            print(f"      ğŸ” '{selector}': {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
            
            temp_urls = []
            for i, element in enumerate(elements):
                try:
                    href = element.get_attribute('href')
                    if href and is_valid_klook_url(href):
                        normalized_url = normalize_klook_url(href)
                        if normalized_url not in temp_urls and normalized_url not in collected_urls:
                            temp_urls.append(normalized_url)
                            print(f"        ğŸ“ ìˆœì„œ {i+1}: {normalized_url.split('/')[-1][:50]}...")
                            
                        if len(temp_urls) >= limit:
                            break
                            
                except Exception:
                    continue
            
            if temp_urls:
                collected_urls.extend(temp_urls[:limit])
                print(f"      âœ… ì…€ë ‰í„°ì—ì„œ {len(temp_urls)}ê°œ URL ìˆ˜ì§‘ (í˜ì´ì§€ ìˆœì„œ ë³´ì¥)")
                break  # ì²« ë²ˆì§¸ ì„±ê³µí•œ ì…€ë ‰í„°ì—ì„œ ìˆ˜ì§‘ ì™„ë£Œ
                
        except Exception as e:
            print(f"      âš ï¸ ì…€ë ‰í„° '{selector}' ì‹¤íŒ¨: {e}")
            continue
    
    print(f"      ğŸ“Š ìµœì¢… ìˆ˜ì§‘: {len(collected_urls)}ê°œ URL (ìˆœì„œ ë³´ì¥)")
    return collected_urls[:limit]

def navigate_to_next_page(driver, current_page):
    """ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ (í†µí•© í˜ì´ì§€ë„¤ì´ì…˜ ë§¤ë‹ˆì € ì‚¬ìš©)"""
    if not SELENIUM_AVAILABLE:
        return False
    
    try:
        from .pagination_utils import KlookPageTool
        
        # í…ŒìŠ¤íŠ¸ ê²€ì¦ëœ KLOOK í˜ì´ì§€ ë„êµ¬ ì‚¬ìš©
        page_tool = KlookPageTool(driver)
        
        # ë¶€ë“œëŸ¬ìš´ ìŠ¤í¬ë¡¤ë¡œ í˜ì´ì§€ë„¤ì´ì…˜ ì˜ì—­ ì°¾ê¸°
        page_tool.smooth_scroll_to_pagination()
        
        # ê³ ê¸‰ ë‹¤ìŒ í˜ì´ì§€ í´ë¦­
        current_url = driver.current_url
        result = page_tool.click_next_page(current_url)
        
        if result['success']:
            print(f"    âœ… í˜ì´ì§€ {current_page + 1}ë¡œ ì´ë™ ì„±ê³µ (ë°©ë²•: {result['method']})")
            return True
        else:
            print(f"    âŒ í˜ì´ì§€ {current_page + 1}ë¡œ ì´ë™ ì‹¤íŒ¨")
            return False
            
    except Exception as e:
        print(f"    âŒ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
        return False

# =============================================================================
# ğŸ—ºï¸ Sitemap ê¸°ë°˜ URL ìˆ˜ì§‘
# =============================================================================

def collect_urls_from_sitemap(city_name, limit=1000):
    """Sitemapì—ì„œ KLOOK URL ìˆ˜ì§‘"""
    if not REQUESTS_AVAILABLE:
        print("âŒ requests/beautifulsoup4ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    
    print(f"ğŸ—ºï¸ '{city_name}' Sitemap URL ìˆ˜ì§‘ ì‹œì‘...")
    
    # KLOOK sitemap URLë“¤
    sitemap_urls = [
        "https://www.klook.com/sitemap.xml",
        "https://www.klook.com/sitemap-activities.xml",
        "https://www.klook.com/sitemap-ko.xml",
        f"https://www.klook.com/sitemap-{city_name.lower()}.xml"
    ]
    
    collected_urls = []
    
    for sitemap_url in sitemap_urls:
        try:
            print(f"  ğŸ“‹ Sitemap ì²˜ë¦¬ ì¤‘: {sitemap_url}")
            
            response = requests.get(sitemap_url, timeout=30, headers={
                'User-Agent': CONFIG.get("USER_AGENT", "Mozilla/5.0")
            })
            
            if response.status_code == 200:
                # XML íŒŒì‹±
                soup = BeautifulSoup(response.content, 'xml')
                
                # URL ì¶”ì¶œ
                urls = soup.find_all('url')
                for url_element in urls:
                    loc = url_element.find('loc')
                    if loc and loc.text:
                        url = loc.text.strip()
                        
                        # KLOOK activity URL í•„í„°ë§
                        if is_valid_klook_url(url):
                            # ë„ì‹œëª…ê³¼ ê´€ë ¨ëœ URL í•„í„°ë§ (ì„ íƒì )
                            if city_name.lower() in url.lower() or len(collected_urls) < limit:
                                normalized_url = normalize_klook_url(url)
                                if normalized_url not in collected_urls:
                                    collected_urls.append(normalized_url)
                                    
                                    if len(collected_urls) >= limit:
                                        break
                
                print(f"    âœ… {len([u for u in collected_urls])}ê°œ URL ë°œê²¬")
                
            else:
                print(f"    âš ï¸ Sitemap ì ‘ê·¼ ì‹¤íŒ¨: HTTP {response.status_code}")
                
        except Exception as e:
            print(f"    âŒ Sitemap ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue
    
    unique_urls = list(set(collected_urls))
    print(f"ğŸ‰ Sitemap ìˆ˜ì§‘ ì™„ë£Œ: {len(unique_urls)}ê°œ URL (ì¤‘ë³µ ì œê±° í›„)")
    
    return unique_urls[:limit]

# =============================================================================
# ğŸ”„ í†µí•© URL ìˆ˜ì§‘ ì‹œìŠ¤í…œ
# =============================================================================

def execute_comprehensive_url_collection(driver, city_name, strategy="hybrid"):
    """í†µí•© URL ìˆ˜ì§‘ ì‹œìŠ¤í…œ ì‹¤í–‰"""
    print(f"ğŸš€ '{city_name}' í†µí•© URL ìˆ˜ì§‘ ì‹œì‘!")
    print(f"ğŸ“Š ì „ëµ: {strategy}")
    print("=" * 80)
    
    all_collected_urls = []
    collection_results = {}
    
    # ì „ëµë³„ ìˆ˜ì§‘ ì‹¤í–‰
    if strategy == "browser_only":
        # ë¸Œë¼ìš°ì € ìˆ˜ì§‘ë§Œ
        print("\nğŸŒ ë¸Œë¼ìš°ì € ê¸°ë°˜ ìˆ˜ì§‘ ì‹¤í–‰...")
        browser_urls = collect_urls_with_pagination(driver, city_name, max_pages=5)
        all_collected_urls.extend(browser_urls)
        collection_results["browser"] = len(browser_urls)
        
    elif strategy == "sitemap_only":
        # Sitemap ìˆ˜ì§‘ë§Œ
        print("\nğŸ—ºï¸ Sitemap ê¸°ë°˜ ìˆ˜ì§‘ ì‹¤í–‰...")
        sitemap_urls = collect_urls_from_sitemap(city_name, limit=500)
        all_collected_urls.extend(sitemap_urls)
        collection_results["sitemap"] = len(sitemap_urls)
        
    elif strategy == "hybrid":
        # í•˜ì´ë¸Œë¦¬ë“œ (ë¸Œë¼ìš°ì € + Sitemap)
        print("\nğŸ”€ í•˜ì´ë¸Œë¦¬ë“œ ìˆ˜ì§‘ ì‹¤í–‰...")
        
        # 1. ë¸Œë¼ìš°ì € ìˆ˜ì§‘ (ìˆœìœ„ ì •ë³´ í¬í•¨)
        print("\n  ğŸŒ 1ë‹¨ê³„: ë¸Œë¼ìš°ì € ìˆ˜ì§‘...")
        browser_urls = collect_urls_with_pagination(driver, city_name, max_pages=3)
        all_collected_urls.extend(browser_urls)
        collection_results["browser"] = len(browser_urls)
        
        # 2. Sitemap ìˆ˜ì§‘ (ëŒ€ëŸ‰ URL)
        print("\n  ğŸ—ºï¸ 2ë‹¨ê³„: Sitemap ìˆ˜ì§‘...")
        sitemap_urls = collect_urls_from_sitemap(city_name, limit=1000)
        all_collected_urls.extend(sitemap_urls)
        collection_results["sitemap"] = len(sitemap_urls)
        
    elif strategy == "comprehensive":
        # í¬ê´„ì  ìˆ˜ì§‘ (ëª¨ë“  ë°©ë²• ì‚¬ìš©)
        print("\nâš¡ í¬ê´„ì  ìˆ˜ì§‘ ì‹¤í–‰...")
        
        # 1. ë¸Œë¼ìš°ì € ìˆ˜ì§‘
        print("\n  ğŸŒ 1ë‹¨ê³„: ë¸Œë¼ìš°ì € ìˆ˜ì§‘...")
        browser_urls = collect_urls_with_pagination(driver, city_name, max_pages=10)
        all_collected_urls.extend(browser_urls)
        collection_results["browser"] = len(browser_urls)
        
        # 2. Sitemap ìˆ˜ì§‘
        print("\n  ğŸ—ºï¸ 2ë‹¨ê³„: Sitemap ìˆ˜ì§‘...")
        sitemap_urls = collect_urls_from_sitemap(city_name, limit=2000)
        all_collected_urls.extend(sitemap_urls)
        collection_results["sitemap"] = len(sitemap_urls)
        
        # 3. ê²€ìƒ‰ ê¸°ë°˜ ìˆ˜ì§‘ (ì„ íƒì )
        try:
            print("\n  ğŸ” 3ë‹¨ê³„: ê²€ìƒ‰ ê¸°ë°˜ ìˆ˜ì§‘...")
            search_urls = collect_urls_from_search(driver, city_name)
            all_collected_urls.extend(search_urls)
            collection_results["search"] = len(search_urls)
        except Exception as e:
            print(f"    âš ï¸ ê²€ìƒ‰ ê¸°ë°˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            collection_results["search"] = 0
    
    # ì¤‘ë³µ ì œê±° ë° ìœ íš¨ì„± ê²€ì¦
    print(f"\nğŸ§¹ ì¤‘ë³µ ì œê±° ë° ìœ íš¨ì„± ê²€ì¦...")
    unique_urls = []
    for url in all_collected_urls:
        if is_valid_klook_url(url):
            normalized_url = normalize_klook_url(url)
            if normalized_url not in unique_urls:
                unique_urls.append(normalized_url)
    
    # ê²°ê³¼ ì €ì¥
    if unique_urls:
        save_success = save_urls_to_collection(unique_urls, city_name, f"collection_{strategy}")
        print(f"ğŸ’¾ URL ì €ì¥: {'ì„±ê³µ' if save_success else 'ì‹¤íŒ¨'}")
    
    # ê²°ê³¼ ì •ë¦¬
    print(f"\nğŸ‰ === '{city_name}' URL ìˆ˜ì§‘ ì™„ë£Œ ===")
    print(f"ğŸ“Š ì „ëµ: {strategy}")
    print(f"ğŸ”— ìµœì¢… ìˆ˜ì§‘ URL: {len(unique_urls)}ê°œ (ì¤‘ë³µ ì œê±° í›„)")
    print(f"ğŸ“ˆ ìˆ˜ì§‘ ìƒì„¸:")
    for source, count in collection_results.items():
        print(f"   ğŸ“‹ {source}: {count}ê°œ")
    
    return {
        "success": True,
        "strategy": strategy,
        "total_collected": len(unique_urls),
        "urls": unique_urls,
        "source_breakdown": collection_results,
        "city_name": city_name
    }

def collect_urls_from_search(driver, city_name):
    """ê²€ìƒ‰ ê¸°ë°˜ URL ìˆ˜ì§‘"""
    if not SELENIUM_AVAILABLE:
        return []
    
    print(f"  ğŸ” '{city_name}' ê²€ìƒ‰ ê¸°ë°˜ URL ìˆ˜ì§‘...")
    
    try:
        # ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™
        search_url = f"https://www.klook.com/ko/search/result/?query={city_name}"
        driver.get(search_url)
        time.sleep(random.uniform(3, 5))
        
        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ URL ìˆ˜ì§‘
        # ì„¤ì •ê°’ ì‚¬ìš© (ì „ì—­ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        collect_limit = globals().get('MAX_COLLECT_LIMIT', 50)
        search_urls = collect_urls_from_current_page(driver, limit=collect_limit)
        
        print(f"    âœ… ê²€ìƒ‰ì—ì„œ {len(search_urls)}ê°œ URL ìˆ˜ì§‘")
        return search_urls
        
    except Exception as e:
        print(f"    âŒ ê²€ìƒ‰ ê¸°ë°˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

# =============================================================================
# ğŸ“Š URL ìˆ˜ì§‘ ë¶„ì„ ë° í†µê³„
# =============================================================================

def analyze_collection_results(results):
    """URL ìˆ˜ì§‘ ê²°ê³¼ ë¶„ì„"""
    if not results or not results.get("urls"):
        return {"error": "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
    
    urls = results["urls"]
    
    analysis = {
        "total_urls": len(urls),
        "unique_activity_ids": set(),
        "url_patterns": {},
        "domain_distribution": {},
        "language_distribution": {"ko": 0, "en": 0, "other": 0}
    }
    
    for url in urls:
        # Activity ID ì¶”ì¶œ
        activity_match = re.search(r'/activity/(\d+)', url)
        if activity_match:
            analysis["unique_activity_ids"].add(activity_match.group(1))
        
        # ë„ë©”ì¸ ë¶„ì„
        parsed = urlparse(url)
        domain = parsed.netloc
        analysis["domain_distribution"][domain] = analysis["domain_distribution"].get(domain, 0) + 1
        
        # ì–¸ì–´ ë¶„ì„
        if '/ko/' in url:
            analysis["language_distribution"]["ko"] += 1
        elif '/en/' in url:
            analysis["language_distribution"]["en"] += 1
        else:
            analysis["language_distribution"]["other"] += 1
    
    analysis["unique_activities"] = len(analysis["unique_activity_ids"])
    analysis["unique_activity_ids"] = list(analysis["unique_activity_ids"])
    
    return analysis


# =============================================================================
# ğŸŒ€ ìŠ¤í¬ë¡¤ íŒ¨í„´ ì‹œìŠ¤í…œ (ì›ë³¸ì—ì„œ ëˆ„ë½ëœ ê¸°ëŠ¥)
# =============================================================================

def human_like_scroll_patterns(driver):
    """ìˆœìˆ˜ ìŠ¤í¬ë¡¤ íŒ¨í„´ë§Œ (ì¸ê°„ê³¼ ìœ ì‚¬í•œ ìì—°ìŠ¤ëŸ¬ìš´ íŒ¨í„´) - í•­ìƒ ì‹¤í–‰"""
    
    patterns = [
        # íŒ¨í„´ 1: ëŠë¦° ë…ì„œ ìŠ¤íƒ€ì¼
        {"type": "slow_reading", "scrolls": 3, "pause": (2, 4), "step": 300},
        # íŒ¨í„´ 2: ë¹ ë¥¸ ìŠ¤ìº”
        {"type": "quick_scan", "scrolls": 5, "pause": (0.5, 1.5), "step": 500},
        # íŒ¨í„´ 3: ë¹„êµ ê²€í†  (ìœ„ì•„ë˜ ì›€ì§ì„)
        {"type": "comparison", "scrolls": 4, "pause": (1, 3), "step": 200},
        # íŒ¨í„´ 4: ì •ë°€ ê²€í† 
        {"type": "detailed", "scrolls": 6, "pause": (1.5, 3), "step": 150},
        # íŒ¨í„´ 5: ìì—°ìŠ¤ëŸ¬ìš´ íƒìƒ‰
        {"type": "natural", "scrolls": 4, "pause": (2, 5), "step": 400}
    ]
    
    # ëœë¤ íŒ¨í„´ ì„ íƒ
    pattern = random.choice(patterns)
    print(f"    ğŸŒ€ ìŠ¤í¬ë¡¤ íŒ¨í„´: {pattern['type']}")
    
    try:
        for i in range(pattern["scrolls"]):
            # ìŠ¤í¬ë¡¤ ì‹¤í–‰
            driver.execute_script(f"window.scrollBy(0, {pattern['step']});")
            
            # ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€ê¸°
            wait_time = random.uniform(pattern["pause"][0], pattern["pause"][1])
            time.sleep(wait_time)
            
            # ê°€ë” ì—­ë°©í–¥ ìŠ¤í¬ë¡¤ (ë¹„êµ ê²€í† )
            if pattern["type"] == "comparison" and random.random() < 0.3:
                driver.execute_script(f"window.scrollBy(0, -{pattern['step']//2});")
                time.sleep(random.uniform(0.5, 1.5))
                driver.execute_script(f"window.scrollBy(0, {pattern['step']});")
                
    except Exception as e:
        print(f"    âš ï¸ ìŠ¤í¬ë¡¤ íŒ¨í„´ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

def enhanced_scroll_patterns(driver):
    """í–¥ìƒëœ 5ê°€ì§€ ìŠ¤í¬ë¡¤ íŒ¨í„´ (í˜¸í™˜ì„± ê°œì„ ) - í•­ìƒ ì‹¤í–‰"""
    
    patterns = [
        {"name": "smooth_reading", "description": "ë¶€ë“œëŸ¬ìš´ ì½ê¸° íŒ¨í„´"},
        {"name": "comparison_scroll", "description": "ë¹„êµ ìŠ¤í¬ë¡¤"},
        {"name": "quick_scan", "description": "ë¹ ë¥¸ ìŠ¤ìº”"},
        {"name": "detailed_review", "description": "ìƒì„¸ ê²€í† "},
        {"name": "natural_browse", "description": "ìì—°ìŠ¤ëŸ¬ìš´ íƒìƒ‰"}
    ]
    
    selected = random.choice(patterns)
    print(f"    ğŸŒ€ Enhanced íŒ¨í„´: {selected['name']} - {selected['description']}")
    
    try:
        if selected["name"] == "smooth_reading":
            # ë¶€ë“œëŸ¬ìš´ ì½ê¸° íŒ¨í„´
            for _ in range(4):
                driver.execute_script("window.scrollBy(0, 250);")
                time.sleep(random.uniform(2, 4))
                
        elif selected["name"] == "comparison_scroll":
            # ë¹„êµ ìŠ¤í¬ë¡¤ (ìœ„ì•„ë˜ ë°˜ë³µ)
            for _ in range(3):
                driver.execute_script("window.scrollBy(0, 400);")
                time.sleep(random.uniform(1, 2))
                driver.execute_script("window.scrollBy(0, -200);")
                time.sleep(random.uniform(0.5, 1))
                driver.execute_script("window.scrollBy(0, 300);")
                time.sleep(random.uniform(1.5, 3))
                
        elif selected["name"] == "quick_scan":
            # ë¹ ë¥¸ ìŠ¤ìº”
            for _ in range(6):
                driver.execute_script("window.scrollBy(0, 600);")
                time.sleep(random.uniform(0.3, 1))
                
        elif selected["name"] == "detailed_review":
            # ìƒì„¸ ê²€í†  (ì‘ì€ ë‹¨ìœ„ë¡œ ì²œì²œíˆ)
            for _ in range(8):
                driver.execute_script("window.scrollBy(0, 150);")
                time.sleep(random.uniform(1.5, 3))
                
        elif selected["name"] == "natural_browse":
            # ìì—°ìŠ¤ëŸ¬ìš´ íƒìƒ‰ (ë¶ˆê·œì¹™í•œ íŒ¨í„´)
            scroll_amounts = [200, 350, 180, 450, 280, 320]
            for amount in scroll_amounts:
                driver.execute_script(f"window.scrollBy(0, {amount});")
                time.sleep(random.uniform(1, 4))
                
    except Exception as e:
        print(f"    âš ï¸ Enhanced ìŠ¤í¬ë¡¤ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

def smart_scroll_selector(driver):
    """ìŠ¤ë§ˆíŠ¸ ìŠ¤í¬ë¡¤ ì„ íƒê¸° (ë‘ í•¨ìˆ˜ ì¤‘ ëœë¤ ì„ íƒ) - í•­ìƒ ì‹¤í–‰"""
    
    # 50% í™•ë¥ ë¡œ ê°ê° ì„ íƒ
    if random.random() < 0.5:
        print("    ğŸ¯ ì„ íƒëœ ìŠ¤í¬ë¡¤: human_like_scroll_patterns")
        human_like_scroll_patterns(driver)
    else:
        print("    ğŸ¯ ì„ íƒëœ ìŠ¤í¬ë¡¤: enhanced_scroll_patterns")
        enhanced_scroll_patterns(driver)

def collect_with_single_scan(driver):
    """í†µí•© ëª¨ë“œ: ìë™ ìŠ¤í¬ë¡¤ê³¼ í•¨ê»˜ URL ìˆ˜ì§‘ (ì¡°ê±´ ì œê±°)"""
    
    print("    ğŸ“‹ í†µí•© ëª¨ë“œ: ìë™ ìŠ¤í¬ë¡¤ + URL ìˆ˜ì§‘")
    
    all_urls = []
    
    # 1. ì²« ë²ˆì§¸ ìˆ˜ì§‘
    initial_urls = collect_urls_from_current_page(driver, limit=30)
    all_urls.extend(initial_urls)
    
    # 2. ìë™ ìŠ¤í¬ë¡¤ ì‹¤í–‰ (í•­ìƒ ì ìš©)
    smart_scroll_selector(driver)
    
    # 3. ìŠ¤í¬ë¡¤ í›„ ì¶”ê°€ ìˆ˜ì§‘
    # ì„¤ì •ê°’ ì‚¬ìš© (ì „ì—­ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
    collect_limit = globals().get('MAX_COLLECT_LIMIT', 50)
    additional_urls = collect_urls_from_current_page(driver, limit=collect_limit)
    all_urls.extend(additional_urls)
    
    # 4. ì¤‘ë³µ ì œê±°
    unique_urls = list(set(all_urls))
    print(f"    âœ… í†µí•© ìˆ˜ì§‘ ì™„ë£Œ: {len(unique_urls)}ê°œ URL")
    
    return unique_urls

def collect_with_infinite_scroll(driver):
    """ë ˆê±°ì‹œ í˜¸í™˜ì„±: collect_with_single_scanê³¼ ë™ì¼í•˜ê²Œ ì‘ë™"""
    return collect_with_single_scan(driver)

# =============================================================================
# â±ï¸ ëŒ€ê¸°/íƒ€ì´ë° ì‹œìŠ¤í…œ (ì›ë³¸ì—ì„œ ëˆ„ë½ëœ ê¸°ëŠ¥)
# =============================================================================

def smart_wait_for_page_load(driver, max_wait=8):
    """ë™ì  ëŒ€ê¸°ì‹œê°„ (í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ ê°ì§€)"""
    if not SELENIUM_AVAILABLE:
        return
    
    print(f"    â±ï¸ ìŠ¤ë§ˆíŠ¸ í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° (ìµœëŒ€ {max_wait}ì´ˆ)")
    
    try:
        # JavaScriptë¥¼ í†µí•œ í˜ì´ì§€ ë¡œë“œ ìƒíƒœ í™•ì¸
        WebDriverWait(driver, max_wait).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # ì¶”ê°€ë¡œ DOM ìš”ì†Œ ë¡œë”© ëŒ€ê¸°
        time.sleep(random.uniform(1, 2))
        print("    âœ… í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
        
    except TimeoutException:
        print(f"    âš ï¸ í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼ ({max_wait}ì´ˆ)")
    except Exception as e:
        print(f"    âš ï¸ í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° ì‹¤íŒ¨: {e}")

def wait_for_page_ready(driver, timeout=10):
    """í˜ì´ì§€ê°€ ì™„ì „íˆ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°"""
    if not SELENIUM_AVAILABLE:
        return
    
    try:
        # í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
        WebDriverWait(driver, timeout).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # jQueryê°€ ìˆë‹¤ë©´ AJAX ì™„ë£Œ ëŒ€ê¸°
        try:
            WebDriverWait(driver, 3).until(
                lambda d: d.execute_script("return typeof jQuery !== 'undefined' ? jQuery.active == 0 : true")
            )
        except:
            pass
        
        print("    âœ… í˜ì´ì§€ ì¤€ë¹„ ì™„ë£Œ")
        
    except Exception as e:
        print(f"    âš ï¸ í˜ì´ì§€ ì¤€ë¹„ ëŒ€ê¸° ì‹¤íŒ¨: {e}")

def adaptive_wait(base_time):
    """ì ì‘í˜• ëŒ€ê¸° ì‹œê°„ (ì‹œìŠ¤í…œ ë¶€í•˜ì— ë”°ë¼ ì¡°ì •)"""
    try:
        # ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ì— ëœë¤ ìš”ì†Œ ì¶”ê°€
        actual_wait = base_time * random.uniform(0.8, 1.2)
        
        # ì‹œìŠ¤í…œ ë¶€í•˜ì— ë”°ë¥¸ ì¶”ê°€ ì¡°ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        import psutil
        cpu_percent = psutil.cpu_percent(interval=0.1)
        if cpu_percent > 80:
            actual_wait *= 1.5  # ë†’ì€ CPU ì‚¬ìš©ë¥ ì¼ ë•Œ ë” ê¸´ ëŒ€ê¸°
        elif cpu_percent < 30:
            actual_wait *= 0.8  # ë‚®ì€ CPU ì‚¬ìš©ë¥ ì¼ ë•Œ ì§§ì€ ëŒ€ê¸°
            
    except ImportError:
        # psutilì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ëœë¤ ëŒ€ê¸°
        actual_wait = base_time * random.uniform(0.8, 1.2)
    except:
        actual_wait = base_time
    
    time.sleep(actual_wait)
    return actual_wait

def safe_tab_operation(driver, operation_func, *args, **kwargs):
    """ì•ˆì „í•œ íƒ­ ì‘ì—… ìˆ˜í–‰"""
    if not SELENIUM_AVAILABLE:
        return False
    
    max_retries = 3
    for attempt in range(max_retries):
        try:
            result = operation_func(driver, *args, **kwargs)
            return result
        except Exception as e:
            print(f"    âš ï¸ íƒ­ ì‘ì—… ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
            if attempt < max_retries - 1:
                time.sleep(2)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
            else:
                print("    âŒ íƒ­ ì‘ì—… ì™„ì „ ì‹¤íŒ¨")
                return False
    
    return False

print("âœ… ê·¸ë£¹ 8 ì™„ë£Œ: KLOOK URL ìˆ˜ì§‘ ì‹œìŠ¤í…œ!")
print("   ğŸ” í˜ì´ì§€ë„¤ì´ì…˜ ìˆ˜ì§‘:")
print("   - collect_urls_with_pagination(): ì²´ê³„ì  í˜ì´ì§€ ìˆœíšŒ")
print("   - collect_urls_from_current_page(): í˜„ì¬ í˜ì´ì§€ URL ìˆ˜ì§‘")
print("   - navigate_to_next_page(): ë‹¤ìŒ í˜ì´ì§€ ì´ë™")
print("   ğŸ—ºï¸ Sitemap ìˆ˜ì§‘:")
print("   - collect_urls_from_sitemap(): Sitemap ê¸°ë°˜ ëŒ€ëŸ‰ ìˆ˜ì§‘")
print("   ğŸ”„ í†µí•© ì‹œìŠ¤í…œ:")
print("   - execute_comprehensive_url_collection(): í†µí•© ìˆ˜ì§‘ ì‹¤í–‰")
print("   - collect_urls_from_search(): ê²€ìƒ‰ ê¸°ë°˜ ìˆ˜ì§‘")
print("   ğŸŒ€ ìŠ¤í¬ë¡¤ íŒ¨í„´ (ì¶”ê°€ë¨):")
print("   - human_like_scroll_patterns(): ì¸ê°„ì  ìŠ¤í¬ë¡¤ íŒ¨í„´")
print("   - enhanced_scroll_patterns(): í–¥ìƒëœ 5ê°€ì§€ íŒ¨í„´")
print("   - smart_scroll_selector(): ìŠ¤ë§ˆíŠ¸ íŒ¨í„´ ì„ íƒ")
print("   - collect_with_single_scan(): ê¸°ë³¸ ìŠ¤ìº” ëª¨ë“œ")
print("   - collect_with_infinite_scroll(): ë¬´í•œ ìŠ¤í¬ë¡¤ ëª¨ë“œ")
print("   â±ï¸ ëŒ€ê¸°/íƒ€ì´ë° (ì¶”ê°€ë¨):")
print("   - smart_wait_for_page_load(): ë™ì  í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°")
print("   - wait_for_page_ready(): í˜ì´ì§€ ì¤€ë¹„ ì™„ë£Œ ëŒ€ê¸°")
print("   - adaptive_wait(): ì ì‘í˜• ëŒ€ê¸° ì‹œê°„")
print("   - safe_tab_operation(): ì•ˆì „í•œ íƒ­ ì‘ì—…")
print("   ğŸ“Š ë¶„ì„ ë„êµ¬:")
print("   - analyze_collection_results(): ìˆ˜ì§‘ ê²°ê³¼ ë¶„ì„")
print("   ğŸ¯ ì§€ì› ì „ëµ: browser_only, sitemap_only, hybrid, comprehensive")