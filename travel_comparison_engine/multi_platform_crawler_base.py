"""
ğŸŒ ë‹¤ì¤‘ í”Œë«í¼ í¬ë¡¤ëŸ¬ ê¸°ë³¸ êµ¬ì¡°
- KKday, GetYourGuide, MyRealTrip í¬ë¡¤ëŸ¬ ê¸°ë³¸ í‹€
- í†µí•© ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ë³´ì¥
- í”Œë«í¼ë³„ íŠ¹í™” ë¡œì§ ë¶„ë¦¬

ì‘ì„±ì¼: 2024-08-24
ê¸°ë°˜: í†µí•© ìŠ¤í‚¤ë§ˆ & KLOOK í¬ë¡¤ëŸ¬ êµ¬ì¡°
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import json
import hashlib
from datetime import datetime
import re


class BasePlatformCrawler(ABC):
    """ëª¨ë“  í”Œë«í¼ í¬ë¡¤ëŸ¬ì˜ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, driver: webdriver.Chrome, wait_timeout: int = 10):
        """
        ê¸°ë³¸ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            driver: Selenium WebDriver ì¸ìŠ¤í„´ìŠ¤
            wait_timeout: ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        """
        self.driver = driver
        self.wait = WebDriverWait(driver, wait_timeout)
        self.platform_name = self.get_platform_name()
        self.base_selectors = self.get_platform_selectors()
        
    @abstractmethod
    def get_platform_name(self) -> str:
        """í”Œë«í¼ ì´ë¦„ ë°˜í™˜"""
        pass
    
    @abstractmethod
    def get_platform_selectors(self) -> Dict[str, List[str]]:
        """í”Œë«í¼ë³„ CSS/XPath ì…€ë ‰í„° ì •ì˜"""
        pass
    
    @abstractmethod
    def get_search_url(self, city: str, **kwargs) -> str:
        """ê²€ìƒ‰ URL ìƒì„±"""
        pass
    
    @abstractmethod
    def extract_product_list(self) -> List[Dict[str, Any]]:
        """ìƒí’ˆ ëª©ë¡ ì¶”ì¶œ"""
        pass
    
    @abstractmethod
    def extract_product_details(self, product_url: str) -> Dict[str, Any]:
        """ê°œë³„ ìƒí’ˆ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        pass
    
    def normalize_to_unified_schema(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì›ì‹œ ë°ì´í„°ë¥¼ í†µí•© ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜"""
        
        # ìƒí’ˆ í•´ì‹œ ìƒì„±
        core_fields = [
            str(raw_data.get('title', '')),
            str(raw_data.get('url', '')),
            str(raw_data.get('price', ''))
        ]
        product_hash = hashlib.sha1(''.join(core_fields).encode()).hexdigest()
        
        return {
            # â­ í•„ìˆ˜ ì‹ë³„ ì •ë³´
            "provider": self.platform_name,
            "provider_product_id": self.extract_product_id(raw_data.get('url', '')),
            "fetch_ts": datetime.utcnow().isoformat() + "Z",
            "fx_rate": None,  # TODO: í™˜ìœ¨ API
            
            # â­ í•„ìˆ˜ ëª©ì ì§€/ë¶„ë¥˜  
            "destination_city": raw_data.get('city', ''),
            "country": raw_data.get('country', ''),
            "theme_tags": json.dumps(raw_data.get('themes', []), ensure_ascii=False),
            
            # â­ í•„ìˆ˜ ìƒí’ˆ ì •ë³´
            "title": raw_data.get('title', ''),
            "subtitle": raw_data.get('subtitle', ''),
            "supplier_name": raw_data.get('supplier', ''),
            "duration_hours": self.parse_duration(raw_data.get('duration', '')),
            "pickup": 1 if raw_data.get('pickup', False) else 0,
            "language": json.dumps(raw_data.get('languages', ['en']), ensure_ascii=False),
            
            # í¬í•¨/ë¶ˆí¬í•¨
            "included": json.dumps(raw_data.get('included', []), ensure_ascii=False),
            "excluded": json.dumps(raw_data.get('excluded', []), ensure_ascii=False),
            "meeting_point": raw_data.get('meeting_point', ''),
            
            # â­ í•„ìˆ˜ ê°€ê²© ì •ë³´
            "price_value": self.parse_price(raw_data.get('price', '')),
            "price_currency": self.normalize_currency(raw_data.get('currency', 'USD')),
            "option_list": json.dumps(raw_data.get('options', []), ensure_ascii=False),
            "price_basis": raw_data.get('price_basis', 'adult'),
            
            # í‰ì /ë¦¬ë·°
            "rating_value": self.normalize_rating(raw_data.get('rating', '')),
            "rating_count": raw_data.get('review_count', 0),
            
            # ì·¨ì†Œ/í™˜ë¶ˆ ì •ì±…
            "cancel_policy": json.dumps(raw_data.get('cancel_policy', {}), ensure_ascii=False),
            
            # ê°€ìš©ì„±
            "availability_calendar": json.dumps(raw_data.get('availability', []), ensure_ascii=False),
            
            # ë…¸ì¶œ/ìˆœìœ„
            "rank_position": raw_data.get('rank', 999),
            
            # â­ í•„ìˆ˜ ë§í¬/ì´ë¯¸ì§€
            "landing_url": raw_data.get('url', ''),
            "affiliate_url": self.generate_affiliate_url(raw_data.get('url', '')),
            "images": json.dumps(raw_data.get('images', []), ensure_ascii=False),
            
            # ë©”íƒ€ë°ì´í„°
            "product_hash": product_hash,
            "data_source_meta": json.dumps({
                "crawler_version": "1.0.0",
                "platform": self.platform_name,
                "raw_fields": list(raw_data.keys())
            }, ensure_ascii=False)
        }
    
    # ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    def extract_product_id(self, url: str) -> str:
        """URLì—ì„œ ìƒí’ˆ ID ì¶”ì¶œ (í”Œë«í¼ë³„ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)"""
        try:
            # ì¼ë°˜ì ì¸ íŒ¨í„´ ì‹œë„
            match = re.search(r'/(\d+)', url)
            if match:
                return match.group(1)
            return hashlib.md5(url.encode()).hexdigest()[:12]
        except:
            return hashlib.md5(str(url).encode()).hexdigest()[:12]
    
    def parse_duration(self, duration_str: str) -> Optional[float]:
        """ì†Œìš”ì‹œê°„ íŒŒì‹±"""
        if not duration_str:
            return None
        try:
            duration_str = str(duration_str).lower()
            if 'hour' in duration_str or 'ì‹œê°„' in duration_str:
                match = re.search(r'(\d+)', duration_str)
                return float(match.group(1)) if match else None
            elif 'day' in duration_str or 'ì¼' in duration_str:
                match = re.search(r'(\d+)', duration_str)
                return float(match.group(1)) * 24 if match else None
            return None
        except:
            return None
    
    def parse_price(self, price_str: str) -> float:
        """ê°€ê²© íŒŒì‹±"""
        try:
            price_clean = re.sub(r'[^\d.]', '', str(price_str))
            return float(price_clean) if price_clean else 0.0
        except:
            return 0.0
    
    def normalize_currency(self, currency: str) -> str:
        """í†µí™” ì •ê·œí™”"""
        currency_map = {
            "$": "USD", "dollar": "USD", "usd": "USD",
            "â‚¬": "EUR", "euro": "EUR", "eur": "EUR", 
            "Â£": "GBP", "pound": "GBP", "gbp": "GBP",
            "Â¥": "JPY", "yen": "JPY", "jpy": "JPY",
            "â‚©": "KRW", "won": "KRW", "krw": "KRW"
        }
        return currency_map.get(currency.lower(), currency.upper())
    
    def normalize_rating(self, rating_str: str) -> Optional[float]:
        """í‰ì ì„ 0~5 ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™”"""
        try:
            rating = float(str(rating_str).replace(',', ''))
            if 0 <= rating <= 5:
                return round(rating, 2)
            elif 5 < rating <= 10:
                return round(rating / 2, 2)
            elif 10 < rating <= 100:
                return round(rating / 20, 2)
            return None
        except:
            return None
    
    def generate_affiliate_url(self, original_url: str) -> Optional[str]:
        """ì œíœ´ URL ìƒì„± (í”Œë«í¼ë³„ ì˜¤ë²„ë¼ì´ë“œ)"""
        return None  # ê¸°ë³¸ì ìœ¼ë¡œ None, ê° í”Œë«í¼ì—ì„œ êµ¬í˜„


class KKdayCrawler(BasePlatformCrawler):
    """KKday í”Œë«í¼ í¬ë¡¤ëŸ¬"""
    
    def get_platform_name(self) -> str:
        return "KKday"
    
    def get_platform_selectors(self) -> Dict[str, List[str]]:
        return {
            "product_cards": [
                ".product-card",
                "[data-testid='product-card']",
                ".product-item"
            ],
            "product_title": [
                ".product-name", 
                ".card-title",
                "h3", "h4"
            ],
            "product_price": [
                ".price",
                ".product-price", 
                "[data-testid='price']"
            ],
            "product_rating": [
                ".rating",
                ".score",
                "[data-testid='rating']"
            ],
            "product_link": [
                "a[href*='/product/']",
                ".product-link"
            ],
            "next_page": [
                ".pagination-next",
                "[aria-label='Next']",
                ".next-page"
            ]
        }
    
    def get_search_url(self, city: str, **kwargs) -> str:
        """KKday ê²€ìƒ‰ URL ìƒì„±"""
        base_url = "https://www.kkday.com"
        # KKdayëŠ” ë„ì‹œë³„ URL êµ¬ì¡°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
        return f"{base_url}/ko/product/productlist/{city}"
    
    def extract_product_list(self) -> List[Dict[str, Any]]:
        """KKday ìƒí’ˆ ëª©ë¡ ì¶”ì¶œ"""
        products = []
        
        try:
            # ìƒí’ˆ ì¹´ë“œ ìš”ì†Œë“¤ ì°¾ê¸°
            card_selectors = self.base_selectors["product_cards"]
            product_cards = []
            
            for selector in card_selectors:
                try:
                    cards = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if cards:
                        product_cards = cards
                        break
                except:
                    continue
            
            for i, card in enumerate(product_cards[:20], 1):  # ìƒìœ„ 20ê°œë§Œ
                try:
                    product_data = {
                        'rank': i,
                        'title': self._extract_text_from_card(card, self.base_selectors["product_title"]),
                        'price': self._extract_text_from_card(card, self.base_selectors["product_price"]),
                        'rating': self._extract_text_from_card(card, self.base_selectors["product_rating"]),
                        'url': self._extract_link_from_card(card),
                        'platform': self.platform_name
                    }
                    products.append(product_data)
                except Exception as e:
                    print(f"âš ï¸ KKday ìƒí’ˆ {i} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
                    
        except Exception as e:
            print(f"âŒ KKday ìƒí’ˆ ëª©ë¡ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return products
    
    def extract_product_details(self, product_url: str) -> Dict[str, Any]:
        """KKday ìƒí’ˆ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        details = {}
        
        try:
            self.driver.get(product_url)
            time.sleep(2)
            
            # TODO: KKday ìƒì„¸ í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ ë¡œì§
            details = {
                'url': product_url,
                'title': self._safe_extract(".product-title", "h1"),
                'subtitle': self._safe_extract(".product-subtitle"),
                'price': self._safe_extract(".price-current", ".price"),
                'currency': 'USD',  # KKday ê¸°ë³¸ í†µí™” (ì§€ì—­ì— ë”°ë¼ ë‹¤ë¦„)
                'rating': self._safe_extract(".rating-score"),
                'review_count': self._safe_extract(".review-count"),
                'duration': self._safe_extract(".duration"),
                'included': [],  # TODO: í¬í•¨í•­ëª© íŒŒì‹±
                'excluded': [],  # TODO: ë¶ˆí¬í•¨í•­ëª© íŒŒì‹±
                'images': self._extract_images(),
                'supplier': self._safe_extract(".supplier-name"),
                'meeting_point': self._safe_extract(".meeting-point")
            }
            
        except Exception as e:
            print(f"âŒ KKday ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return details
    
    def _extract_text_from_card(self, card, selectors: List[str]) -> str:
        """ì¹´ë“œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        for selector in selectors:
            try:
                element = card.find_element(By.CSS_SELECTOR, selector)
                return element.text.strip()
            except:
                continue
        return ""
    
    def _extract_link_from_card(self, card) -> str:
        """ì¹´ë“œì—ì„œ ë§í¬ ì¶”ì¶œ"""
        try:
            link = card.find_element(By.CSS_SELECTOR, "a")
            href = link.get_attribute("href")
            if href and not href.startswith("http"):
                href = f"https://www.kkday.com{href}"
            return href
        except:
            return ""
    
    def _safe_extract(self, *selectors) -> str:
        """ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        for selector in selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                return element.text.strip()
            except:
                continue
        return ""
    
    def _extract_images(self) -> List[str]:
        """ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        images = []
        try:
            img_elements = self.driver.find_elements(By.CSS_SELECTOR, ".product-images img, .gallery img")
            for img in img_elements[:5]:  # ìƒìœ„ 5ê°œë§Œ
                src = img.get_attribute("src") or img.get_attribute("data-src")
                if src:
                    images.append(src)
        except:
            pass
        return images


class GetYourGuideCrawler(BasePlatformCrawler):
    """GetYourGuide í”Œë«í¼ í¬ë¡¤ëŸ¬"""
    
    def get_platform_name(self) -> str:
        return "GetYourGuide"
    
    def get_platform_selectors(self) -> Dict[str, List[str]]:
        return {
            "product_cards": [
                "[data-testid='activity-card']",
                ".activity-card",
                ".product-card"
            ],
            "product_title": [
                "[data-testid='activity-card-title']",
                ".activity-title",
                "h3"
            ],
            "product_price": [
                "[data-testid='activity-card-price']", 
                ".price-from",
                ".price"
            ],
            "product_rating": [
                "[data-testid='activity-card-rating']",
                ".rating"
            ],
            "next_page": [
                "[aria-label='Go to next page']",
                ".pagination-next"
            ]
        }
    
    def get_search_url(self, city: str, **kwargs) -> str:
        """GetYourGuide ê²€ìƒ‰ URL ìƒì„±"""
        return f"https://www.getyourguide.com/s/?q={city}&sort=popularity"
    
    def extract_product_list(self) -> List[Dict[str, Any]]:
        """GetYourGuide ìƒí’ˆ ëª©ë¡ ì¶”ì¶œ (KKdayì™€ ìœ ì‚¬í•˜ì§€ë§Œ ì…€ë ‰í„° ë‹¤ë¦„)"""
        # KKdayì™€ ìœ ì‚¬í•œ êµ¬ì¡°ì§€ë§Œ GetYourGuide íŠ¹í™” ì…€ë ‰í„° ì‚¬ìš©
        # TODO: êµ¬ì²´ì ì¸ êµ¬í˜„
        return []
    
    def extract_product_details(self, product_url: str) -> Dict[str, Any]:
        """GetYourGuide ìƒí’ˆ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        # TODO: GetYourGuide íŠ¹í™” êµ¬í˜„
        return {}


class MyRealTripCrawler(BasePlatformCrawler):
    """MyRealTrip í”Œë«í¼ í¬ë¡¤ëŸ¬ (í•œêµ­ì–´ íŠ¹í™”)"""
    
    def get_platform_name(self) -> str:
        return "MyRealTrip"
    
    def get_platform_selectors(self) -> Dict[str, List[str]]:
        return {
            "product_cards": [
                ".product-item",
                ".tour-card", 
                "[data-testid='product']"
            ],
            "product_title": [
                ".tour-name",
                ".product-title",
                "h3", "h4"
            ],
            "product_price": [
                ".price-info",
                ".tour-price",
                ".price"
            ],
            "product_rating": [
                ".rating",
                ".review-score"
            ],
            "next_page": [
                ".btn-next",
                ".pagination-next",
                "[aria-label='ë‹¤ìŒ']"
            ]
        }
    
    def get_search_url(self, city: str, **kwargs) -> str:
        """MyRealTrip ê²€ìƒ‰ URL ìƒì„± (í•œêµ­ì–´)"""
        return f"https://www.myrealtrip.com/search?keyword={city}&sort=popular"
    
    def extract_product_list(self) -> List[Dict[str, Any]]:
        """MyRealTrip ìƒí’ˆ ëª©ë¡ ì¶”ì¶œ (í•œêµ­ì–´ íŠ¹í™”)"""
        # TODO: MyRealTrip íŠ¹í™” êµ¬í˜„ (í•œêµ­ì–´ ì²˜ë¦¬ í¬í•¨)
        return []
    
    def extract_product_details(self, product_url: str) -> Dict[str, Any]:
        """MyRealTrip ìƒí’ˆ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        # TODO: MyRealTrip íŠ¹í™” êµ¬í˜„
        return {}


class MultiPlatformCrawlerManager:
    """ë‹¤ì¤‘ í”Œë«í¼ í¬ë¡¤ëŸ¬ í†µí•© ê´€ë¦¬ì"""
    
    def __init__(self, driver: webdriver.Chrome):
        """
        ë‹¤ì¤‘ í”Œë«í¼ í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        
        Args:
            driver: ê³µìœ  WebDriver ì¸ìŠ¤í„´ìŠ¤
        """
        self.driver = driver
        self.crawlers = {
            "Klook": None,  # ê¸°ì¡´ KLOOK í¬ë¡¤ëŸ¬ ì—°ë™
            "KKday": KKdayCrawler(driver),
            "GetYourGuide": GetYourGuideCrawler(driver),
            "MyRealTrip": MyRealTripCrawler(driver)
        }
    
    def crawl_all_platforms(self, city: str, max_products_per_platform: int = 20) -> Dict[str, List[Dict]]:
        """ëª¨ë“  í”Œë«í¼ì—ì„œ ìƒí’ˆ ìˆ˜ì§‘"""
        all_results = {}
        
        for platform_name, crawler in self.crawlers.items():
            if crawler is None:
                continue
                
            try:
                print(f"ğŸ”„ {platform_name} í¬ë¡¤ë§ ì‹œì‘...")
                
                # ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™
                search_url = crawler.get_search_url(city)
                self.driver.get(search_url)
                time.sleep(3)
                
                # ìƒí’ˆ ëª©ë¡ ì¶”ì¶œ
                products = crawler.extract_product_list()
                
                # í†µí•© ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜
                unified_products = []
                for product in products[:max_products_per_platform]:
                    try:
                        unified_data = crawler.normalize_to_unified_schema(product)
                        unified_products.append(unified_data)
                    except Exception as e:
                        print(f"   âš ï¸ {platform_name} ìƒí’ˆ ë³€í™˜ ì‹¤íŒ¨: {e}")
                        continue
                
                all_results[platform_name] = unified_products
                print(f"   âœ… {platform_name}: {len(unified_products)}ê°œ ìƒí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")
                
            except Exception as e:
                print(f"   âŒ {platform_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                all_results[platform_name] = []
        
        return all_results
    
    def get_crawler(self, platform: str) -> Optional[BasePlatformCrawler]:
        """íŠ¹ì • í”Œë«í¼ í¬ë¡¤ëŸ¬ ë°˜í™˜"""
        return self.crawlers.get(platform)


def create_multi_platform_manager(driver: webdriver.Chrome) -> MultiPlatformCrawlerManager:
    """ë‹¤ì¤‘ í”Œë«í¼ ë§¤ë‹ˆì € ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    return MultiPlatformCrawlerManager(driver)


if __name__ == "__main__":
    print("ğŸŒ ë‹¤ì¤‘ í”Œë«í¼ í¬ë¡¤ëŸ¬ ê¸°ë³¸ êµ¬ì¡°")
    print("   âœ… ì¶”ìƒí™”ëœ ê¸°ë³¸ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤")
    print("   âœ… KKday í¬ë¡¤ëŸ¬ ê¸°ë³¸ êµ¬í˜„")
    print("   âœ… GetYourGuide í¬ë¡¤ëŸ¬ í‹€")
    print("   âœ… MyRealTrip í¬ë¡¤ëŸ¬ í‹€") 
    print("   âœ… í†µí•© ìŠ¤í‚¤ë§ˆ ìë™ ë³€í™˜")
    print("   âœ… ë‹¤ì¤‘ í”Œë«í¼ ë§¤ë‹ˆì €")