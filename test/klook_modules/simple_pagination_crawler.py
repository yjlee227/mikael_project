#!/usr/bin/env python3
"""
ğŸš€ ë‹¨ìˆœí™”ëœ í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ëŸ¬
í•µì‹¬ ê¸°ëŠ¥ë§Œ ë‚¨ê¸´ ê°„ë‹¨í•œ í˜ì´ì§€ë„¤ì´ì…˜ ê¸°ë°˜ í¬ë¡¤ë§ ì‹œìŠ¤í…œ
"""

import os
import time
from datetime import datetime
from selenium.webdriver.common.by import By

class SimplePaginationCrawler:
    """ë‹¨ìˆœí™”ëœ í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, driver):
        self.driver = driver
        self.collected_urls = []
    
    def crawl_with_pagination(self, city_name, target_count=15, max_pages=5):
        """í˜ì´ì§€ë„¤ì´ì…˜ ê¸°ë°˜ í¬ë¡¤ë§ - í•µì‹¬ ê¸°ëŠ¥ë§Œ"""
        print(f"ğŸš€ '{city_name}' í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ë§ ì‹œì‘")
        print(f"ğŸ¯ ëª©í‘œ: {target_count}ê°œ ìƒí’ˆ, ìµœëŒ€ {max_pages}í˜ì´ì§€")
        
        all_urls = []
        current_page = 1
        current_rank = 1
        
        while len(all_urls) < target_count and current_page <= max_pages:
            print(f"\nğŸ“– í˜ì´ì§€ {current_page} ì²˜ë¦¬ ì¤‘...")
            
            # 1. í˜„ì¬ í˜ì´ì§€ì—ì„œ URL ìˆ˜ì§‘
            page_urls = self._get_urls_from_page()
            
            if not page_urls:
                print("âŒ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - ì¢…ë£Œ")
                break
            
            # 2. í•„ìš”í•œ ë§Œí¼ë§Œ ì¶”ê°€ (ìˆœìœ„ ìˆœì„œ ë³´ì¥)
            remaining = target_count - len(all_urls)
            for i, url in enumerate(page_urls[:remaining]):
                all_urls.append({
                    'url': url,
                    'rank': current_rank,
                    'page': current_page
                })
                current_rank += 1
            
            print(f"âœ… ìˆ˜ì§‘: {len(page_urls[:remaining])}ê°œ URL ({len(all_urls)}/{target_count})")
            
            # 3. ëª©í‘œ ë‹¬ì„±ì‹œ ì¢…ë£Œ
            if len(all_urls) >= target_count:
                break
            
            # 4. ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
            if not self._go_to_next_page():
                print("âŒ ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨ - ì¢…ë£Œ")
                break
                
            current_page += 1
            time.sleep(2)
        
        # 5. ê²°ê³¼ ì €ì¥
        if all_urls:
            self._save_results(all_urls, city_name)
        
        print(f"\nâœ… ì™„ë£Œ: {len(all_urls)}ê°œ URL ìˆ˜ì§‘ (1ìœ„~{len(all_urls)}ìœ„)")
        return all_urls
    
    def _get_urls_from_page(self):
        """í˜„ì¬ í˜ì´ì§€ì—ì„œ URL ìˆ˜ì§‘"""
        try:
            time.sleep(2)  # ë¡œë”© ëŒ€ê¸°
            
            # KLOOK activity URL ì°¾ê¸°
            elements = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/activity/']")
            
            urls = []
            seen_urls = set()
            
            for element in elements:
                try:
                    url = element.get_attribute('href')
                    if url and '/activity/' in url and url not in seen_urls:
                        # í™”ë©´ì— ë³´ì´ëŠ” ìš”ì†Œë§Œ (ìˆœìœ„ ìˆœì„œ ë³´ì¥)
                        location = element.location
                        if location.get('y', 0) > 0:
                            urls.append((url, location['y']))
                            seen_urls.add(url)
                except:
                    continue
            
            # Y ì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìœ„ì—ì„œ ì•„ë˜ë¡œ = ìˆœìœ„ ìˆœì„œ)
            urls.sort(key=lambda x: x[1])
            return [url for url, _ in urls]
            
        except Exception as e:
            print(f"âŒ URL ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def _go_to_next_page(self):
        """ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ (í†µí•© í˜ì´ì§€ë„¤ì´ì…˜ ë§¤ë‹ˆì € ì‚¬ìš©)"""
        try:
            from .pagination_utils import KlookPageTool
            
            # í…ŒìŠ¤íŠ¸ ê²€ì¦ëœ KLOOK í˜ì´ì§€ ë„êµ¬ ì‚¬ìš©
            page_tool = KlookPageTool(self.driver)
            
            # ë¶€ë“œëŸ¬ìš´ ìŠ¤í¬ë¡¤ë¡œ í˜ì´ì§€ë„¤ì´ì…˜ ì˜ì—­ ì°¾ê¸°
            page_tool.smooth_scroll_to_pagination()
            
            # ê³ ê¸‰ ë‹¤ìŒ í˜ì´ì§€ í´ë¦­
            current_url = self.driver.current_url
            result = page_tool.click_next_page(current_url)
            
            return result['success']
            
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
            return False
    
    def _save_results(self, urls, city_name):
        """ê²°ê³¼ ì €ì¥ - ìƒˆë¡œìš´ í†µí•© ì„¸ì…˜ êµ¬ì¡°"""
        try:
            from .config import get_city_code
            city_code = get_city_code(city_name)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ì„¸ì…˜ í´ë” ìƒì„±
            session_dir = f"crawl_sessions/{city_code}_{timestamp}"
            os.makedirs(session_dir, exist_ok=True)
            
            # 1. ì„¸ì…˜ ì •ë³´ ì €ì¥
            session_info = {
                "city_name": city_name,
                "city_code": city_code,
                "session_date": datetime.now().strftime("%Y-%m-%d"),
                "session_time": datetime.now().strftime("%H:%M:%S"),
                "crawling_method": "simple_pagination",
                "total_urls_found": len(urls),
                "status": "url_collection_completed"
            }
            
            import json
            with open(f"{session_dir}/session_info.json", 'w', encoding='utf-8') as f:
                json.dump(session_info, f, ensure_ascii=False, indent=2)
            
            # 2. URL ëª©ë¡ ì €ì¥ (ê°„ë‹¨í•œ í…ìŠ¤íŠ¸)
            with open(f"{session_dir}/url_list.txt", 'w', encoding='utf-8') as f:
                f.write(f"# {city_name} í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ë§ ê²°ê³¼\n")
                f.write(f"# ìˆ˜ì§‘ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# ì´ {len(urls)}ê°œ URL (ìˆœìœ„ë³„)\n\n")
                
                for url_data in urls:
                    f.write(f"{url_data['rank']:2d}ìœ„ | í˜ì´ì§€{url_data['page']} | {url_data['url']}\n")
            
            # 3. ìƒì„¸ ë°ì´í„° ì €ì¥ (JSON)
            detailed_data = {
                "session_info": session_info,
                "urls_with_ranking": []
            }
            
            for url_data in urls:
                detailed_data["urls_with_ranking"].append({
                    "rank": url_data['rank'],
                    "page": url_data['page'],
                    "url": url_data['url'],
                    "collected_at": datetime.now().isoformat()
                })
            
            with open(f"{session_dir}/ranking_details.json", 'w', encoding='utf-8') as f:
                json.dump(detailed_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ í†µí•© ì„¸ì…˜ ì €ì¥: {session_dir}/")
            print(f"   ğŸ“Š session_info.json: ê¸°ë³¸ ì •ë³´")
            print(f"   ğŸ“ url_list.txt: URL ëª©ë¡")
            print(f"   ğŸ“‹ ranking_details.json: ìƒì„¸ ë°ì´í„°")
            
        except Exception as e:
            print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")

class SimplePaginationSystem:
    """ë‹¨ìˆœí™”ëœ ì „ì²´ ì‹œìŠ¤í…œ"""
    
    def __init__(self, driver):
        self.driver = driver
        self.crawler = SimplePaginationCrawler(driver)
    
    def run_full_crawl(self, city_name, target_count=15, max_pages=5):
        """ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰"""
        print("ğŸš€ ë‹¨ìˆœí™”ëœ í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ë§ ì‹œìŠ¤í…œ")
        print("=" * 50)
        
        start_time = time.time()
        
        try:
            # 0. ë„ì‹œëª… ë³„ì¹­ ì²˜ë¦¬
            from .city_alias_system import smart_city_search, get_search_variations
            
            print(f"ğŸ” ë„ì‹œëª… ê²€ì¦: '{city_name}'")
            city_result = smart_city_search(city_name)
            
            if city_result['success']:
                standard_city = city_result['standard']
                search_variations = get_search_variations(standard_city)
                print(f"âœ… í‘œì¤€ ë„ì‹œëª…: '{standard_city}'")
                print(f"ğŸ”„ ê²€ìƒ‰ ë³€í˜•: {search_variations}")
            else:
                print(f"âš ï¸ ë„ì‹œëª… ê²€ì¦ ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: '{city_name}'")
                standard_city = city_name
                search_variations = [city_name]
            
            # 1. KLOOK ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™ (ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©)
            from .driver_manager import go_to_main_page, find_and_fill_search, click_search_button
            
            print(f"ğŸŒ KLOOK ë©”ì¸ í˜ì´ì§€ ì´ë™...")
            go_to_main_page(self.driver)
            
            # 2. ì—¬ëŸ¬ ê²€ìƒ‰ ë³€í˜•ìœ¼ë¡œ ì‹œë„
            search_success = False
            for i, search_term in enumerate(search_variations, 1):
                print(f"ğŸ” ê²€ìƒ‰ ì‹œë„ {i}/{len(search_variations)}: '{search_term}'")
                
                find_and_fill_search(self.driver, search_term)
                click_search_button(self.driver)
                time.sleep(5)
                
                # ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
                if self._check_search_results():
                    print(f"âœ… ê²€ìƒ‰ ì„±ê³µ: '{search_term}'")
                    search_success = True
                    break
                else:
                    print(f"âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: '{search_term}'")
                    if i < len(search_variations):
                        print("ğŸ”„ ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì¬ì‹œë„...")
                        time.sleep(2)
            
            if not search_success:
                print(f"âŒ ëª¨ë“  ê²€ìƒ‰ ë³€í˜• ì‹¤íŒ¨")
                return False
            
            # 3. í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ë§ (í‘œì¤€ ë„ì‹œëª… ì‚¬ìš©)
            collected_urls = self.crawler.crawl_with_pagination(
                standard_city, target_count, max_pages
            )
            
            # 3. ì‹¤ì œ í¬ë¡¤ë§ (ê¸°ì¡´ í¬ë¡¤ëŸ¬ ì‚¬ìš©)
            if collected_urls:
                print(f"\nğŸ”¥ ìƒì„¸ í¬ë¡¤ë§ ì‹œì‘...")
                success_count = self._crawl_products(collected_urls, city_name)
                
                print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
                print(f"   URL ìˆ˜ì§‘: {len(collected_urls)}ê°œ")
                print(f"   í¬ë¡¤ë§ ì„±ê³µ: {success_count}ê°œ")
                print(f"   ì†Œìš”ì‹œê°„: {int((time.time() - start_time)//60)}ë¶„")
                
                return True
            else:
                print("âŒ URL ìˆ˜ì§‘ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return False
    
    def _check_search_results(self):
        """ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        try:
            # KLOOK activity ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
            elements = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/activity/']")
            return len(elements) > 0
        except:
            return False
    
    def _crawl_products(self, urls, city_name):
        """ìƒí’ˆ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§"""
        try:
            from .crawler_engine import KlookCrawlerEngine
            
            crawler_engine = KlookCrawlerEngine(self.driver)
            success_count = 0
            
            for i, url_data in enumerate(urls, 1):
                url = url_data['url']
                rank = url_data['rank']
                
                print(f"\nğŸ“Š ì§„í–‰ë¥ : {i}/{len(urls)} | {rank}ìœ„")
                print(f"ğŸ”— URL: {url[:60]}...")
                
                try:
                    # ìƒí’ˆ í˜ì´ì§€ë¡œ ì´ë™
                    self.driver.get(url)
                    time.sleep(3)
                    
                    # ìƒí’ˆ ì •ë³´ ì¶”ì¶œ (ê¸°ì¡´ ì—”ì§„ ì‚¬ìš©)
                    result = crawler_engine._extract_product_info(url, city_name, i)
                    
                    if result:
                        # ë­í‚¹ ì •ë³´ ì¶”ê°€
                        result['íƒ­ëª…'] = 'ì „ì²´'
                        result['íƒ­ë‚´_ë­í‚¹'] = rank
                        result['í˜ì´ì§€'] = url_data['page']
                        
                        # CSV ì €ì¥ (ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©)
                        from .data_handler import save_to_csv_klook
                        if save_to_csv_klook(result, city_name):
                            product_name = result.get('ìƒí’ˆëª…', 'N/A')[:30]
                            print(f"   âœ… ì„±ê³µ: {product_name}...")
                            success_count += 1
                        else:
                            print(f"   âŒ CSV ì €ì¥ ì‹¤íŒ¨")
                    else:
                        print(f"   âŒ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨")
                        
                except Exception as e:
                    print(f"   ğŸ’¥ ì˜¤ë¥˜: {e}")
                    
                time.sleep(2)
            
            return success_count
            
        except Exception as e:
            print(f"âŒ ìƒì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return 0

# ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ í•¨ìˆ˜
def quick_pagination_crawl(driver, city_name, target_count=15):
    """ë¹ ë¥¸ í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ë§"""
    system = SimplePaginationSystem(driver)
    return system.run_full_crawl(city_name, target_count)

print("âœ… ë‹¨ìˆœí™”ëœ í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ëŸ¬ ë¡œë“œ ì™„ë£Œ!")
print("ğŸš€ ì‚¬ìš©ë²•: quick_pagination_crawl(driver, 'ë¡œë§ˆ', 15)")