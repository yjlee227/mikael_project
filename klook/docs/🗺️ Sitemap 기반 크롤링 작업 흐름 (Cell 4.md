ğŸ—ºï¸ Sitemap ê¸°ë°˜ í¬ë¡¤ë§ ì‘ì—… íë¦„ (Cell 4.5)

  ğŸ“‹ ì „ì²´ ì‘ì—… íë¦„:

  graph TD
      A[Cell 4.5 ì‹œì‘] --> B[ê¸°ì¡´ ìˆ˜ì§‘ URL ì •ë¦¬]
      B --> C[Sitemapì—ì„œ ìƒˆ URL ìˆ˜ì§‘]
      C --> D{ìƒˆ URL ìˆìŒ?}
      D -->|ì—†ìŒ| E[ì¢…ë£Œ]
      D -->|ìˆìŒ| F[ë“œë¼ì´ë²„ ì¬ì´ˆê¸°í™”]
      F --> G[Sitemap URL ë£¨í”„ ì‹œì‘]

      G --> H[URL ë°©ë¬¸ ì „ ì¤‘ë³µ ì²´í¬]
      H --> I{ì´ë¯¸ ì²˜ë¦¬ë¨?}
      I -->|ì˜ˆ| J[ê±´ë„ˆë›°ê¸° 0.001ì´ˆ]
      I -->|ì•„ë‹ˆì˜¤| K[í˜ì´ì§€ ë°©ë¬¸ 5-10ì´ˆ]

      J --> L[ë‹¤ìŒ URL]
      K --> M[ë°ì´í„° ì¶”ì¶œ]
      M --> N[ë²ˆí˜¸ í• ë‹¹]
      N --> O[ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ]
      O --> P[CSV ì €ì¥]
      P --> Q{ì €ì¥ ì„±ê³µ?}
      Q -->|ì„±ê³µ| R[ìºì‹œ ë§ˆí‚¹]
      Q -->|ì‹¤íŒ¨| S[ì‹¤íŒ¨ ì²˜ë¦¬]

      R --> L
      S --> L
      L --> T{ë” ë§ì€ URL?}
      T -->|ìˆìŒ| H
      T -->|ì—†ìŒ| U[ì™„ë£Œ]

  ğŸ” ìƒì„¸ ë‹¨ê³„ë³„ ë¶„ì„:

  1ë‹¨ê³„: ì´ˆê¸° ì¤€ë¹„ (1-2ì´ˆ)

  # ê¸°ì¡´ ìˆ˜ì§‘ URL ì •ë¦¬
  collected_urls = set()
  for item in ranking_data:
      collected_urls.add(item['url'])
  print(f"ğŸ“Š í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ URL: {len(collected_urls)}ê°œ")

  2ë‹¨ê³„: Sitemap URL ìˆ˜ì§‘ (3-5ì´ˆ)

  sitemap_urls = collect_urls_from_sitemap(
      city_name=CITY_NAME,
      exclude_urls=list(collected_urls),  # ì¤‘ë³µ ì œì™¸
      limit=200
  )
  print(f"ğŸ‰ Sitemapì—ì„œ {len(sitemap_urls)}ê°œ ìƒˆë¡œìš´ URL ë°œê²¬!")

  3ë‹¨ê³„: ë“œë¼ì´ë²„ ì¤€ë¹„ (2-3ì´ˆ)

  driver = setup_driver()
  print("âœ… ë“œë¼ì´ë²„ ì¬ì´ˆê¸°í™” ì„±ê³µ")

  4ë‹¨ê³„: URLë³„ ì²˜ë¦¬ ë£¨í”„

  í˜„ì¬ ë²„ì „ (ë¹„íš¨ìœ¨ì ):

  for i, url in enumerate(sitemap_urls):
      print(f"ğŸ” Sitemap {current_rank}ìœ„ í¬ë¡¤ë§ ì¤‘...")

      # âŒ ì¤‘ë³µ ì²´í¬ ì—†ì´ ë°”ë¡œ ë°©ë¬¸
      driver.get(url)  # 5-10ì´ˆ ì†Œìš”
      time.sleep(random.uniform(2, 4))

      # ë°ì´í„° ì¶”ì¶œ (5-10ì´ˆ)
      product_data = extract_all_product_data(driver, url, current_rank)

      # ë²ˆí˜¸ í• ë‹¹ ë° ì €ì¥
      next_num = get_next_product_number(CITY_NAME)
      base_data = create_product_data_structure(CITY_NAME, next_num, current_rank)

      # CSV ì €ì¥
      if save_to_csv_klook(base_data, CITY_NAME):
          # âŒ ìºì‹œ ë§ˆí‚¹ ì—†ìŒ
          print("âœ… ì €ì¥ ì™„ë£Œ")

  ê°œì„ ëœ ë²„ì „ (íš¨ìœ¨ì ):

  for i, url in enumerate(sitemap_urls):
      print(f"ğŸ” Sitemap {current_rank}ìœ„ í¬ë¡¤ë§ ì¤‘...")

      # âœ… 1. ë°©ë¬¸ ì „ ì´ˆê³ ì† ì¤‘ë³µ ì²´í¬ (0.001ì´ˆ)
      if is_url_processed_fast(url, CITY_NAME):
          print(f"â­ï¸ ì¤‘ë³µ URL ê±´ë„ˆë›°ê¸°")
          current_rank += 1
          continue

      # âœ… 2. ìƒˆë¡œìš´ URLë§Œ ë°©ë¬¸ (5-10ì´ˆ)
      driver.get(url)
      time.sleep(random.uniform(2, 4))

      # ë°ì´í„° ì¶”ì¶œ (5-10ì´ˆ)
      product_data = extract_all_product_data(driver, url, current_rank)

      # ë²ˆí˜¸ í• ë‹¹ ë° ì €ì¥
      next_num = get_next_product_number(CITY_NAME)
      base_data = create_product_data_structure(CITY_NAME, next_num, current_rank)

      # CSV ì €ì¥
      if save_to_csv_klook(base_data, CITY_NAME):
          # âœ… 3. ì„±ê³µ í›„ ìºì‹œ ë§ˆí‚¹ (0.001ì´ˆ)
          mark_url_processed_fast(url, CITY_NAME, next_num, current_rank)
          print("âœ… ì €ì¥ ë° ìºì‹œ ë§ˆí‚¹ ì™„ë£Œ")

  âš¡ ì„±ëŠ¥ ë¹„êµ:

  ì‹œë‚˜ë¦¬ì˜¤: Sitemapì—ì„œ 20ê°œ URL ë°œê²¬

  í˜„ì¬ ë²„ì „ (ì¤‘ë³µ ì²´í¬ ì—†ìŒ):

  1ì¼ì°¨: 20ê°œ URL Ã— 15ì´ˆ = 300ì´ˆ (5ë¶„)
  2ì¼ì°¨: 20ê°œ URL Ã— 15ì´ˆ = 300ì´ˆ (5ë¶„) â† ê°™ì€ URL ì¬ì²˜ë¦¬!
  3ì¼ì°¨: 20ê°œ URL Ã— 15ì´ˆ = 300ì´ˆ (5ë¶„) â† ë˜ ì¬ì²˜ë¦¬!

  ê°œì„  ë²„ì „ (ì¤‘ë³µ ì²´í¬ ìˆìŒ):

  1ì¼ì°¨: 20ê°œ URL Ã— 15ì´ˆ = 300ì´ˆ (5ë¶„)
  2ì¼ì°¨: 20ê°œ ì¤‘ë³µ Ã— 0.001ì´ˆ = 0.02ì´ˆ (ì¦‰ì‹œ ê±´ë„ˆë›°ê¸°!)
  3ì¼ì°¨: 20ê°œ ì¤‘ë³µ Ã— 0.001ì´ˆ = 0.02ì´ˆ (ì¦‰ì‹œ ê±´ë„ˆë›°ê¸°!)

  ğŸ¯ í•µì‹¬ ë¬¸ì œ:

  í˜„ì¬ Cell 4.5ëŠ”:
  - ğŸŒ ë§¤ë²ˆ ëª¨ë“  Sitemap URLì„ ì¬ë°©ë¬¸
  - ğŸ’¾ ê°™ì€ ë°ì´í„° ì¤‘ë³µ ì €ì¥ ìœ„í—˜
  - â° ì—„ì²­ë‚œ ì‹œê°„ ë‚­ë¹„
  - ğŸš« ìºì‹œ ì‹œìŠ¤í…œ ë¯¸í™œìš©

  ì´ê²ƒì´ ë°”ë¡œ ì¤‘ë³µ ì²´í¬ê°€ í•„ìˆ˜ì¸ ì´ìœ ì…ë‹ˆë‹¤! ğŸš¨