"""
URL ìˆ˜ì§‘ ë° ê´€ë¦¬ ì‹œìŠ¤í…œ
- KLOOK URL íŒ¨í„´ ê²€ì¦ ë° ìˆ˜ì§‘
- URL ì¤‘ë³µ ë°©ì§€ ì‹œìŠ¤í…œ
- URL ìƒíƒœ ê´€ë¦¬ ë° ì¶”ì 
"""

import os
import re
import hashlib
import json
import time
import random
from datetime import datetime
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

from ..config import CONFIG, get_city_code, is_url_processed_fast, mark_url_processed_fast, SELENIUM_AVAILABLE

# ì¡°ê±´ë¶€ import (sitemap ê¸°ëŠ¥ìš©)
try:
    import requests
    from bs4 import BeautifulSoup
    REQUESTS_AVAILABLE = True
except ImportError:
    print("âš ï¸ requests/beautifulsoup4ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Sitemap ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
    REQUESTS_AVAILABLE = False

if SELENIUM_AVAILABLE:
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException

# =============================================================================
# KLOOK URL íŒ¨í„´ ë° ê²€ì¦ ì‹œìŠ¤í…œ
# =============================================================================

def is_valid_klook_url(url):
    """KLOOK URL ìœ íš¨ì„± ê²€ì‚¬"""
    if not url or not isinstance(url, str):
        return False
    
    # KLOOK ë„ë©”ì¸ ì²´í¬
    klook_domains = [
        'klook.com',
        'www.klook.com', 
        'm.klook.com'
    ]
    
    parsed = urlparse(url)
    domain_valid = any(domain in parsed.netloc.lower() for domain in klook_domains)
    
    if not domain_valid:
        return False
    
    # /activity/ íŒ¨í„´ ì²´í¬
    activity_patterns = [
        r'/activity/\d+',           # /activity/123456
        r'/ko/activity/\d+',        # /ko/activity/123456  
        r'/en/activity/\d+',        # /en/activity/123456
        r'/activity/[^/]+',         # /activity/slug-name
    ]
    
    path_valid = any(re.search(pattern, url) for pattern in activity_patterns)
    return path_valid

def normalize_klook_url(url):
    """KLOOK URL ì •ê·œí™”"""
    if not url:
        return url
    
    try:
        parsed = urlparse(url)
        
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì •ë¦¬ (ì¶”ì ìš© íŒŒë¼ë¯¸í„° ì œê±°)
        query_params = parse_qs(parsed.query)
        
        # ìœ ì§€í•  íŒŒë¼ë¯¸í„°ë§Œ ì„ íƒ
        keep_params = ['lang', 'currency']
        cleaned_params = {k: v for k, v in query_params.items() if k in keep_params}
        
        # URL ì¬êµ¬ì„±
        cleaned_query = urlencode(cleaned_params, doseq=True)
        cleaned_url = urlunparse((
            parsed.scheme,
            parsed.netloc,
            parsed.path,
            parsed.params,
            cleaned_query,
            ''  # fragment ì œê±°
        ))
        
        return cleaned_url
        
    except Exception:
        return url

def extract_activity_id(url):
    """URLì—ì„œ activity ID ì¶”ì¶œ"""
    if not url:
        return None
    
    # ìˆ«ì ID íŒ¨í„´ ë§¤ì¹­
    id_patterns = [
        r'/activity/(\d+)',
        r'/ko/activity/(\d+)',
        r'/en/activity/(\d+)',
    ]
    
    for pattern in id_patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    
    return None

# =============================================================================
# URL ìˆ˜ì§‘ ì‹œìŠ¤í…œ
# =============================================================================

def collect_urls_from_page(driver, city_name):
    """í˜„ì¬ í˜ì´ì§€ì—ì„œ KLOOK URL ìˆ˜ì§‘"""
    print("ğŸ”— í˜ì´ì§€ì—ì„œ URL ìˆ˜ì§‘ ì¤‘...")
    
    if not SELENIUM_AVAILABLE:
        print("âš ï¸ Seleniumì´ ì—†ì–´ URL ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []
    
    try:
        # KLOOK activity URLì„ ì°¾ëŠ” CSS ì„ íƒìë“¤
        url_selectors = [
            "a[href*='/activity/']",
            ".product-card a",
            ".activity-card a",
            ".card a[href*='klook']",
            ".item a[href*='/activity/']",
            "[data-testid='activity-card'] a"
        ]
        
        found_urls = set()
        
        for selector in url_selectors:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                
                for element in elements:
                    try:
                        href = element.get_attribute("href")
                        if href and is_valid_klook_url(href):
                            normalized_url = normalize_klook_url(href)
                            found_urls.add(normalized_url)
                    except:
                        continue
                        
            except Exception:
                continue
        
        print(f"  âœ… ìˆ˜ì§‘ëœ URL: {len(found_urls)}ê°œ")
        return list(found_urls)
        
    except Exception as e:
        print(f"  âš ï¸ URL ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

def get_pagination_urls(driver, max_pages=5):
    """í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•œ URL ìˆ˜ì§‘"""
    print(f"ğŸ“„ í˜ì´ì§€ë„¤ì´ì…˜ URL ìˆ˜ì§‘ ì¤‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)...")
    
    all_urls = set()
    current_page = 1
    
    while current_page <= max_pages:
        print(f"  ğŸ“„ {current_page}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
        
        # í˜„ì¬ í˜ì´ì§€ì—ì„œ URL ìˆ˜ì§‘
        page_urls = collect_urls_from_page(driver, "")
        
        if not page_urls:
            print("  âš ï¸ URLì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìˆ˜ì§‘ ì¤‘ë‹¨")
            break
        
        all_urls.update(page_urls)
        
        # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
        if current_page < max_pages:
            if not go_to_next_page(driver):
                print("  â„¹ï¸ ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ìŒ")
                break
        
        current_page += 1
        time.sleep(2)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
    
    print(f"âœ… ì´ ìˆ˜ì§‘ëœ URL: {len(all_urls)}ê°œ")
    return list(all_urls)

def go_to_next_page(driver):
    """ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™"""
    if not SELENIUM_AVAILABLE:
        return False
    
    next_button_selectors = [
        "a[aria-label='Next page']",
        "button[aria-label='Next page']",
        ".pagination .next",
        ".pagination-next",
        "a:contains('ë‹¤ìŒ')",
        "button:contains('ë‹¤ìŒ')",
        ".pager .next"
    ]
    
    try:
        for selector in next_button_selectors:
            try:
                next_button = driver.find_element(By.CSS_SELECTOR, selector)
                
                if next_button and next_button.is_enabled():
                    # ë¹„í™œì„±í™” ìƒíƒœ ì²´í¬
                    classes = next_button.get_attribute("class") or ""
                    if "disabled" not in classes.lower():
                        next_button.click()
                        time.sleep(3)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
                        return True
                        
            except Exception:
                continue
        
        return False
        
    except Exception as e:
        print(f"  âš ï¸ í˜ì´ì§€ë„¤ì´ì…˜ ì´ë™ ì‹¤íŒ¨: {e}")
        return False

# =============================================================================
# URL ìƒíƒœ ê´€ë¦¬ ì‹œìŠ¤í…œ
# =============================================================================

def is_url_already_processed(url, city_name):
    """URL ì²˜ë¦¬ ì—¬ë¶€ í™•ì¸"""
    # hashlib ì‹œìŠ¤í…œ ìš°ì„  ì‚¬ìš©
    if CONFIG.get("USE_HASH_SYSTEM", True):
        return is_url_processed_fast(url, city_name)
    
    # í´ë°±: CSV ê¸°ë°˜ í™•ì¸ (ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ í˜¸í™˜)
    try:
        from ..utils.file_handler import get_csv_stats
        
        stats = get_csv_stats(city_name)
        if isinstance(stats, dict) and 'error' not in stats:
            # CSVì—ì„œ URL í™•ì¸ ë¡œì§ (ê°„ë‹¨í™”)
            return False  # ì¼ë‹¨ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ê°€ì •
            
    except Exception:
        pass
    
    return False

def mark_url_as_processed(url, city_name, product_number=None, rank=None):
    """URLì„ ì²˜ë¦¬ ì™„ë£Œë¡œ í‘œì‹œ"""
    # hashlib ì‹œìŠ¤í…œ ìš°ì„  ì‚¬ìš©
    if CONFIG.get("USE_HASH_SYSTEM", True):
        return mark_url_processed_fast(url, city_name, product_number, rank)
    
    return True

def get_unprocessed_urls(url_list, city_name):
    """ì²˜ë¦¬ë˜ì§€ ì•Šì€ URL ëª©ë¡ ë°˜í™˜"""
    unprocessed = []
    
    for url in url_list:
        if not is_url_already_processed(url, city_name):
            unprocessed.append(url)
    
    print(f"ğŸ“Š ì „ì²´ URL: {len(url_list)}ê°œ, ë¯¸ì²˜ë¦¬ URL: {len(unprocessed)}ê°œ")
    return unprocessed

def save_urls_to_file(urls, city_name, filename_suffix="collected"):
    """URL ëª©ë¡ì„ íŒŒì¼ë¡œ ì €ì¥"""
    try:
        city_code = get_city_code(city_name)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        save_dir = "url_logs"
        os.makedirs(save_dir, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„±
        filename = f"{city_code}_{filename_suffix}_{timestamp}.txt"
        filepath = os.path.join(save_dir, filename)
        
        # URL ì €ì¥
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"# {city_name} URL ëª©ë¡\n")
            f.write(f"# ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# ì´ URL ê°œìˆ˜: {len(urls)}\n\n")
            
            for i, url in enumerate(urls, 1):
                f.write(f"{i}. {url}\n")
        
        print(f"âœ… URL ëª©ë¡ ì €ì¥ ì™„ë£Œ: {filepath}")
        return filepath
        
    except Exception as e:
        print(f"âš ï¸ URL ëª©ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")
        return None

def load_urls_from_file(filepath):
    """íŒŒì¼ì—ì„œ URL ëª©ë¡ ë¡œë“œ"""
    try:
        urls = []
        
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # ì£¼ì„ê³¼ ë¹ˆ ì¤„ ê±´ë„ˆë›°ê¸°
                if line and not line.startswith('#'):
                    # ë²ˆí˜¸ ì œê±° (ì˜ˆ: "1. https://...")
                    if '. ' in line:
                        url = line.split('. ', 1)[1]
                    else:
                        url = line
                    
                    if is_valid_klook_url(url):
                        urls.append(url)
        
        print(f"âœ… URL ëª©ë¡ ë¡œë“œ ì™„ë£Œ: {len(urls)}ê°œ")
        return urls
        
    except Exception as e:
        print(f"âš ï¸ URL ëª©ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

# =============================================================================
# í†µí•© URL ìˆ˜ì§‘ ì‹œìŠ¤í…œ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±)
# =============================================================================

def execute_comprehensive_url_collection(driver, city_name, max_pages=3):
    """ì¢…í•©ì ì¸ URL ìˆ˜ì§‘ ì‹œìŠ¤í…œ"""
    print(f"ğŸš€ {city_name} ì¢…í•© URL ìˆ˜ì§‘ ì‹œì‘...")
    
    try:
        # í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•œ URL ìˆ˜ì§‘
        all_urls = get_pagination_urls(driver, max_pages)
        
        # ì¤‘ë³µ ì œê±° ë° ê²€ì¦
        valid_urls = []
        for url in all_urls:
            if is_valid_klook_url(url):
                normalized = normalize_klook_url(url)
                valid_urls.append(normalized)
        
        # ì¤‘ë³µ ì œê±°
        unique_urls = list(set(valid_urls))
        
        # ë¯¸ì²˜ë¦¬ URLë§Œ í•„í„°ë§
        unprocessed_urls = get_unprocessed_urls(unique_urls, city_name)
        
        # URL ì €ì¥
        if unprocessed_urls:
            save_urls_to_file(unprocessed_urls, city_name)
        
        print(f"âœ… URL ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(unique_urls)}ê°œ, ë¯¸ì²˜ë¦¬ {len(unprocessed_urls)}ê°œ")
        return unprocessed_urls
        
    except Exception as e:
        print(f"âŒ URL ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

# =============================================================================
# ğŸ—ºï¸ Sitemap ê¸°ë°˜ URL ìˆ˜ì§‘ (í˜ì´ì§€ë„¤ì´ì…˜ ë³´ì™„ìš©)
# =============================================================================

def collect_urls_from_sitemap(city_name, exclude_urls=None, limit=1000):
    """Sitemapì—ì„œ KLOOK URL ìˆ˜ì§‘ (ì¤‘ë³µ ì œì™¸)"""
    if not REQUESTS_AVAILABLE:
        print("âŒ requests/beautifulsoup4ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    
    print(f"ğŸ—ºï¸ '{city_name}' Sitemap URL ìˆ˜ì§‘ ì‹œì‘...")
    
    exclude_set = set(exclude_urls or [])
    print(f"   ğŸš« ì œì™¸í•  URL: {len(exclude_set)}ê°œ")
    
    # KLOOK sitemap URLë“¤
    sitemap_urls = [
        "https://www.klook.com/sitemap.xml",
        "https://www.klook.com/sitemap-activities.xml", 
        "https://www.klook.com/sitemap-ko.xml",
        f"https://www.klook.com/sitemap-{city_name.lower()}.xml"
    ]
    
    collected_urls = []
    
    for sitemap_url in sitemap_urls:
        try:
            print(f"  ğŸ“‹ Sitemap ì²˜ë¦¬ ì¤‘: {sitemap_url}")
            
            response = requests.get(sitemap_url, timeout=30, headers={
                'User-Agent': CONFIG.get("USER_AGENT", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
            })
            
            if response.status_code == 200:
                # XML íŒŒì‹±
                soup = BeautifulSoup(response.content, 'xml')
                
                # URL ì¶”ì¶œ
                urls = soup.find_all('url')
                for url_element in urls:
                    loc = url_element.find('loc')
                    if loc and loc.text:
                        url = loc.text.strip()
                        
                        # KLOOK activity URL í•„í„°ë§
                        if is_valid_klook_url(url):
                            normalized_url = normalize_klook_url(url)
                            
                            # ì¤‘ë³µ ì²´í¬ (ì´ë¯¸ ìˆ˜ì§‘í•œ URL ì œì™¸)
                            if normalized_url not in exclude_set and normalized_url not in collected_urls:
                                # ë„ì‹œëª…ê³¼ ê´€ë ¨ëœ URL ìš°ì„  (ì„ íƒì )
                                if city_name.lower() in url.lower() or len(collected_urls) < limit:
                                    collected_urls.append(normalized_url)
                                    
                                    if len(collected_urls) >= limit:
                                        break
                
                print(f"    âœ… ìƒˆë¡œìš´ URL {len([u for u in collected_urls if u not in exclude_set])}ê°œ ë°œê²¬")
                
            else:
                print(f"    âš ï¸ Sitemap ì ‘ê·¼ ì‹¤íŒ¨: HTTP {response.status_code}")
                
        except Exception as e:
            print(f"    âŒ Sitemap ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue
    
    # ìµœì¢… ì¤‘ë³µ ì œê±° (ì•ˆì „ì¥ì¹˜)
    final_urls = [url for url in collected_urls if url not in exclude_set]
    
    print(f"ğŸ‰ Sitemap ìˆ˜ì§‘ ì™„ë£Œ: {len(final_urls)}ê°œ ìƒˆë¡œìš´ URL")
    return final_urls[:limit]

def save_urls_to_collection(urls, city_name, collection_type="sitemap"):
    """URL ì»¬ë ‰ì…˜ì„ íŒŒì¼ë¡œ ì €ì¥"""
    try:
        city_code = get_city_code(city_name)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        save_dir = "url_collections"
        os.makedirs(save_dir, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„±
        filename = f"{city_code}_{collection_type}_{timestamp}.json"
        filepath = os.path.join(save_dir, filename)
        
        # ì»¬ë ‰ì…˜ ë°ì´í„° êµ¬ì„±
        collection_data = {
            "city_name": city_name,
            "city_code": city_code,
            "collection_type": collection_type,
            "collected_at": datetime.now().isoformat(),
            "total_urls": len(urls),
            "urls": urls
        }
        
        # JSON ì €ì¥
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(collection_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… URL ì»¬ë ‰ì…˜ ì €ì¥ ì™„ë£Œ: {filepath}")
        return True
        
    except Exception as e:
        print(f"âš ï¸ URL ì»¬ë ‰ì…˜ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

print("âœ… url_manager.py ë¡œë“œ ì™„ë£Œ: URL ê´€ë¦¬ ì‹œìŠ¤í…œ ì¤€ë¹„!")
print("   ğŸ—ºï¸ Sitemap ê¸°ëŠ¥ ì¶”ê°€:")
print("   - collect_urls_from_sitemap(): ì¤‘ë³µ ì œì™¸ Sitemap ìˆ˜ì§‘")
print("   - save_urls_to_collection(): URL ì»¬ë ‰ì…˜ ì €ì¥")