"""
ë©”ì¸ í¬ë¡¤ë§ ì—”ì§„
- ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ í†µí•© ê´€ë¦¬
- í˜ì´ì§€ë„¤ì´ì…˜ ë° ë°ì´í„° ìˆ˜ì§‘
- ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ ì‹œìŠ¤í…œ
"""

import time
import random
from datetime import datetime

from ..config import CONFIG, SELENIUM_AVAILABLE
from ..utils.file_handler import create_product_data_structure, save_to_csv_kkday, get_dual_image_urls_kkday, download_and_save_image_kkday, ensure_directory_structure
from .driver_manager import setup_driver, go_to_main_page, find_and_fill_search, click_search_button, handle_popup, smart_scroll_selector
from .url_manager import collect_urls_from_page, get_pagination_urls, is_url_already_processed, mark_url_as_processed
from .parsers import extract_all_product_data, validate_product_data
from .ranking import save_url_with_rank, ranking_manager, get_collected_ranks_summary

if SELENIUM_AVAILABLE:
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException

# =============================================================================
# ë©”ì¸ í¬ë¡¤ë§ í´ë˜ìŠ¤
# =============================================================================

class KKdayCrawler:
    """KKday í¬ë¡¤ë§ í†µí•© ì‹œìŠ¤í…œ"""
    
    def __init__(self, city_name="ì„œìš¸"):
        self.city_name = city_name
        self.driver = None
        self.stats = {
            "start_time": None,
            "end_time": None,
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "skip_count": 0,
            "urls_collected": 0,
            "current_rank": 0
        }
        
    def initialize(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        print(f"ğŸš€ KKday í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”: {self.city_name}")
        
        try:
            # ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ë³´
            ensure_directory_structure(self.city_name)
            
            # ë“œë¼ì´ë²„ ì„¤ì •
            self.driver = setup_driver()
            if not self.driver:
                raise Exception("ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            # ë©”ì¸ í˜ì´ì§€ ì´ë™ ë° ê²€ìƒ‰
            if not go_to_main_page(self.driver):
                raise Exception("ë©”ì¸ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨")
            
            handle_popup(self.driver)
            
            search_input = find_and_fill_search(self.driver, self.city_name)
            if not search_input:
                raise Exception("ê²€ìƒ‰ì°½ ì…ë ¥ ì‹¤íŒ¨")
            
            if not click_search_button(self.driver):
                raise Exception("ê²€ìƒ‰ ì‹¤í–‰ ì‹¤íŒ¨")
            
            self.stats["start_time"] = datetime.now()
            print("âœ… í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if self.driver:
                # self.driver.quit() - ì œê±°ë¨: ë¸Œë¼ìš°ì € ì—´ì–´ë‘ê¸°
                pass
            return False
    
    def collect_urls(self, max_pages=3):
        """URL ìˆ˜ì§‘"""
        print(f"ğŸ”— URL ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
        
        try:
            urls = get_pagination_urls(self.driver, max_pages)
            
            # ë¯¸ì²˜ë¦¬ URL í•„í„°ë§
            unprocessed_urls = []
            for url in urls:
                if not is_url_already_processed(url, self.city_name):
                    unprocessed_urls.append(url)
            
            self.stats["urls_collected"] = len(urls)
            print(f"âœ… URL ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(urls)}ê°œ, ë¯¸ì²˜ë¦¬ {len(unprocessed_urls)}ê°œ")
            return unprocessed_urls
            
        except Exception as e:
            print(f"âŒ URL ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def crawl_product(self, url, rank=None):
        """ê°œë³„ ìƒí’ˆ í¬ë¡¤ë§"""
        print(f"ğŸ” ìƒí’ˆ í¬ë¡¤ë§ ì‹œì‘: ìˆœìœ„ {rank}")
        
        try:
            # ìƒí’ˆ í˜ì´ì§€ ì´ë™
            self.driver.get(url)
            time.sleep(random.uniform(2, 4))
            
            # ë°ì´í„° ì¶”ì¶œ
            product_data = extract_all_product_data(self.driver, url, rank, city_name=self.city_name)
            
            # ë°ì´í„° ê²€ì¦
            if not validate_product_data(product_data):
                print(f"âš ï¸ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: ìˆœìœ„ {rank}")
                self.stats["error_count"] += 1
                return False
            
            # ê¸°ë³¸ êµ¬ì¡°ì— ë§ì¶° ë°ì´í„° ë³‘í•©
            base_data = create_product_data_structure(self.city_name, self.stats["total_processed"] + 1, rank)
            base_data.update(product_data)
            
            # ì´ë¯¸ì§€ ì²˜ë¦¬
            try:
                main_img, thumb_img = get_dual_image_urls_kkday(self.driver)
                if main_img:
                    base_data["ë©”ì¸ì´ë¯¸ì§€"] = main_img
                    
                    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (ì„ íƒì )
                    if CONFIG.get("SAVE_IMAGES", False):
                        img_path = f"images/{self.city_name}/rank_{rank}_main.jpg"
                        download_and_save_image_kkday(main_img, img_path)
                
                if thumb_img:
                    base_data["ì¸ë„¤ì¼ì´ë¯¸ì§€"] = thumb_img
                    
            except Exception as e:
                print(f"  âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            # CSV ì €ì¥
            if save_to_csv_kkday(base_data, self.city_name):
                # ìˆœìœ„ ì •ë³´ ì €ì¥
                save_url_with_rank(url, rank, self.city_name)
                
                # URL ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
                mark_url_as_processed(url, self.city_name, base_data["ìƒí’ˆë²ˆí˜¸"], rank)
                
                self.stats["success_count"] += 1
                self.stats["current_rank"] = rank
                print(f"âœ… ìƒí’ˆ í¬ë¡¤ë§ ì™„ë£Œ: ìˆœìœ„ {rank}")
                return True
            else:
                self.stats["error_count"] += 1
                return False
                
        except Exception as e:
            print(f"âŒ ìƒí’ˆ í¬ë¡¤ë§ ì‹¤íŒ¨ (ìˆœìœ„ {rank}): {e}")
            self.stats["error_count"] += 1
            return False
        finally:
            self.stats["total_processed"] += 1
    
    def crawl_products_batch(self, urls, start_rank=1):
        """ë°°ì¹˜ ìƒí’ˆ í¬ë¡¤ë§"""
        print(f"ğŸ“¦ ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘: {len(urls)}ê°œ ìƒí’ˆ")
        
        current_rank = start_rank
        
        for i, url in enumerate(urls):
            print(f"\n{'='*50}")
            print(f"ì§„í–‰ë¥ : {i+1}/{len(urls)} ({((i+1)/len(urls)*100):.1f}%)")
            print(f"URL: {url}")
            
            # ì´ë¯¸ ì²˜ë¦¬ëœ URLì¸ì§€ í™•ì¸
            if is_url_already_processed(url, self.city_name):
                print(f"â­ï¸ ì´ë¯¸ ì²˜ë¦¬ëœ URL, ê±´ë„ˆëœ€")
                self.stats["skip_count"] += 1
                continue
            
            # ìƒí’ˆ í¬ë¡¤ë§
            success = self.crawl_product(url, current_rank)
            
            if success:
                current_rank += 1
            
            # ì§„í–‰ìƒí™© ì¶œë ¥
            self.print_progress()
            
            # ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€ê¸°
            delay = random.uniform(
                CONFIG.get("MEDIUM_MIN_DELAY", 3),
                CONFIG.get("MEDIUM_MAX_DELAY", 8)
            )
            print(f"â³ {delay:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(delay)
            
            # ì¤‘ê°„ íœ´ì‹ (10ê°œë§ˆë‹¤)
            if (i + 1) % 10 == 0:
                long_delay = random.uniform(
                    CONFIG.get("LONG_MIN_DELAY", 15),
                    CONFIG.get("LONG_MAX_DELAY", 30)
                )
                print(f"ğŸ˜´ ê¸´ íœ´ì‹: {long_delay:.1f}ì´ˆ...")
                time.sleep(long_delay)
        
        print("\nğŸ“¦ ë°°ì¹˜ í¬ë¡¤ë§ ì™„ë£Œ")
        return True
    
    def run_full_crawling(self, max_pages=3, max_products=None):
        """ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰"""
        print(f"ğŸ¯ {self.city_name} ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
        
        try:
            # 1. ì´ˆê¸°í™”
            if not self.initialize():
                return False
            
            # 2. URL ìˆ˜ì§‘
            urls = self.collect_urls(max_pages)
            if not urls:
                print("âš ï¸ ìˆ˜ì§‘í•  URLì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # 3. ìµœëŒ€ ìƒí’ˆ ìˆ˜ ì œí•œ
            max_products_config = CONFIG.get("MAX_PRODUCTS_PER_CITY", None)
            if max_products:
                urls = urls[:max_products]
            elif max_products_config:
                urls = urls[:max_products_config]
            
            print(f"ğŸ“Š í¬ë¡¤ë§í•  ìƒí’ˆ ìˆ˜: {len(urls)}ê°œ")
            
            # 4. ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰
            success = self.crawl_products_batch(urls)
            
            # 5. ìµœì¢… í†µê³„
            self.stats["end_time"] = datetime.now()
            self.print_final_stats()
            
            return success
            
        except Exception as e:
            print(f"âŒ ì „ì²´ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return False
        finally:
            if self.driver:
                # self.driver.quit() - ì œê±°ë¨: ë¸Œë¼ìš°ì € ì—´ì–´ë‘ê¸°
                pass
    
    def print_progress(self):
        """ì§„í–‰ìƒí™© ì¶œë ¥"""
        total = self.stats["total_processed"]
        success = self.stats["success_count"]
        error = self.stats["error_count"]
        skip = self.stats["skip_count"]
        
        if total > 0:
            success_rate = (success / total) * 100
            print(f"ğŸ“Š ì§„í–‰ìƒí™©: ì„±ê³µ {success}, ì‹¤íŒ¨ {error}, ê±´ë„ˆëœ€ {skip}, ì„±ê³µë¥  {success_rate:.1f}%")
    
    def print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        print("\n" + "="*60)
        print(f"ğŸ‰ {self.city_name} í¬ë¡¤ë§ ì™„ë£Œ!")
        print("="*60)
        
        if self.stats["start_time"] and self.stats["end_time"]:
            duration = self.stats["end_time"] - self.stats["start_time"]
            print(f"â±ï¸ ì†Œìš”ì‹œê°„: {duration}")
        
        print(f"ğŸ“Š ì²˜ë¦¬ í†µê³„:")
        print(f"   â€¢ ì „ì²´ ì²˜ë¦¬: {self.stats['total_processed']}ê°œ")
        print(f"   â€¢ ì„±ê³µ: {self.stats['success_count']}ê°œ")
        print(f"   â€¢ ì‹¤íŒ¨: {self.stats['error_count']}ê°œ")
        print(f"   â€¢ ê±´ë„ˆëœ€: {self.stats['skip_count']}ê°œ")
        print(f"   â€¢ URL ìˆ˜ì§‘: {self.stats['urls_collected']}ê°œ")
        print(f"   â€¢ ë§ˆì§€ë§‰ ìˆœìœ„: {self.stats['current_rank']}")
        
        if self.stats["total_processed"] > 0:
            success_rate = (self.stats["success_count"] / self.stats["total_processed"]) * 100
            print(f"   â€¢ ì„±ê³µë¥ : {success_rate:.1f}%")

# =============================================================================
# í¸ì˜ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±)
# =============================================================================

def execute_kkday_crawling_system(city_name="ì„œìš¸", max_pages=3, max_products=None):
    """KKday í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹¤í–‰ (ê¸°ì¡´ í•¨ìˆ˜ëª… í˜¸í™˜)"""
    crawler = KKdayCrawler(city_name)
    return crawler.run_full_crawling(max_pages, max_products)

def quick_crawl_test(city_name="ì„œìš¸", max_products=3):
    """ë¹ ë¥¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
    print(f"ğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§: {city_name}")
    
    crawler = KKdayCrawler(city_name)
    return crawler.run_full_crawling(max_pages=1, max_products=max_products)

def get_crawling_status(city_name):
    """í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ"""
    try:
        summary = get_collected_ranks_summary(city_name)
        
        print(f"\nğŸ“Š {city_name} í¬ë¡¤ë§ í˜„í™©:")
        print(f"   â€¢ ìˆ˜ì§‘ëœ URL: {summary.get('total_urls', 0)}ê°œ")
        print(f"   â€¢ ìˆœìœ„ ë²”ìœ„: {summary.get('rank_range', 'ì—†ìŒ')}")
        print(f"   â€¢ ëˆ„ë½ ìˆœìœ„: {len(summary.get('missing_ranks', []))}ê°œ")
        
        missing = summary.get('missing_ranks', [])
        if missing:
            print(f"   â€¢ ëˆ„ë½ ìƒì„¸: {missing[:10]}{'...' if len(missing) > 10 else ''}")
        
        return summary
        
    except Exception as e:
        print(f"âš ï¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {}

print("âœ… crawler.py ë¡œë“œ ì™„ë£Œ: ë©”ì¸ í¬ë¡¤ë§ ì—”ì§„ ì¤€ë¹„!")