# ê°œì„ ëœ ìˆœìœ„ ì—°ì†ì„± ì‹œìŠ¤í…œ: 2ë‹¨ê³„ ë¶„ë¦¬ ì „í™˜

## ğŸ¯ **ëª©í‘œ: ì™„ë²½í•œ ìˆœìœ„ ì—°ì†ì„± ë³´ì¥**

### ğŸš¨ **ê¸°ì¡´ ë¬¸ì œì **
- 1ë‹¨ê³„ ìˆ˜ì§‘ ìˆœì„œ â‰  2ë‹¨ê³„ ì²˜ë¦¬ ìˆœìœ„
- ì¤‘ë³µ URL ê±´ë„ˆë›°ê¸°ë¡œ ì¸í•œ ìˆœìœ„ ëˆ„ë½
- ë­í‚¹ ë°ì´í„° ë¶ˆì™„ì „ì„±

### âœ… **í•´ê²° ë°©ì•ˆ: 3ë‹¨ê³„ ìˆœìœ„ ë§¤í•‘ ì‹œìŠ¤í…œ**

---

## ğŸ“‹ **Phase 1: 1ë‹¨ê³„ - ìˆœìœ„ ì •ë³´ í¬í•¨ URL ìˆ˜ì§‘**

### **ê°œì„ ëœ URL ì €ì¥ í¬ë§· (JSON)**

```python
# ===== 1ë‹¨ê³„: URL ìˆ˜ì§‘ + ìˆœìœ„ ì •ë³´ ì €ì¥ =====

def collect_urls_with_rank_info():
    """ìˆœìœ„ ì •ë³´ë¥¼ í¬í•¨í•œ URL ìˆ˜ì§‘"""

    collected_data = {
        "collection_info": {
            "city": CITY_NAME,
            "tab": TARGET_TAB,
            "timestamp": datetime.now().isoformat(),
            "target_products": TARGET_PRODUCTS,
            "max_pages": MAX_PAGES
        },
        "url_rank_mapping": [],  # ìˆœìœ„-URL ë§¤í•‘
        "collection_stats": {
            "total_urls_found": 0,
            "total_pages_processed": 0,
            "collection_success": False
        }
    }

    current_rank = 1  # ì „ì—­ ìˆœìœ„ (í˜ì´ì§€ ê°„ ì—°ì†)
    current_page = 1
    current_listing_url = listing_page_url

    while len(collected_data["url_rank_mapping"]) < TARGET_PRODUCTS and current_page <= MAX_PAGES:
        print(f"\nğŸ“„ {current_page}í˜ì´ì§€ URL ìˆ˜ì§‘ ì¤‘...")

        # í˜„ì¬ í˜ì´ì§€ì—ì„œ URL ìˆ˜ì§‘
        activity_urls = collect_activity_urls_only(driver)

        if not activity_urls:
            # í˜ì´ì§€ ì´ë™ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
            success, current_listing_url = go_to_next_page(driver, current_listing_url)
            if not success:
                break
            current_page += 1
            continue

        print(f"   ğŸ“Š {current_page}í˜ì´ì§€ì—ì„œ Activity {len(activity_urls)}ê°œ ë°œê²¬")

        # ê° URLì— ìˆœìœ„ í• ë‹¹ (í˜ì´ì§€ ë‚´ ìˆœì„œëŒ€ë¡œ)
        for page_index, url in enumerate(activity_urls):
            if len(collected_data["url_rank_mapping"]) >= TARGET_PRODUCTS:
                break

            # ìˆœìœ„-URL-í˜ì´ì§€ ì •ë³´ ì €ì¥
            url_info = {
                "rank": current_rank,
                "url": url,
                "page": current_page,
                "page_index": page_index + 1,
                "collected_at": datetime.now().isoformat(),
                "is_duplicate": is_url_processed_fast(url, CITY_NAME)  # ì¤‘ë³µ ì—¬ë¶€ ë¯¸ë¦¬ ì²´í¬
            }

            collected_data["url_rank_mapping"].append(url_info)

            print(f"   âœ… {current_rank}ìœ„ URL í• ë‹¹: {url[:50]}... {'(ì¤‘ë³µ)' if url_info['is_duplicate'] else ''}")

            current_rank += 1

        # ëª©í‘œ ë‹¬ì„± ì‹œ ì¤‘ë‹¨
        if len(collected_data["url_rank_mapping"]) >= TARGET_PRODUCTS:
            break

        # ë‹¤ìŒ í˜ì´ì§€ ì´ë™
        if current_page < MAX_PAGES:
            success, current_listing_url = go_to_next_page(driver, current_listing_url)
            if success:
                current_page += 1
                time.sleep(2)
            else:
                break

    # ìˆ˜ì§‘ í†µê³„ ì—…ë°ì´íŠ¸
    collected_data["collection_stats"] = {
        "total_urls_found": len(collected_data["url_rank_mapping"]),
        "total_pages_processed": current_page,
        "collection_success": len(collected_data["url_rank_mapping"]) > 0,
        "duplicate_count": sum(1 for item in collected_data["url_rank_mapping"] if item["is_duplicate"]),
        "new_count": sum(1 for item in collected_data["url_rank_mapping"] if not item["is_duplicate"])
    }

    return collected_data

# 1ë‹¨ê³„ ì‹¤í–‰ ë° ì €ì¥
URL_DATA_FILE = f"klook_urls_data_{CITY_NAME}_{TARGET_TAB.replace('&', 'and').replace(' ', '_')}.json"

try:
    collected_data = collect_urls_with_rank_info()

    if collected_data["collection_stats"]["collection_success"]:
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(URL_DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(collected_data, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… ìˆœìœ„-URL ë°ì´í„°ë¥¼ '{URL_DATA_FILE}'ì— ì €ì¥!")
        print(f"   ğŸ“Š ì´ {collected_data['collection_stats']['total_urls_found']}ê°œ URL")
        print(f"   ğŸ†• ì‹ ê·œ: {collected_data['collection_stats']['new_count']}ê°œ")
        print(f"   ğŸ”„ ì¤‘ë³µ: {collected_data['collection_stats']['duplicate_count']}ê°œ")

        stage1_success = True
    else:
        print("âš ï¸ URL ìˆ˜ì§‘ ì‹¤íŒ¨")
        stage1_success = False

except Exception as e:
    print(f"âŒ 1ë‹¨ê³„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    stage1_success = False
```

---

## ğŸ“‹ **Phase 2: 2ë‹¨ê³„ - ìˆœìœ„ ê¸°ë°˜ ìƒì„¸ í¬ë¡¤ë§**

### **ê°œì„ ëœ ìˆœìœ„ ì—°ì†ì„± ì²˜ë¦¬**

```python
# ===== 2ë‹¨ê³„: ìˆœìœ„ ì •ë³´ ê¸°ë°˜ ìƒì„¸ í¬ë¡¤ë§ =====

def crawl_with_preserved_ranks():
    """ìˆœìœ„ ì •ë³´ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ìƒì„¸ í¬ë¡¤ë§"""

    URL_DATA_FILE = f"klook_urls_data_{CITY_NAME}_{TARGET_TAB.replace('&', 'and').replace(' ', '_')}.json"

    if not os.path.exists(URL_DATA_FILE):
        print(f"âŒ URL ë°ì´í„° íŒŒì¼ '{URL_DATA_FILE}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False

    # URL ë°ì´í„° ë¡œë“œ
    with open(URL_DATA_FILE, 'r', encoding='utf-8') as f:
        url_data = json.load(f)

    collection_info = url_data["collection_info"]
    url_rank_mapping = url_data["url_rank_mapping"]

    print(f"âœ… URL ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
    print(f"   ğŸ™ï¸ ë„ì‹œ: {collection_info['city']}")
    print(f"   ğŸ“‘ íƒ­: {collection_info['tab']}")
    print(f"   ğŸ“Š ì´ URL: {len(url_rank_mapping)}ê°œ")
    print(f"   ğŸ• ìˆ˜ì§‘ ì‹œê°„: {collection_info['timestamp'][:19]}")

    # ë“œë¼ì´ë²„ ì´ˆê¸°í™” (2ë‹¨ê³„ìš©)
    driver = setup_driver()
    if not driver:
        raise Exception("ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨")

    # í¬ë¡¤ë§ í†µê³„
    crawling_stats = {
        "total_urls": len(url_rank_mapping),
        "processed_count": 0,
        "success_count": 0,
        "skip_count": 0,
        "error_count": 0,
        "actual_ranks_saved": []  # ì‹¤ì œ ì €ì¥ëœ ìˆœìœ„ ëª©ë¡
    }

    ranking_data = []

    print(f"\nğŸ“¦ ìˆœìœ„ ê¸°ë°˜ ìƒì„¸ í¬ë¡¤ë§ ì‹œì‘...")

    for i, url_info in enumerate(url_rank_mapping, 1):
        rank = url_info["rank"]
        url = url_info["url"]
        page = url_info["page"]
        is_duplicate = url_info["is_duplicate"]

        print(f"\n   ğŸ” {rank}ìœ„ ì²˜ë¦¬ ì¤‘... ({i}/{len(url_rank_mapping)})")
        print(f"      URL: {url[:60]}...")
        print(f"      ì›ë³¸ í˜ì´ì§€: {page}í˜ì´ì§€")

        crawling_stats["processed_count"] += 1

        # ì¤‘ë³µ URL ê±´ë„ˆë›°ê¸° (ìˆœìœ„ëŠ” ìœ ì§€)
        if is_duplicate or is_url_processed_fast(url, CITY_NAME):
            print(f"      â­ï¸ {rank}ìœ„ ì¤‘ë³µ URL ê±´ë„ˆë›°ê¸°")
            crawling_stats["skip_count"] += 1
            continue

        try:
            # ìƒí’ˆ í˜ì´ì§€ ì´ë™
            driver.get(url)
            time.sleep(random.uniform(2, 4))
            print("ğŸ“œ ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë¡¤ ì‹¤í–‰...")
            smart_scroll_selector(driver)

            # ìƒí’ˆ ë°ì´í„° ì¶”ì¶œ (ì›ë³¸ ìˆœìœ„ ì‚¬ìš©)
            product_data = extract_all_product_data(driver, url, rank, city_name=CITY_NAME)

            # CSV ë²ˆí˜¸ëŠ” ì—°ì†ì„± ë³´ì¥
            next_num = get_next_product_number(CITY_NAME)

            # ê¸°ë³¸ êµ¬ì¡° ìƒì„± (ì›ë³¸ ìˆœìœ„ ì‚¬ìš©)
            base_data = create_product_data_structure(CITY_NAME, next_num, rank)
            base_data.update(product_data)
            base_data['íƒ­'] = TARGET_TAB
            base_data['ì›ë³¸í˜ì´ì§€'] = page  # ì¶”ê°€ ì •ë³´

            # ì´ë¯¸ì§€ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ)
            try:
                main_img, thumb_img = get_dual_image_urls_klook(driver)
                if SAVE_IMAGES and (main_img or thumb_img):
                    image_urls = {"main": main_img, "thumb": thumb_img}
                    download_results = download_dual_images_klook(image_urls, next_num, CITY_NAME)

                    if download_results.get("main"):
                        base_data['ë©”ì¸ì´ë¯¸ì§€'] = get_smart_image_path(CITY_NAME, next_num, "main")
                        base_data['ë©”ì¸ì´ë¯¸ì§€_íŒŒì¼ëª…'] = download_results["main"]
                    else:
                        base_data['ë©”ì¸ì´ë¯¸ì§€'] = "ì´ë¯¸ì§€ ì—†ìŒ"
                        base_data['ë©”ì¸ì´ë¯¸ì§€_íŒŒì¼ëª…'] = ""

                    if download_results.get("thumb"):
                        base_data['ì¸ë„¤ì¼ì´ë¯¸ì§€'] = get_smart_image_path(CITY_NAME, next_num, "thumb")
                        base_data['ì¸ë„¤ì¼ì´ë¯¸ì§€_íŒŒì¼ëª…'] = download_results["thumb"]
                    else:
                        base_data['ì¸ë„¤ì¼ì´ë¯¸ì§€'] = "ì´ë¯¸ì§€ ì—†ìŒ"
                        base_data['ì¸ë„¤ì¼ì´ë¯¸ì§€_íŒŒì¼ëª…'] = ""
                else:
                    base_data['ë©”ì¸ì´ë¯¸ì§€'] = "ì´ë¯¸ì§€ ì—†ìŒ"
                    base_data['ì¸ë„¤ì¼ì´ë¯¸ì§€'] = "ì´ë¯¸ì§€ ì—†ìŒ"
                    base_data['ë©”ì¸ì´ë¯¸ì§€_íŒŒì¼ëª…'] = ""
                    base_data['ì¸ë„¤ì¼ì´ë¯¸ì§€_íŒŒì¼ëª…'] = ""

            except Exception as e:
                print(f"      âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                base_data['ë©”ì¸ì´ë¯¸ì§€'] = "ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨"
                base_data['ì¸ë„¤ì¼ì´ë¯¸ì§€'] = "ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨"

            # CSV ì €ì¥
            if save_to_csv_klook(base_data, CITY_NAME):
                # ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹
                mark_url_processed_fast(url, CITY_NAME, next_num, rank)

                # ë­í‚¹ ì •ë³´ ì €ì¥ (ì›ë³¸ ì •ë³´ í¬í•¨)
                ranking_info = {
                    "url": url,
                    "rank": rank,  # ì›ë³¸ ìˆœìœ„ ë³´ì¡´
                    "csv_number": next_num,  # CSV ë²ˆí˜¸
                    "tab": TARGET_TAB,
                    "city": CITY_NAME,
                    "original_page": page,  # ì›ë³¸ í˜ì´ì§€ ì •ë³´
                    "page_index": url_info["page_index"],
                    "collected_at": url_info["collected_at"],
                    "processed_at": datetime.now().isoformat()
                }
                ranking_data.append(ranking_info)

                crawling_stats["success_count"] += 1
                crawling_stats["actual_ranks_saved"].append(rank)

                print(f"      âœ… {rank}ìœ„ ìˆ˜ì§‘ ì™„ë£Œ (CSVë²ˆí˜¸: {next_num})")
            else:
                print(f"      âŒ {rank}ìœ„ ì €ì¥ ì‹¤íŒ¨")
                crawling_stats["error_count"] += 1

            time.sleep(random.uniform(1, 3))

        except Exception as e:
            print(f"      âŒ {rank}ìœ„ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            crawling_stats["error_count"] += 1
            continue

    # ë“œë¼ì´ë²„ ì¢…ë£Œ
    if driver:
        driver.quit()

    # ìµœì¢… í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š ìˆœìœ„ ì—°ì†ì„± í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"   â€¢ ì´ ì²˜ë¦¬: {crawling_stats['processed_count']}ê°œ")
    print(f"   â€¢ ì„±ê³µ: {crawling_stats['success_count']}ê°œ")
    print(f"   â€¢ ê±´ë„ˆëœ€: {crawling_stats['skip_count']}ê°œ")
    print(f"   â€¢ ì‹¤íŒ¨: {crawling_stats['error_count']}ê°œ")

    if crawling_stats["actual_ranks_saved"]:
        saved_ranks = sorted(crawling_stats["actual_ranks_saved"])
        print(f"   â€¢ ì €ì¥ëœ ìˆœìœ„: {saved_ranks[:5]}{'...' if len(saved_ranks) > 5 else ''}")
        print(f"   â€¢ ìˆœìœ„ ë²”ìœ„: {min(saved_ranks)}ìœ„ ~ {max(saved_ranks)}ìœ„")

    return ranking_data, crawling_stats

# 2ë‹¨ê³„ ì‹¤í–‰
try:
    ranking_data, crawling_stats = crawl_with_preserved_ranks()
    stage2_success = crawling_stats["success_count"] > 0
except Exception as e:
    print(f"âŒ 2ë‹¨ê³„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    stage2_success = False
```

---

## ğŸ“‹ **Phase 3: ìˆœìœ„ ì—°ì†ì„± ê²€ì¦ ì‹œìŠ¤í…œ**

### **ìˆœìœ„ ë¬´ê²°ì„± ì²´í¬**

```python
def verify_rank_continuity():
    """ìˆœìœ„ ì—°ì†ì„± ë° ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦"""

    print(f"\nğŸ” ìˆœìœ„ ì—°ì†ì„± ê²€ì¦ ì‹œì‘...")

    # 1. URL ë°ì´í„° íŒŒì¼ ê²€ì¦
    URL_DATA_FILE = f"klook_urls_data_{CITY_NAME}_{TARGET_TAB.replace('&', 'and').replace(' ', '_')}.json"

    if os.path.exists(URL_DATA_FILE):
        with open(URL_DATA_FILE, 'r', encoding='utf-8') as f:
            url_data = json.load(f)

        expected_ranks = [item["rank"] for item in url_data["url_rank_mapping"]]
        expected_range = f"{min(expected_ranks)}ìœ„ ~ {max(expected_ranks)}ìœ„"

        print(f"   âœ… 1ë‹¨ê³„ ìˆ˜ì§‘ ìˆœìœ„: {expected_range} ({len(expected_ranks)}ê°œ)")
    else:
        print(f"   âŒ URL ë°ì´í„° íŒŒì¼ ì—†ìŒ")
        return False

    # 2. ë­í‚¹ ë°ì´í„° ê²€ì¦
    if 'ranking_data' in locals() and ranking_data:
        actual_ranks = [item["rank"] for item in ranking_data]
        actual_range = f"{min(actual_ranks)}ìœ„ ~ {max(actual_ranks)}ìœ„"

        print(f"   âœ… 2ë‹¨ê³„ ì €ì¥ ìˆœìœ„: {actual_range} ({len(actual_ranks)}ê°œ)")

        # ìˆœìœ„ ì—°ì†ì„± ì²´í¬
        missing_ranks = set(expected_ranks) - set(actual_ranks)
        if missing_ranks:
            missing_sorted = sorted(list(missing_ranks))
            print(f"   âš ï¸ ëˆ„ë½ëœ ìˆœìœ„: {missing_sorted[:10]}{'...' if len(missing_sorted) > 10 else ''}")
            print(f"   ğŸ“Š ëˆ„ë½ ì´ìœ : ì¤‘ë³µ URL ë˜ëŠ” í¬ë¡¤ë§ ì‹¤íŒ¨")
        else:
            print(f"   ğŸ‰ ì™„ë²½í•œ ìˆœìœ„ ì—°ì†ì„± ë‹¬ì„±!")
    else:
        print(f"   âŒ ë­í‚¹ ë°ì´í„° ì—†ìŒ")
        return False

    # 3. CSV ë°ì´í„° ê²€ì¦
    try:
        from src.utils.file_handler import get_csv_stats
        csv_stats = get_csv_stats(CITY_NAME)

        if csv_stats and 'error' not in csv_stats:
            print(f"   âœ… CSV ì €ì¥: {csv_stats.get('total_products', 0)}ê°œ ìƒí’ˆ")

            # ìˆœìœ„-CSVë²ˆí˜¸ ì¼ì¹˜ì„± ì²´í¬
            if 'ranking_data' in locals() and ranking_data:
                csv_numbers = [item["csv_number"] for item in ranking_data]
                print(f"   âœ… CSV ë²ˆí˜¸ ë²”ìœ„: {min(csv_numbers)} ~ {max(csv_numbers)}")
        else:
            print(f"   âŒ CSV ë°ì´í„° í™•ì¸ ì‹¤íŒ¨")

    except Exception as e:
        print(f"   âš ï¸ CSV ê²€ì¦ ì‹¤íŒ¨: {e}")

    print(f"   ğŸ¯ ìˆœìœ„ ì—°ì†ì„± ê²€ì¦ ì™„ë£Œ!")
    return True

# ê²€ì¦ ì‹¤í–‰
verify_rank_continuity()
```

---

## ğŸ“Š **ìµœì¢… ê²°ê³¼: ì™„ë²½í•œ ìˆœìœ„ ì—°ì†ì„±**

### âœ… **ë³´ì¥ë˜ëŠ” ê²ƒë“¤**

1. **ìˆœìœ„ ì •í™•ì„±**: 1ë‹¨ê³„ ìˆ˜ì§‘ ìˆœì„œ = ì‹¤ì œ ìˆœìœ„
2. **ìˆœìœ„ ì—°ì†ì„±**: ì¤‘ë³µ ê±´ë„ˆë›°ê¸°ì™€ ë¬´ê´€í•˜ê²Œ ìˆœìœ„ ë³´ì¡´
3. **ë°ì´í„° ë¬´ê²°ì„±**: ì›ë³¸ í˜ì´ì§€, ìˆ˜ì§‘ ì‹œê°„ ë“± ëª¨ë“  ë©”íƒ€ë°ì´í„° ë³´ì¡´
4. **ì¶”ì  ê°€ëŠ¥ì„±**: ê° ìƒí’ˆì˜ ìˆ˜ì§‘ ê²½ë¡œ ì™„ì „ ì¶”ì 

### ğŸ“‹ **ìƒì„±ë˜ëŠ” ë°ì´í„° êµ¬ì¡°**

#### **1ë‹¨ê³„ ì¶œë ¥: JSON íŒŒì¼**
```json
{
  "collection_info": {
    "city": "ì‚¿í¬ë¡œ",
    "tab": "í‹°ì¼“&ì…ì¥ê¶Œ",
    "timestamp": "2025-09-19T10:30:00",
    "target_products": 10
  },
  "url_rank_mapping": [
    {
      "rank": 1,
      "url": "https://www.klook.com/ko/activity/1304-sapporo...",
      "page": 1,
      "page_index": 1,
      "is_duplicate": false
    }
  ]
}
```

#### **2ë‹¨ê³„ ì¶œë ¥: ì™„ë²½í•œ ìˆœìœ„ ë§¤í•‘**
```json
{
  "rank": 1,
  "csv_number": 157,
  "url": "https://www.klook.com/ko/activity/1304-sapporo...",
  "original_page": 1,
  "collected_at": "2025-09-19T10:25:15",
  "processed_at": "2025-09-19T14:30:22"
}
```

### ğŸ¯ **ìˆœìœ„ ì—°ì†ì„± ì™„ë²½ ë³´ì¥!**

ì´ ì‹œìŠ¤í…œìœ¼ë¡œ ì¤‘ë³µ URLì´ ìˆì–´ë„ ìˆœìœ„ëŠ” ì ˆëŒ€ ì–´ê¸‹ë‚˜ì§€ ì•ŠìŠµë‹ˆë‹¤!