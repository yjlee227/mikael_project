"""
ğŸš€ ë­í‚¹ ë§¤ë‹ˆì €: ì¤‘ë³µ URL ë­í‚¹ ëˆ„ì  ë° ê´€ë¦¬ ì‹œìŠ¤í…œ
- íƒ­ë³„ ë­í‚¹ ì •ë³´ ìˆ˜ì§‘
- ì¤‘ë³µ URL ë­í‚¹ ëˆ„ì  ì €ì¥
- ë­í‚¹ ë°ì´í„° ì¡°íšŒ ë° ë¶„ì„
"""

import os
import json
import hashlib
from datetime import datetime
from collections import defaultdict

# config ëª¨ë“ˆì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë“¤ import
from .config import get_city_code

# =============================================================================
# ğŸ† ë­í‚¹ ë°ì´í„° êµ¬ì¡°
# =============================================================================

class RankingManager:
    """ì¤‘ë³µ URL ë­í‚¹ ëˆ„ì  ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.ranking_dir = "ranking_data"
        self.ranking_cache = {}
        
    def save_tab_ranking(self, urls_with_ranking, city_name, tab_name, strategy):
        """íƒ­ì—ì„œ ìˆ˜ì§‘í•œ URLë“¤ê³¼ ë­í‚¹ ì •ë³´ ì €ì¥"""
        if not urls_with_ranking:
            return False
        
        try:
            os.makedirs(self.ranking_dir, exist_ok=True)
            
            city_code = get_city_code(city_name)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{city_code}_{tab_name}_{strategy}_{timestamp}.json"
            filepath = os.path.join(self.ranking_dir, filename)
            
            # URLë³„ ë­í‚¹ ì •ë³´ êµ¬ì¡°í™”
            ranking_data = {
                "city_name": city_name,
                "city_code": city_code,
                "tab_name": tab_name,
                "strategy": strategy,
                "collected_at": datetime.now().isoformat(),
                "total_urls": len(urls_with_ranking),
                "url_rankings": []
            }
            
            for idx, url in enumerate(urls_with_ranking, 1):
                url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()
                
                ranking_data["url_rankings"].append({
                    "url": url,
                    "url_hash": url_hash,
                    "tab_name": tab_name,
                    "tab_ranking": idx,
                    "found_at": datetime.now().isoformat()
                })
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(ranking_data, f, ensure_ascii=False, indent=2)
            
            print(f"    ğŸ’¾ ë­í‚¹ ë°ì´í„° ì €ì¥: {filename} ({len(urls_with_ranking)}ê°œ)")
            
            # ë­í‚¹ ëˆ„ì  ì—…ë°ì´íŠ¸
            self._update_accumulated_rankings(ranking_data)
            
            return True
            
        except Exception as e:
            print(f"    âŒ ë­í‚¹ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def _update_accumulated_rankings(self, ranking_data):
        """ì¤‘ë³µ URL ë­í‚¹ ëˆ„ì  ì—…ë°ì´íŠ¸"""
        try:
            city_code = ranking_data["city_code"]
            accumulated_file = os.path.join(self.ranking_dir, f"{city_code}_accumulated_rankings.json")
            
            # ê¸°ì¡´ ëˆ„ì  ë°ì´í„° ë¡œë“œ
            if os.path.exists(accumulated_file):
                with open(accumulated_file, 'r', encoding='utf-8') as f:
                    accumulated = json.load(f)
            else:
                accumulated = {
                    "city_name": ranking_data["city_name"],
                    "city_code": city_code,
                    "last_updated": None,
                    "url_rankings": {},
                    "stats": {
                        "total_urls": 0,
                        "tabs_processed": [],
                        "duplicate_urls": 0
                    }
                }
            
            # ìƒˆ ë­í‚¹ ë°ì´í„° ëˆ„ì 
            tab_name = ranking_data["tab_name"]
            
            for url_info in ranking_data["url_rankings"]:
                url_hash = url_info["url_hash"]
                url = url_info["url"]
                
                if url_hash not in accumulated["url_rankings"]:
                    # ìƒˆë¡œìš´ URL
                    accumulated["url_rankings"][url_hash] = {
                        "url": url,
                        "url_hash": url_hash,
                        "first_found": url_info["found_at"],
                        "tab_rankings": {},
                        "is_duplicate": False
                    }
                else:
                    # ì¤‘ë³µ URL - ì¤‘ë³µ í‘œì‹œ
                    accumulated["url_rankings"][url_hash]["is_duplicate"] = True
                    accumulated["stats"]["duplicate_urls"] += 1
                
                # íƒ­ë³„ ë­í‚¹ ì •ë³´ ì¶”ê°€
                accumulated["url_rankings"][url_hash]["tab_rankings"][tab_name] = {
                    "ranking": url_info["tab_ranking"],
                    "found_at": url_info["found_at"]
                }
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            accumulated["last_updated"] = datetime.now().isoformat()
            accumulated["stats"]["total_urls"] = len(accumulated["url_rankings"])
            if tab_name not in accumulated["stats"]["tabs_processed"]:
                accumulated["stats"]["tabs_processed"].append(tab_name)
            
            # ëˆ„ì  ë°ì´í„° ì €ì¥
            with open(accumulated_file, 'w', encoding='utf-8') as f:
                json.dump(accumulated, f, ensure_ascii=False, indent=2)
            
            print(f"    ğŸ“Š ëˆ„ì  ë­í‚¹ ì—…ë°ì´íŠ¸: ì´ {accumulated['stats']['total_urls']}ê°œ URL, ì¤‘ë³µ {accumulated['stats']['duplicate_urls']}ê°œ")
            
        except Exception as e:
            print(f"    âŒ ëˆ„ì  ë­í‚¹ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def get_url_rankings(self, url, city_name):
        """íŠ¹ì • URLì˜ ëª¨ë“  íƒ­ ë­í‚¹ ì •ë³´ ì¡°íšŒ"""
        try:
            city_code = get_city_code(city_name)
            accumulated_file = os.path.join(self.ranking_dir, f"{city_code}_accumulated_rankings.json")
            
            if not os.path.exists(accumulated_file):
                return None
            
            with open(accumulated_file, 'r', encoding='utf-8') as f:
                accumulated = json.load(f)
            
            url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()
            
            if url_hash in accumulated["url_rankings"]:
                return accumulated["url_rankings"][url_hash]
            
            return None
            
        except Exception as e:
            print(f"âŒ URL ë­í‚¹ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def get_city_ranking_stats(self, city_name):
        """ë„ì‹œë³„ ë­í‚¹ í†µê³„ ì¡°íšŒ"""
        try:
            city_code = get_city_code(city_name)
            accumulated_file = os.path.join(self.ranking_dir, f"{city_code}_accumulated_rankings.json")
            
            if not os.path.exists(accumulated_file):
                return None
            
            with open(accumulated_file, 'r', encoding='utf-8') as f:
                accumulated = json.load(f)
            
            return accumulated["stats"]
            
        except Exception as e:
            print(f"âŒ ë­í‚¹ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def should_crawl_url(self, url, city_name):
        """URLì´ í¬ë¡¤ë§ ëŒ€ìƒì¸ì§€ í™•ì¸ (ì²« ë²ˆì§¸ ë°œê²¬ëœ íƒ­ì—ì„œë§Œ í¬ë¡¤ë§)"""
        url_rankings = self.get_url_rankings(url, city_name)
        
        if not url_rankings:
            return True  # ìƒˆë¡œìš´ URLì€ í¬ë¡¤ë§
        
        # ì´ë¯¸ í¬ë¡¤ë§ëœ URLì¸ì§€ í™•ì¸
        return not url_rankings.get("crawled", False)
    
    def mark_url_crawled(self, url, city_name):
        """URLì„ í¬ë¡¤ë§ ì™„ë£Œë¡œ í‘œì‹œ"""
        try:
            city_code = get_city_code(city_name)
            accumulated_file = os.path.join(self.ranking_dir, f"{city_code}_accumulated_rankings.json")
            
            if not os.path.exists(accumulated_file):
                return False
            
            with open(accumulated_file, 'r', encoding='utf-8') as f:
                accumulated = json.load(f)
            
            url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()
            
            if url_hash in accumulated["url_rankings"]:
                accumulated["url_rankings"][url_hash]["crawled"] = True
                accumulated["url_rankings"][url_hash]["crawled_at"] = datetime.now().isoformat()
                
                with open(accumulated_file, 'w', encoding='utf-8') as f:
                    json.dump(accumulated, f, ensure_ascii=False, indent=2)
                
                return True
            
            return False
            
        except Exception as e:
            print(f"âŒ URL í¬ë¡¤ë§ ì™„ë£Œ í‘œì‹œ ì‹¤íŒ¨: {e}")
            return False
    
    def get_collected_ranks(self, city_name, tab_name=None):
        """ğŸ†• íŠ¹ì • ë„ì‹œì—ì„œ ì´ë¯¸ ìˆ˜ì§‘ëœ ìˆœìœ„ë“¤ ì¡°íšŒ (ë²”ìš©)"""
        try:
            city_code = get_city_code(city_name)
            accumulated_file = os.path.join(self.ranking_dir, f"{city_code}_accumulated_rankings.json")
            
            if not os.path.exists(accumulated_file):
                return []
            
            with open(accumulated_file, 'r', encoding='utf-8') as f:
                accumulated = json.load(f)
            
            collected_ranks = set()
            
            # ëª¨ë“  URLì˜ ë­í‚¹ ì •ë³´ì—ì„œ ìˆ˜ì§‘ëœ ìˆœìœ„ ì¶”ì¶œ
            for url_hash, url_info in accumulated["url_rankings"].items():
                if url_info.get("crawled", False):  # ì‹¤ì œë¡œ í¬ë¡¤ë§ ì™„ë£Œëœ ê²ƒë§Œ
                    tab_rankings = url_info.get("tab_rankings", {})
                    
                    if tab_name:
                        # íŠ¹ì • íƒ­ì˜ ìˆœìœ„ë§Œ
                        if tab_name in tab_rankings:
                            collected_ranks.add(tab_rankings[tab_name]["ranking"])
                    else:
                        # ëª¨ë“  íƒ­ì˜ ìˆœìœ„
                        for tab, ranking_info in tab_rankings.items():
                            collected_ranks.add(ranking_info["ranking"])
            
            return sorted(list(collected_ranks))
            
        except Exception as e:
            print(f"âŒ ìˆ˜ì§‘ëœ ìˆœìœ„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_next_available_rank(self, city_name, tab_name=None, start_from=1):
        """ğŸ†• ë‹¤ìŒ ê°€ìš©í•œ ìˆœìœ„ ì°¾ê¸° (ë²”ìš©)"""
        try:
            collected_ranks = self.get_collected_ranks(city_name, tab_name)
            
            # start_fromë¶€í„° ì‹œì‘í•´ì„œ ì²« ë²ˆì§¸ ë¹ˆ ìˆœìœ„ ì°¾ê¸°
            next_rank = start_from
            while next_rank in collected_ranks:
                next_rank += 1
            
            return next_rank
            
        except Exception as e:
            print(f"âŒ ë‹¤ìŒ ê°€ìš© ìˆœìœ„ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            return start_from
    
    def get_next_available_range(self, city_name, count=3, tab_name=None, fill_gaps=True):
        """ğŸ†• ë‹¤ìŒ ìˆ˜ì§‘ ê°€ëŠ¥í•œ ìˆœìœ„ ë²”ìœ„ ê³„ì‚° (ë²”ìš© - í•µì‹¬ ê¸°ëŠ¥)"""
        try:
            collected_ranks = self.get_collected_ranks(city_name, tab_name)
            
            if not collected_ranks:
                # ì•„ë¬´ê²ƒë„ ìˆ˜ì§‘ë˜ì§€ ì•Šì€ ê²½ìš°
                return 1, count
            
            available_ranks = []
            
            if fill_gaps:
                # ê°­ ì±„ìš°ê¸° ëª¨ë“œ: 1ë¶€í„° ì‹œì‘í•´ì„œ ë¹ˆ ìˆœìœ„ë“¤ ì°¾ê¸°
                max_collected = max(collected_ranks)
                for rank in range(1, max_collected + count + 1):
                    if rank not in collected_ranks:
                        available_ranks.append(rank)
                    if len(available_ranks) >= count:
                        break
            else:
                # ì—°ì† ëª¨ë“œ: ë§ˆì§€ë§‰ ìˆ˜ì§‘ ìˆœìœ„ ë‹¤ìŒë¶€í„°
                max_collected = max(collected_ranks)
                for rank in range(max_collected + 1, max_collected + count + 1):
                    available_ranks.append(rank)
            
            if available_ranks:
                return min(available_ranks), max(available_ranks)
            else:
                # ëª¨ë“  ìˆœìœ„ê°€ ì±„ì›Œì§„ ê²½ìš°
                max_collected = max(collected_ranks) if collected_ranks else 0
                return max_collected + 1, max_collected + count
                
        except Exception as e:
            print(f"âŒ ë‹¤ìŒ ê°€ìš© ë²”ìœ„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 1, count
    
    def clear_city_data(self, city_name):
        """ğŸ†• íŠ¹ì • ë„ì‹œì˜ ë­í‚¹ ë°ì´í„° ì™„ì „ ì´ˆê¸°í™”"""
        try:
            city_code = get_city_code(city_name)
            accumulated_file = os.path.join(self.ranking_dir, f"{city_code}_accumulated_rankings.json")
            
            if os.path.exists(accumulated_file):
                os.remove(accumulated_file)
                print(f"âœ… '{city_name}' ë­í‚¹ ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ")
                return True
            else:
                print(f"â„¹ï¸ '{city_name}' ë­í‚¹ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return True
                
        except Exception as e:
            print(f"âŒ ë­í‚¹ ë°ì´í„° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

# ì „ì—­ ë­í‚¹ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
ranking_manager = RankingManager()

print("âœ… ë­í‚¹ ë§¤ë‹ˆì € ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ!")
print("   ğŸ† ê¸°ëŠ¥:")
print("   - save_tab_ranking(): íƒ­ë³„ ë­í‚¹ ë°ì´í„° ì €ì¥")
print("   - get_url_rankings(): URLë³„ ëª¨ë“  íƒ­ ë­í‚¹ ì¡°íšŒ")
print("   - should_crawl_url(): ì¤‘ë³µ URL í¬ë¡¤ë§ ì—¬ë¶€ í™•ì¸")
print("   - mark_url_crawled(): í¬ë¡¤ë§ ì™„ë£Œ í‘œì‹œ")