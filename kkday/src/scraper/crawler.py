"""
ë©”ì¸ í¬ë¡¤ë§ ì—”ì§„
- ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ í†µí•© ê´€ë¦¬
- í˜ì´ì§€ë„¤ì´ì…˜ ë° ë°ì´í„° ìˆ˜ì§‘
- ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ ì‹œìŠ¤í…œ
"""

import time
import random
import re
from datetime import datetime

from ..config import CONFIG, SELENIUM_AVAILABLE
from ..utils.file_handler import create_product_data_structure, save_to_csv_kkday, get_dual_image_urls_kkday, download_and_save_image_kkday, ensure_directory_structure
from .driver_manager import setup_driver, go_to_main_page, find_and_fill_search, click_search_button, handle_kkday_cookie_popup, handle_popup, smart_scroll_selector
from .url_manager import collect_urls_from_page, get_pagination_urls, is_url_already_processed, mark_url_as_processed, go_to_next_page
from .parsers import extract_all_product_data, validate_product_data
from .ranking import save_url_with_rank, get_next_start_rank
from . import human_scroll_patterns

if SELENIUM_AVAILABLE:
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException

# =============================================================================
# ë©”ì¸ í¬ë¡¤ë§ í´ë˜ìŠ¤
# =============================================================================

class KKdayCrawler:
    """KKday í¬ë¡¤ë§ í†µí•© ì‹œìŠ¤í…œ"""

    def __init__(self, city_name="ì„œìš¸"):
        self.city_name = city_name
        self.driver = None
        self.stats = {
            "start_time": None,
            "end_time": None,
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "skip_count": 0,
            "urls_collected": 0,
            "current_rank": 0
        }

    def initialize(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        print(f"ğŸš€ KKday í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”: {self.city_name}")
        try:
            # ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ë³´
            ensure_directory_structure(self.city_name)
            
            # ë“œë¼ì´ë²„ ì„¤ì •
            self.driver = setup_driver()
            if not self.driver:
                raise Exception("ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨")
            time.sleep(random.uniform(1, 3))
            
            # [ìˆ˜ì •] ë„ì‹œë³„ ìƒí’ˆ ëª©ë¡ URLë¡œ ì§ì ‘ ì´ë™
            from urllib.parse import quote
            encoded_city_name = quote(self.city_name)
            target_url = f"https://www.kkday.com/ko/product/productlist/{encoded_city_name}"
            print(f"ìƒí’ˆ ëª©ë¡ í˜ì´ì§€ë¡œ ì§ì ‘ ì´ë™: {target_url}")
            
            self.driver.get(target_url)
            time.sleep(random.uniform(3, 5))  # í˜ì´ì§€ ë¡œë“œë¥¼ ìœ„í•œ ìµœì†Œ ëŒ€ê¸°
            
            # íŒì—… ì²˜ë¦¬ ë° ìŠ¤í¬ë¡¤
            handle_popup(self.driver)
            smart_scroll_selector(self.driver)
            time.sleep(random.uniform(2, 5))
            
            self.stats["start_time"] = datetime.now()
            print("âœ… í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if self.driver:
                pass
            return False

    def collect_urls(self, max_pages=3, max_products=None):
        """URL ìˆ˜ì§‘ (íš¨ìœ¨í™” ë²„ì „: max_productsì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨)"""
        print(f"ğŸ”— URL ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€, ëª©í‘œ ìƒí’ˆ: {max_products or 'ì œí•œ ì—†ìŒ'})")
        time.sleep(random.uniform(2, 4))

        all_product_urls = []
        seen_urls = set()
        current_page = 1

        try:
            while current_page <= max_pages:
                print(f"  ğŸ“„ {current_page}í˜ì´ì§€ íƒìƒ‰ ì¤‘... (í˜„ì¬ ìˆ˜ì§‘: {len(all_product_urls)}ê°œ)")

                # í˜„ì¬ í˜ì´ì§€ì—ì„œ URL ìˆ˜ì§‘
                page_urls = collect_urls_from_page(self.driver, self.city_name)
                if not page_urls:
                    print("  âš ï¸ í˜„ì¬ í˜ì´ì§€ì—ì„œ URLì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break
                
                # ìƒí’ˆ URLë§Œ í•„í„°ë§
                product_urls_on_page = self.filter_product_detail_urls(page_urls)

                # ìƒˆë¡œìš´ ìƒí’ˆ URLë§Œ ì¶”ê°€
                for url in product_urls_on_page:
                    if url not in seen_urls:
                        all_product_urls.append(url)
                        seen_urls.add(url)

                # ëª©í‘œ ìƒí’ˆ ìˆ˜ì— ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸
                if max_products and len(all_product_urls) >= max_products:
                    print(f"  ğŸ¯ ëª©í‘œ ìƒí’ˆ ìˆ˜({max_products}ê°œ)ì— ë„ë‹¬í•˜ì—¬ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break

                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                if current_page < max_pages:
                    if not go_to_next_page(self.driver):
                        print("  â„¹ï¸ ë” ì´ìƒ ë‹¤ìŒ í˜ì´ì§€ê°€ ì—†ì–´ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        break
                
                current_page += 1
                # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
                time.sleep(random.uniform(2, 5))

            # ë¯¸ì²˜ë¦¬ URL í•„í„°ë§
            unprocessed_urls = []
            for url in all_product_urls:
                if not is_url_already_processed(url, self.city_name):
                    unprocessed_urls.append(url)

            self.stats["urls_collected"] = len(all_product_urls)
            print(f"âœ… URL ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_product_urls)}ê°œ ìƒí’ˆ URL, ë¯¸ì²˜ë¦¬ {len(unprocessed_urls)}ê°œ")
            
            # max_productsì— ë§ì¶° ìµœì¢… ê²°ê³¼ ìŠ¬ë¼ì´ì‹± (ì•ˆì „ì¥ì¹˜)
            if max_products:
                return unprocessed_urls[:max_products]
            return unprocessed_urls

        except Exception as e:
            print(f"âŒ URL ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return []

    def filter_product_detail_urls(self, urls):
        """ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ URLë§Œ í•„í„°ë§ (ëª©ë¡ í˜ì´ì§€ ì œì™¸)"""
        # ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ íŒ¨í„´ (ë” í¬ê´„ì ìœ¼ë¡œ ìˆ˜ì •)
        product_detail_patterns = [
            r'/ko/product/\d+$',                           # /ko/product/287674
            r'/ko/product/\d+-[\w\-]+',                    # /ko/product/146520-krabi-kayaking
            r'/product/\d+',                               # /product/287674 (ì–¸ì–´ ìƒëµ)
            r'/product/\d+-[\w\-]+',                       # /product/146520-krabi-kayaking
        ]

        filtered_urls = []
        excluded_count = 0

        for url in urls:
            # ëª©ë¡ í˜ì´ì§€ íŒ¨í„´ ì œì™¸
            if 'productlist' in url:
                excluded_count += 1
                print(f"  ğŸš« ëª©ë¡ í˜ì´ì§€ ì œì™¸: {url}")
                continue

            # ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ íŒ¨í„´ í™•ì¸
            is_product_detail = any(re.search(pattern, url) for pattern in product_detail_patterns)

            if is_product_detail:
                filtered_urls.append(url)
            else:
                excluded_count += 1
                print(f"  ğŸš« ë¹„ìƒí’ˆ í˜ì´ì§€ ì œì™¸: {url}")

        print(f"âœ… URL í•„í„°ë§ ì™„ë£Œ: {len(filtered_urls)}ê°œ ìœ ì§€, {excluded_count}ê°œ ì œì™¸")
        return filtered_urls

    def get_next_available_rank(self):
        """ë„ì‹œë³„ ë‹¤ìŒ ì‚¬ìš© ê°€ëŠ¥í•œ ìˆœìœ„ ì¡°íšŒ"""
        try:
            next_rank = get_next_start_rank(self.city_name)
            print(f"ğŸ“Š {self.city_name} ë‹¤ìŒ ìˆœìœ„: {next_rank}")
            return next_rank
        except Exception as e:
            print(f"âš ï¸ ìˆœìœ„ ì¡°íšŒ ì‹¤íŒ¨, 1ë¶€í„° ì‹œì‘: {e}")
            return 1

    def crawl_product(self, url, rank=None):
        """ê°œë³„ ìƒí’ˆ í¬ë¡¤ë§"""
        print(f"ğŸ” ìƒí’ˆ í¬ë¡¤ë§ ì‹œì‘: ìˆœìœ„ {rank}")
        try:
            # ìƒí’ˆ í˜ì´ì§€ ì´ë™
            self.driver.get(url)
            time.sleep(random.uniform(3, 8))
            
            # [ì¶”ê°€] ì¸ê°„ í–‰ë™ ê¸°ë°˜ ìŠ¤í¬ë¡¤ ì‹¤í–‰ 
            print("   - ğŸ¤– ì¸ê°„ í–‰ë™ ê¸°ë°˜ ìŠ¤í¬ë¡¤ ì‹œì‘...")
            try:
                human_scroll_patterns.simulate_human_scroll(self.driver)
            except Exception as e:
                print(f"   - âš ï¸ ìŠ¤í¬ë¡¤ íŒ¨í„´ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:{e}")
                # ìŠ¤í¬ë¡¤ì— ì‹¤íŒ¨í•´ë„ ë°ì´í„° ìˆ˜ì§‘ì€ ê³„ì† ì‹œë„
                pass 
            
            # ë°ì´í„° ì¶”ì¶œ
            product_data = extract_all_product_data(self.driver, url, rank, city_name=self.city_name)
            time.sleep(random.uniform(4, 9))

            # ë°ì´í„° ê²€ì¦
            if not validate_product_data(product_data):
                print(f"âš ï¸ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: ìˆœìœ„ {rank}")
                self.stats["error_count"] += 1
                return False
            
            # ê¸°ë³¸ êµ¬ì¡°ì— ë§ì¶° ë°ì´í„° ë³‘í•©
            base_data = create_product_data_structure(self.city_name, self.stats["total_processed"] + 1, rank)
            base_data.update(product_data)
            
            # ì´ë¯¸ì§€ ì²˜ë¦¬
            try:
                main_img_url, thumb_img_url = get_dual_image_urls_kkday(self.driver)
                # íŒŒì¼ëª…ì— ìˆœì°¨ì ì¸ rankë¥¼ ì‚¬ìš© (ì—†ìœ¼ë©´ 0ë²ˆ)
                image_identifier = rank if rank is not None else 0
                
                if main_img_url:
                    if CONFIG.get("SAVE_IMAGES", False):
                        print("    ğŸ“¥ ë©”ì¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                        time.sleep(random.uniform(0.5, 1.5))  # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì „ ì§§ì€ ëŒ€ê¸°

                        main_img_filename = download_and_save_image_kkday(
                            main_img_url,
                            image_identifier,
                            self.city_name,
                            image_type="main"
                        )
                        base_data["ë©”ì¸ì´ë¯¸ì§€"] = main_img_filename if main_img_filename else "ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"
                        time.sleep(random.uniform(1, 2))

                    else:
                        base_data["ë©”ì¸ì´ë¯¸ì§€"] = main_img_url
                        
                if thumb_img_url:
                    if CONFIG.get("SAVE_IMAGES", False):
                        print("    ğŸ“¥ ì¸ë„¤ì¼ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                        thumb_img_filename = download_and_save_image_kkday(
                            thumb_img_url,
                            image_identifier,
                            self.city_name,
                            image_type="thumb"
                        )
                        base_data["ì¸ë„¤ì¼ì´ë¯¸ì§€"] = thumb_img_filename if thumb_img_filename else "ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"
                        time.sleep(random.uniform(1, 2))
                    else:
                        base_data["ì¸ë„¤ì¼ì´ë¯¸ì§€"] = thumb_img_url
                        
            except Exception as e:
                import traceback
                print(f"  âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                traceback.print_exc()
            
            # CSV ì €ì¥
            if save_to_csv_kkday(base_data, self.city_name):
                # ìˆœìœ„ ì •ë³´ ì €ì¥ (product_id í¬í•¨)
                save_url_with_rank(url, rank, self.city_name, base_data["ìƒí’ˆë²ˆí˜¸"])
                
                # URL ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
                mark_url_as_processed(url, self.city_name, base_data["ìƒí’ˆë²ˆí˜¸"], rank)
                
                self.stats["success_count"] += 1
                self.stats["current_rank"] = rank
                print(f"âœ… ìƒí’ˆ í¬ë¡¤ë§ ì™„ë£Œ: ìˆœìœ„ {rank}")
                return True
            else:
                self.stats["error_count"] += 1
                return False
                
        except Exception as e:
            print(f"âŒ ìƒí’ˆ í¬ë¡¤ë§ ì‹¤íŒ¨ (ìˆœìœ„ {rank}): {e}")
            self.stats["error_count"] += 1
            return False
        finally:
            self.stats["total_processed"] += 1
    
    def crawl_products_batch(self, urls):
        """ë°°ì¹˜ ìƒí’ˆ í¬ë¡¤ë§"""
        # ì‹œì‘ ìˆœìœ„ ìë™ ê³„ì‚°
        start_rank = self.get_next_available_rank()
        print(f"ğŸ“¦ ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘: {len(urls)}ê°œ ìƒí’ˆ (ì‹œì‘ ìˆœìœ„: {start_rank})")

        current_rank = start_rank

        for i, url in enumerate(urls):
            time.sleep(random.uniform(2, 4))
            print(f"\n{'='*50}")
            print(f"ì§„í–‰ë¥ : {i+1}/{len(urls)} ({((i+1)/len(urls)*100):.1f}%)")

            # ì´ë¯¸ ì²˜ë¦¬ëœ URLì¸ì§€ í™•ì¸
            if is_url_already_processed(url, self.city_name):
                print(f"â­ï¸ ì´ë¯¸ ì²˜ë¦¬ëœ URL, ê±´ë„ˆëœ€")
                self.stats["skip_count"] += 1
                continue

            # ìƒí’ˆ í¬ë¡¤ë§
            success = self.crawl_product(url, current_rank)

            if success:
                current_rank += 1

            # ì§„í–‰ìƒí™© ì¶œë ¥
            self.print_progress()

            # ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€ê¸°
            delay = random.uniform(
                CONFIG.get("MEDIUM_MIN_DELAY", 3),
                CONFIG.get("MEDIUM_MAX_DELAY", 8)
            )
            print(f"â³ {delay:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(delay)

            # ì¤‘ê°„ íœ´ì‹ (10ê°œë§ˆë‹¤)
            if (i + 1) % 10 == 0:
                long_delay = random.uniform(
                    CONFIG.get("LONG_MIN_DELAY", 15),
                    CONFIG.get("LONG_MAX_DELAY", 30)
                )
                print(f"ğŸ˜´ ê¸´ íœ´ì‹: {long_delay:.1f}ì´ˆ...")
                time.sleep(long_delay)

        print("\nğŸ“¦ ë°°ì¹˜ í¬ë¡¤ë§ ì™„ë£Œ")
        return True

    def run_full_crawling(self, max_pages=3, max_products=None):
        """ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰"""
        print(f"ğŸ¯ {self.city_name} ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")

        try:
            # 1. ì´ˆê¸°í™”
            if not self.initialize():
                return False

            # 2. URL ìˆ˜ì§‘
            urls = self.collect_urls(max_pages)
            if not urls:
                print("âš ï¸ ìˆ˜ì§‘í•  URLì´ ì—†ìŠµë‹ˆë‹¤.")
                return False

            # 2.1 ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ URLë§Œ í•„í„°ë§ (ëª©ë¡ í˜ì´ì§€ ì œì™¸)
            product_urls = self.filter_product_detail_urls(urls)
            if not product_urls:
                print("âš ï¸ ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ URLì´ ì—†ìŠµë‹ˆë‹¤.")
                return False

            print(f"ğŸ“Š ì „ì²´ URL: {len(urls)}ê°œ, ìƒí’ˆ ìƒì„¸ URL: {len(product_urls)}ê°œ")

            # 3. ìµœëŒ€ ìƒí’ˆ ìˆ˜ ì œí•œ
            max_products_config = CONFIG.get("MAX_PRODUCTS_PER_CITY", None)
            if max_products:
                product_urls = product_urls[:max_products]
            elif max_products_config:
                product_urls = product_urls[:max_products_config]

            print(f"ğŸ“Š í¬ë¡¤ë§í•  ìƒí’ˆ ìˆ˜: {len(product_urls)}ê°œ")

            # 4. ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰ (ìˆœìœ„ëŠ” ë‚´ë¶€ì—ì„œ ìë™ ê³„ì‚°)
            success = self.crawl_products_batch(product_urls)

            # 5. ìµœì¢… í†µê³„
            self.stats["end_time"] = datetime.now()
            self.print_final_stats()

            # ğŸ†• êµ­ê°€ë³„ í†µí•© CSV ìë™ ìƒì„± (ì—¬ê¸°ì— ì¶”ê°€)
            from ..utils.file_handler import auto_create_country_csv_after_crawling
            auto_create_country_csv_after_crawling(self.city_name)

            return success

        except Exception as e:
            print(f"âŒ ì „ì²´ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return False
        finally:
            if self.driver:
                pass

    def print_progress(self):
        """ì§„í–‰ìƒí™© ì¶œë ¥"""
        total = self.stats["total_processed"]
        success = self.stats["success_count"]
        error = self.stats["error_count"]
        skip = self.stats["skip_count"]

        if total > 0:
            success_rate = (success / total) * 100
            print(f"ğŸ“Š ì§„í–‰ìƒí™©: ì„±ê³µ {success}, ì‹¤íŒ¨ {error}, ê±´ë„ˆëœ€ {skip}, ì„±ê³µë¥  {success_rate:.1f}%")

    def print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        print("\n" + "="*60)
        print(f"ğŸ‰ {self.city_name} í¬ë¡¤ë§ ì™„ë£Œ!")
        print("="*60)

        if self.stats["start_time"] and self.stats["end_time"]:
            duration = self.stats["end_time"] - self.stats["start_time"]
            print(f"â±ï¸ ì†Œìš”ì‹œê°„: {duration}")

        print(f"ğŸ“Š ì²˜ë¦¬ í†µê³„:")
        print(f"   â€¢ ì „ì²´ ì²˜ë¦¬: {self.stats['total_processed']}ê°œ")
        print(f"   â€¢ ì„±ê³µ: {self.stats['success_count']}ê°œ")
        print(f"   â€¢ ì‹¤íŒ¨: {self.stats['error_count']}ê°œ")
        print(f"   â€¢ ê±´ë„ˆëœ€: {self.stats['skip_count']}ê°œ")
        print(f"   â€¢ URL ìˆ˜ì§‘: {self.stats['urls_collected']}ê°œ")
        print(f"   â€¢ ë§ˆì§€ë§‰ ìˆœìœ„: {self.stats['current_rank']}")

        if self.stats["total_processed"] > 0:
            success_rate = (self.stats["success_count"] / self.stats["total_processed"]) * 100
            print(f"   â€¢ ì„±ê³µë¥ : {success_rate:.1f}%")
