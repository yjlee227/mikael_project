# KKday í¬ë¡¤ë§ ì‹œìŠ¤í…œ - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™” ê°€ì´ë“œ

## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê°œìš”

ì´ ë¬¸ì„œëŠ” KKday í¬ë¡¤ë§ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¸¡ì •, ëª¨ë‹ˆí„°ë§, ìµœì í™”í•˜ê¸° ìœ„í•œ ì™„ì „í•œ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¶”ì ë¶€í„° ë³‘ëª© ì§€ì  í•´ê²°ê¹Œì§€ ì²´ê³„ì ì¸ ì ‘ê·¼ ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## ğŸ¯ ì„±ëŠ¥ ëª©í‘œ ë° KPI

### í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ (KPI)

#### ì²˜ë¦¬ ì„±ëŠ¥ ëª©í‘œ
```yaml
í¬ë¡¤ë§ ì†ë„:
  í˜ì´ì§€_ë¡œë”©: "3-5ì´ˆ/í˜ì´ì§€"
  ë°ì´í„°_ì¶”ì¶œ: "1-2ì´ˆ/ìƒí’ˆ"
  ì´ë¯¸ì§€_ë‹¤ìš´ë¡œë“œ: "2-3ì´ˆ/ì´ë¯¸ì§€"
  ì „ì²´_ì²˜ë¦¬ìœ¨: "10-15ê°œ ìƒí’ˆ/ë¶„"

ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰:
  CPU_ì‚¬ìš©ë¥ : "<50% (í‰ê· )"  
  ë©”ëª¨ë¦¬_ì‚¬ìš©ëŸ‰: "<2GB (ìµœëŒ€)"
  ë„¤íŠ¸ì›Œí¬_ëŒ€ì—­í­: "<10MB/ë¶„"
  ë””ìŠ¤í¬_I/O: "<100MB/ë¶„"

ì•ˆì •ì„± ì§€í‘œ:
  ê°€ë™ì‹œê°„: ">99% (24ì‹œê°„ ê¸°ì¤€)"
  ì˜¤ë¥˜ìœ¨: "<5% (ì „ì²´ ì‘ì—… ëŒ€ë¹„)"
  ì¬ì‹œë„_ì„±ê³µë¥ : ">90%"
  ë©”ëª¨ë¦¬_ëˆ„ìˆ˜: "0MB/ì‹œê°„"
```

#### í’ˆì§ˆ ì„±ëŠ¥ ëª©í‘œ
```yaml
ë°ì´í„°_ì •í™•ì„±:
  í•„ìˆ˜_í•„ë“œ_ì¶”ì¶œë¥ : ">95%"
  ê°€ê²©_ì •ë³´_ì •í™•ë„: ">98%"
  ì´ë¯¸ì§€_ë‹¤ìš´ë¡œë“œ_ì„±ê³µë¥ : ">90%"
  ì¤‘ë³µ_ì œê±°_ì •í™•ë„: ">99%"

ì‚¬ìš©ì„± ì§€í‘œ:
  ì´ˆê¸°_ì„¤ì •ì‹œê°„: "<10ë¶„"
  í‰ê· _ì‘ë‹µì‹œê°„: "<3ì´ˆ"
  UI_ë°˜ì‘ì†ë„: "<1ì´ˆ"
  ë¡œê·¸_ê°€ë…ì„±: "ìƒ/ì¤‘/í•˜ í‰ê°€"
```

## ğŸ“ˆ ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

### ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í´ë˜ìŠ¤

#### ì¢…í•© ì„±ëŠ¥ ëª¨ë‹ˆí„° êµ¬í˜„
```python
import time
import psutil
import threading
import logging
from collections import deque
from datetime import datetime, timedelta

class KKdayPerformanceMonitor:
    """KKday í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self, window_size=100):
        self.window_size = window_size
        self.metrics = {
            'response_times': deque(maxlen=window_size),
            'memory_usage': deque(maxlen=window_size), 
            'cpu_usage': deque(maxlen=window_size),
            'success_count': 0,
            'error_count': 0,
            'start_time': time.time()
        }
        
        self.process = psutil.Process()
        self.monitoring = False
        self.monitor_thread = None
        
        # ì„±ëŠ¥ ë¡œê±° ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - ì„±ëŠ¥ - %(message)s',
            handlers=[
                logging.FileHandler('performance.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('performance')
    
    def start_monitoring(self, interval=5):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_system_metrics,
            args=(interval,)
        )
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        
        self.logger.info("ğŸš€ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ë‹¨"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
        self.logger.info("ğŸ›‘ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¤‘ë‹¨")
    
    def _monitor_system_metrics(self, interval):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì£¼ê¸°ì  ìˆ˜ì§‘"""
        while self.monitoring:
            try:
                # CPU ì‚¬ìš©ë¥ 
                cpu_percent = psutil.cpu_percent()
                self.metrics['cpu_usage'].append(cpu_percent)
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                memory_info = self.process.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024
                self.metrics['memory_usage'].append(memory_mb)
                
                # ì„ê³„ê°’ ì²´í¬
                if cpu_percent > 80:
                    self.logger.warning(f"âš ï¸ CPU ì‚¬ìš©ë¥  ë†’ìŒ: {cpu_percent}%")
                
                if memory_mb > 1500:  # 1.5GB
                    self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_mb:.1f}MB")
                
                time.sleep(interval)
                
            except Exception as e:
                self.logger.error(f"âŒ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                time.sleep(interval)
    
    def record_operation(self, operation_name, duration, success=True):
        """ê°œë³„ ì‘ì—… ì„±ëŠ¥ ê¸°ë¡"""
        self.metrics['response_times'].append({
            'operation': operation_name,
            'duration': duration,
            'timestamp': time.time(),
            'success': success
        })
        
        if success:
            self.metrics['success_count'] += 1
        else:
            self.metrics['error_count'] += 1
        
        # ì„±ëŠ¥ ë¡œê¹…
        status = "âœ…" if success else "âŒ"
        self.logger.info(f"{status} {operation_name}: {duration:.2f}ì´ˆ")
    
    def get_performance_summary(self):
        """ì„±ëŠ¥ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        current_time = time.time()
        uptime = current_time - self.metrics['start_time']
        
        # ì‘ë‹µì‹œê°„ í†µê³„
        if self.metrics['response_times']:
            response_times = [r['duration'] for r in self.metrics['response_times']]
            avg_response = sum(response_times) / len(response_times)
            max_response = max(response_times)
            min_response = min(response_times)
        else:
            avg_response = max_response = min_response = 0
        
        # ë©”ëª¨ë¦¬/CPU í†µê³„
        if self.metrics['memory_usage']:
            avg_memory = sum(self.metrics['memory_usage']) / len(self.metrics['memory_usage'])
            max_memory = max(self.metrics['memory_usage'])
        else:
            avg_memory = max_memory = 0
            
        if self.metrics['cpu_usage']:
            avg_cpu = sum(self.metrics['cpu_usage']) / len(self.metrics['cpu_usage'])
            max_cpu = max(self.metrics['cpu_usage'])
        else:
            avg_cpu = max_cpu = 0
        
        # ì„±ê³µë¥  ê³„ì‚°
        total_operations = self.metrics['success_count'] + self.metrics['error_count']
        success_rate = (self.metrics['success_count'] / max(total_operations, 1)) * 100
        
        summary = {
            'uptime_hours': uptime / 3600,
            'total_operations': total_operations,
            'success_rate': success_rate,
            'performance': {
                'avg_response_time': avg_response,
                'max_response_time': max_response,
                'min_response_time': min_response
            },
            'resources': {
                'avg_memory_mb': avg_memory,
                'max_memory_mb': max_memory,
                'avg_cpu_percent': avg_cpu,
                'max_cpu_percent': max_cpu
            }
        }
        
        return summary
    
    def generate_performance_report(self):
        """ìƒì„¸ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        summary = self.get_performance_summary()
        
        report = f"""
# KKday í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¦¬í¬íŠ¸

**ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**ê°€ë™ ì‹œê°„**: {summary['uptime_hours']:.1f}ì‹œê°„
**ì´ ì‘ì—… ìˆ˜**: {summary['total_operations']}ê°œ
**ì„±ê³µë¥ **: {summary['success_rate']:.1f}%

## ğŸ“Š ì„±ëŠ¥ ì§€í‘œ

### ì‘ë‹µ ì‹œê°„
- **í‰ê· **: {summary['performance']['avg_response_time']:.2f}ì´ˆ
- **ìµœëŒ€**: {summary['performance']['max_response_time']:.2f}ì´ˆ  
- **ìµœì†Œ**: {summary['performance']['min_response_time']:.2f}ì´ˆ

### ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
- **í‰ê·  ë©”ëª¨ë¦¬**: {summary['resources']['avg_memory_mb']:.1f}MB
- **ìµœëŒ€ ë©”ëª¨ë¦¬**: {summary['resources']['max_memory_mb']:.1f}MB
- **í‰ê·  CPU**: {summary['resources']['avg_cpu_percent']:.1f}%
- **ìµœëŒ€ CPU**: {summary['resources']['max_cpu_percent']:.1f}%

## ğŸ“ˆ ì„±ëŠ¥ í‰ê°€

### ëª©í‘œ ë‹¬ì„±ë„
"""
        
        # ëª©í‘œ ë‹¬ì„±ë„ í‰ê°€
        evaluations = self._evaluate_performance(summary)
        for category, result in evaluations.items():
            status = "âœ…" if result['passed'] else "âŒ"
            report += f"- **{category}**: {status} {result['message']}\n"
        
        report += f"""

## ğŸ” ê¶Œì¥ì‚¬í•­
{self._generate_recommendations(summary)}
        """
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'performance_report_{timestamp}.md'
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        self.logger.info(f"ğŸ“‹ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±: {filename}")
        return report
    
    def _evaluate_performance(self, summary):
        """ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±ë„ í‰ê°€"""
        evaluations = {}
        
        # ì‘ë‹µ ì‹œê°„ í‰ê°€ (ëª©í‘œ: 5ì´ˆ ì´ë‚´)
        avg_response = summary['performance']['avg_response_time']
        if avg_response <= 5:
            evaluations['ì‘ë‹µì‹œê°„'] = {'passed': True, 'message': f'{avg_response:.2f}ì´ˆ (ëª©í‘œ: 5ì´ˆ)'}
        else:
            evaluations['ì‘ë‹µì‹œê°„'] = {'passed': False, 'message': f'{avg_response:.2f}ì´ˆ ì´ˆê³¼ (ëª©í‘œ: 5ì´ˆ)'}
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í‰ê°€ (ëª©í‘œ: 2GB ì´ë‚´)
        max_memory = summary['resources']['max_memory_mb']
        if max_memory <= 2048:  # 2GB
            evaluations['ë©”ëª¨ë¦¬ì‚¬ìš©ëŸ‰'] = {'passed': True, 'message': f'{max_memory:.1f}MB (ëª©í‘œ: 2GB)'}
        else:
            evaluations['ë©”ëª¨ë¦¬ì‚¬ìš©ëŸ‰'] = {'passed': False, 'message': f'{max_memory:.1f}MB ì´ˆê³¼ (ëª©í‘œ: 2GB)'}
        
        # ì„±ê³µë¥  í‰ê°€ (ëª©í‘œ: 95% ì´ìƒ)
        success_rate = summary['success_rate']
        if success_rate >= 95:
            evaluations['ì„±ê³µë¥ '] = {'passed': True, 'message': f'{success_rate:.1f}% (ëª©í‘œ: 95%)'}
        else:
            evaluations['ì„±ê³µë¥ '] = {'passed': False, 'message': f'{success_rate:.1f}% ë¯¸ë‹¬ (ëª©í‘œ: 95%)'}
        
        return evaluations
    
    def _generate_recommendations(self, summary):
        """ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì‘ë‹µ ì‹œê°„ ê°œì„ 
        if summary['performance']['avg_response_time'] > 5:
            recommendations.append("ğŸš€ **ì‘ë‹µ ì‹œê°„ ê°œì„ **: ì…€ë ‰í„° ìµœì í™” ë˜ëŠ” ëŒ€ê¸°ì‹œê°„ ë‹¨ì¶• ê²€í† ")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°œì„ 
        if summary['resources']['max_memory_mb'] > 1500:
            recommendations.append("ğŸ’¾ **ë©”ëª¨ë¦¬ ìµœì í™”**: ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì£¼ê¸° ì¡°ì • ë˜ëŠ” ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ")
        
        # CPU ì‚¬ìš©ëŸ‰ ê°œì„ 
        if summary['resources']['avg_cpu_percent'] > 50:
            recommendations.append("âš¡ **CPU ìµœì í™”**: ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™” ë˜ëŠ” ëŒ€ê¸°ì‹œê°„ ì¦ê°€")
        
        # ì„±ê³µë¥  ê°œì„ 
        if summary['success_rate'] < 95:
            recommendations.append("ğŸ¯ **ì•ˆì •ì„± ê°œì„ **: ì¬ì‹œë„ ë¡œì§ ê°•í™” ë˜ëŠ” ì˜¤ë¥˜ ì²˜ë¦¬ ê°œì„ ")
        
        if not recommendations:
            recommendations.append("âœ¨ **í˜„ì¬ ì„±ëŠ¥ ì–‘í˜¸**: ëª¨ë“  ì§€í‘œê°€ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤")
        
        return '\n'.join(f"- {rec}" for rec in recommendations)


# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°
monitor = KKdayPerformanceMonitor()

def performance_track(operation_name):
    """ì„±ëŠ¥ ì¶”ì  ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                duration = time.time() - start_time
                monitor.record_operation(operation_name, duration, success=True)
                return result
            except Exception as e:
                duration = time.time() - start_time
                monitor.record_operation(operation_name, duration, success=False)
                raise e
        return wrapper
    return decorator
```

### ì‹¤ì‹œê°„ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ

#### ì½˜ì†” ê¸°ë°˜ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
```python
import os
import time
from datetime import datetime

class KKdayPerformanceDashboard:
    """ì‹¤ì‹œê°„ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ"""
    
    def __init__(self, monitor):
        self.monitor = monitor
        self.running = False
    
    def start_dashboard(self, refresh_interval=5):
        """ëŒ€ì‹œë³´ë“œ ì‹œì‘"""
        self.running = True
        
        while self.running:
            try:
                self.clear_screen()
                self.display_dashboard()
                time.sleep(refresh_interval)
            except KeyboardInterrupt:
                self.running = False
                print("\nğŸ›‘ ëŒ€ì‹œë³´ë“œ ì¢…ë£Œ")
    
    def clear_screen(self):
        """í™”ë©´ ì§€ìš°ê¸°"""
        os.system('cls' if os.name == 'nt' else 'clear')
    
    def display_dashboard(self):
        """ëŒ€ì‹œë³´ë“œ í™”ë©´ ì¶œë ¥"""
        summary = self.monitor.get_performance_summary()
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        dashboard = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                         KKday í¬ë¡¤ë§ ì„±ëŠ¥ ëª¨ë‹ˆí„° v1.0                         â•‘
â•‘                              {current_time}                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸš€ ì‹œìŠ¤í…œ ìƒíƒœ                                                                 â•‘
â•‘   ê°€ë™ì‹œê°„: {summary['uptime_hours']:.1f}ì‹œê°„                                  â•‘
â•‘   ì´ ì‘ì—…: {summary['total_operations']}ê°œ                                     â•‘
â•‘   ì„±ê³µë¥ : {summary['success_rate']:.1f}%                                       â•‘
â•‘                                                                              â•‘
â•‘ âš¡ ì„±ëŠ¥ ì§€í‘œ                                                                   â•‘
â•‘   í‰ê·  ì‘ë‹µì‹œê°„: {summary['performance']['avg_response_time']:.2f}ì´ˆ             â•‘
â•‘   ìµœëŒ€ ì‘ë‹µì‹œê°„: {summary['performance']['max_response_time']:.2f}ì´ˆ             â•‘
â•‘                                                                              â•‘
â•‘ ğŸ’¾ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰                                                               â•‘
â•‘   í˜„ì¬ ë©”ëª¨ë¦¬: {self.get_current_memory():.1f}MB                              â•‘
â•‘   ìµœëŒ€ ë©”ëª¨ë¦¬: {summary['resources']['max_memory_mb']:.1f}MB                   â•‘
â•‘   í˜„ì¬ CPU: {self.get_current_cpu():.1f}%                                     â•‘
â•‘   ìµœëŒ€ CPU: {summary['resources']['max_cpu_percent']:.1f}%                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š ìµœê·¼ ì‘ì—… ê¸°ë¡:
{self.get_recent_operations()}

âš ï¸ ì•Œë¦¼:
{self.get_alerts(summary)}

ğŸ’¡ Ctrl+Cë¡œ ì¢…ë£Œ
        """
        
        print(dashboard)
    
    def get_current_memory(self):
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        return self.monitor.process.memory_info().rss / 1024 / 1024
    
    def get_current_cpu(self):
        """í˜„ì¬ CPU ì‚¬ìš©ë¥  ì¡°íšŒ"""
        return psutil.cpu_percent()
    
    def get_recent_operations(self):
        """ìµœê·¼ ì‘ì—… ê¸°ë¡ ì¡°íšŒ"""
        recent_ops = list(self.monitor.metrics['response_times'])[-5:]  # ìµœê·¼ 5ê°œ
        if not recent_ops:
            return "  ì•„ì§ ì‘ì—… ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
        
        operations = []
        for op in recent_ops:
            status = "âœ…" if op['success'] else "âŒ"
            timestamp = datetime.fromtimestamp(op['timestamp']).strftime('%H:%M:%S')
            operations.append(f"  {status} {timestamp} | {op['operation']} | {op['duration']:.2f}ì´ˆ")
        
        return '\n'.join(operations)
    
    def get_alerts(self, summary):
        """ê²½ê³  ì•Œë¦¼ ìƒì„±"""
        alerts = []
        
        if summary['resources']['avg_memory_mb'] > 1500:
            alerts.append("ğŸ”¥ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤")
        
        if summary['resources']['avg_cpu_percent'] > 70:
            alerts.append("ğŸ”¥ CPU ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤")
        
        if summary['performance']['avg_response_time'] > 10:
            alerts.append("ğŸŒ ì‘ë‹µ ì‹œê°„ì´ ëŠë¦½ë‹ˆë‹¤")
        
        if summary['success_rate'] < 90:
            alerts.append("âš ï¸ ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤")
        
        if not alerts:
            alerts.append("âœ¨ ëª¨ë“  ì§€í‘œê°€ ì •ìƒ ë²”ìœ„ì…ë‹ˆë‹¤")
        
        return '\n'.join(f"  {alert}" for alert in alerts)


# ì‚¬ìš© ì˜ˆì‹œ
def run_performance_monitoring():
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰"""
    
    # ëª¨ë‹ˆí„° ì‹œì‘
    monitor.start_monitoring(interval=5)
    
    # ëŒ€ì‹œë³´ë“œ ì‹œì‘ (ë³„ë„ ìŠ¤ë ˆë“œ)
    dashboard = KKdayPerformanceDashboard(monitor)
    dashboard_thread = threading.Thread(target=dashboard.start_dashboard)
    dashboard_thread.daemon = True
    dashboard_thread.start()
    
    return monitor
```

## ğŸ”§ ì„±ëŠ¥ ìµœì í™” ì „ëµ

### ë©”ëª¨ë¦¬ ìµœì í™”

#### ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™”
```python
import gc
import weakref
from collections import deque

class MemoryOptimizer:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ë„êµ¬"""
    
    def __init__(self):
        self.object_pool = deque(maxlen=1000)
        self.weak_refs = weakref.WeakSet()
        
    def optimize_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ì‹¤í–‰"""
        
        # ëª…ì‹œì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        collected = gc.collect()
        
        # ìˆœí™˜ ì°¸ì¡° í•´ì œ
        self._break_circular_references()
        
        # ê°ì²´ í’€ ì •ë¦¬
        self.object_pool.clear()
        
        return {
            'collected_objects': collected,
            'active_objects': len(gc.get_objects()),
            'memory_freed_mb': self._calculate_memory_freed()
        }
    
    def _break_circular_references(self):
        """ìˆœí™˜ ì°¸ì¡° í•´ì œ"""
        # ì•½í•œ ì°¸ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆœí™˜ ì°¸ì¡° ë°©ì§€
        for obj in list(self.weak_refs):
            if hasattr(obj, 'clear_references'):
                obj.clear_references()
    
    def _calculate_memory_freed(self):
        """í•´ì œëœ ë©”ëª¨ë¦¬ ê³„ì‚°"""
        # ì‹¤ì œ êµ¬í˜„ ì‹œ psutil ì‚¬ìš©
        current_memory = psutil.Process().memory_info().rss / 1024 / 1024
        return max(0, self.last_memory - current_memory)
    
    @staticmethod
    def memory_efficient_batch_processing(data_list, batch_size=50):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬"""
        for i in range(0, len(data_list), batch_size):
            batch = data_list[i:i + batch_size]
            
            # ë°°ì¹˜ ì²˜ë¦¬
            yield batch
            
            # ë°°ì¹˜ ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
            del batch
            if i % (batch_size * 5) == 0:  # 5ë°°ì¹˜ë§ˆë‹¤ GC
                gc.collect()


# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í¬ë¡¤ë§ ì˜ˆì‹œ
def memory_efficient_crawling(urls, batch_size=20):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ëŒ€ëŸ‰ í¬ë¡¤ë§"""
    
    optimizer = MemoryOptimizer()
    
    for i, batch in enumerate(optimizer.memory_efficient_batch_processing(urls, batch_size)):
        print(f"ğŸ”„ ë°°ì¹˜ {i+1} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ URL)")
        
        # ë°°ì¹˜ë³„ í¬ë¡¤ë§ ì²˜ë¦¬
        for url in batch:
            process_single_url(url)
        
        # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ìµœì í™”
        if i % 5 == 0:  # 5ë°°ì¹˜ë§ˆë‹¤
            result = optimizer.optimize_memory_usage()
            print(f"ğŸ§¹ ë©”ëª¨ë¦¬ ìµœì í™”: {result['collected_objects']}ê°œ ê°ì²´ í•´ì œ")
```

### CPU ìµœì í™”

#### ë¹„ë™ê¸° ì²˜ë¦¬ ë° ë©€í‹°í”„ë¡œì„¸ì‹±
```python
import asyncio
import aiohttp
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

class CPUOptimizer:
    """CPU ì‚¬ìš©ëŸ‰ ìµœì í™” ë„êµ¬"""
    
    def __init__(self, max_workers=None):
        self.max_workers = max_workers or multiprocessing.cpu_count()
        self.session = None
    
    async def async_crawl_urls(self, urls, max_concurrent=10):
        """ë¹„ë™ê¸° URL í¬ë¡¤ë§"""
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async with aiohttp.ClientSession() as session:
            self.session = session
            
            tasks = [
                self.crawl_single_url_async(semaphore, url) 
                for url in urls
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return results
    
    async def crawl_single_url_async(self, semaphore, url):
        """ë‹¨ì¼ URL ë¹„ë™ê¸° í¬ë¡¤ë§"""
        
        async with semaphore:
            try:
                async with self.session.get(url) as response:
                    content = await response.text()
                    # ë°ì´í„° íŒŒì‹± ë¡œì§
                    return self.parse_content(content)
                    
            except Exception as e:
                return {'error': str(e), 'url': url}
    
    def multiprocess_crawling(self, urls, chunk_size=None):
        """ë©€í‹°í”„ë¡œì„¸ì‹± í¬ë¡¤ë§"""
        
        chunk_size = chunk_size or max(1, len(urls) // self.max_workers)
        url_chunks = [urls[i:i + chunk_size] for i in range(0, len(urls), chunk_size)]
        
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                executor.submit(self.process_url_chunk, chunk) 
                for chunk in url_chunks
            ]
            
            results = []
            for future in futures:
                try:
                    chunk_results = future.result()
                    results.extend(chunk_results)
                except Exception as e:
                    print(f"âŒ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            return results
    
    @staticmethod
    def process_url_chunk(url_chunk):
        """URL ì²­í¬ ì²˜ë¦¬ (ë³„ë„ í”„ë¡œì„¸ìŠ¤)"""
        results = []
        
        for url in url_chunk:
            try:
                # ì‹¤ì œ í¬ë¡¤ë§ ë¡œì§
                result = crawl_single_url_sync(url)
                results.append(result)
                
                # CPU ë¶€í•˜ ì¡°ì ˆ
                time.sleep(0.1)
                
            except Exception as e:
                results.append({'error': str(e), 'url': url})
        
        return results


# ì„±ëŠ¥ ìµœì í™”ëœ í¬ë¡¤ë§ ì‹¤í–‰
def optimized_crawling_execution(urls):
    """ìµœì í™”ëœ í¬ë¡¤ë§ ì‹¤í–‰"""
    
    optimizer = CPUOptimizer()
    
    # URL ê°œìˆ˜ì— ë”°ë¥¸ ìµœì  ì „ëµ ì„ íƒ
    if len(urls) < 50:
        # ì†ŒëŸ‰: ì¼ë°˜ ë™ê¸° ì²˜ë¦¬
        results = [crawl_single_url_sync(url) for url in urls]
        
    elif len(urls) < 200:
        # ì¤‘ëŸ‰: ë¹„ë™ê¸° ì²˜ë¦¬
        results = asyncio.run(optimizer.async_crawl_urls(urls))
        
    else:
        # ëŒ€ëŸ‰: ë©€í‹°í”„ë¡œì„¸ì‹±
        results = optimizer.multiprocess_crawling(urls)
    
    return results
```

### ë„¤íŠ¸ì›Œí¬ ìµœì í™”

#### ì—°ê²° í’€ë§ ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
```python
import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
import time

class NetworkOptimizer:
    """ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ ìµœì í™” ë„êµ¬"""
    
    def __init__(self):
        self.session = self._create_optimized_session()
    
    def _create_optimized_session(self):
        """ìµœì í™”ëœ HTTP ì„¸ì…˜ ìƒì„±"""
        
        session = requests.Session()
        
        # ì¬ì‹œë„ ì •ì±… ì„¤ì •
        retry_strategy = Retry(
            total=3,
            status_forcelist=[429, 500, 502, 503, 504],
            method_whitelist=["HEAD", "GET", "OPTIONS"],
            backoff_factor=1  # 1ì´ˆ, 2ì´ˆ, 4ì´ˆ ê°„ê²©ìœ¼ë¡œ ì¬ì‹œë„
        )
        
        # HTTP ì–´ëŒ‘í„° ì„¤ì •
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,  # ì—°ê²° í’€ í¬ê¸°
            pool_maxsize=20       # ìµœëŒ€ ì—°ê²° ìˆ˜
        )
        
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # ê¸°ë³¸ í—¤ë” ì„¤ì •
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        return session
    
    def smart_request(self, url, **kwargs):
        """ì§€ëŠ¥ì  HTTP ìš”ì²­"""
        
        start_time = time.time()
        
        try:
            response = self.session.get(url, timeout=(10, 30), **kwargs)
            response.raise_for_status()
            
            duration = time.time() - start_time
            
            # ì„±ëŠ¥ ê¸°ë¡
            monitor.record_operation('http_request', duration, success=True)
            
            return response
            
        except requests.exceptions.RequestException as e:
            duration = time.time() - start_time
            monitor.record_operation('http_request', duration, success=False)
            
            # ì˜¤ë¥˜ ìœ í˜•ë³„ ì²˜ë¦¬
            if isinstance(e, requests.exceptions.Timeout):
                print(f"â° íƒ€ì„ì•„ì›ƒ: {url}")
            elif isinstance(e, requests.exceptions.ConnectionError):
                print(f"ğŸ”— ì—°ê²° ì˜¤ë¥˜: {url}")
            else:
                print(f"âŒ HTTP ì˜¤ë¥˜: {e}")
            
            raise e
    
    def adaptive_delay(self, response_time, error_count=0):
        """ì‘ë‹µì‹œê°„ ê¸°ë°˜ ì ì‘ì  ì§€ì—°"""
        
        base_delay = 2.0  # ê¸°ë³¸ 2ì´ˆ
        
        # ì‘ë‹µì‹œê°„ì— ë”°ë¥¸ ì¡°ì •
        if response_time > 10:
            delay_factor = 2.0
        elif response_time > 5:
            delay_factor = 1.5
        else:
            delay_factor = 1.0
        
        # ì˜¤ë¥˜ íšŸìˆ˜ì— ë”°ë¥¸ ì¡°ì •
        error_penalty = error_count * 0.5
        
        total_delay = base_delay * delay_factor + error_penalty
        
        # ìµœëŒ€ 10ì´ˆ ì œí•œ
        final_delay = min(total_delay, 10.0)
        
        time.sleep(final_delay)
        return final_delay


# ìµœì í™”ëœ ë„¤íŠ¸ì›Œí¬ í¬ë¡¤ëŸ¬
class OptimizedKKdayCrawler:
    """ë„¤íŠ¸ì›Œí¬ ìµœì í™”ëœ KKday í¬ë¡¤ëŸ¬"""
    
    def __init__(self, city_name):
        self.city_name = city_name
        self.network_optimizer = NetworkOptimizer()
        self.error_count = 0
        self.last_request_time = 0
    
    def crawl_with_optimization(self, url):
        """ìµœì í™”ëœ í¬ë¡¤ë§ ìˆ˜í–‰"""
        
        # ì ì‘ì  ì§€ì—° ì ìš©
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < 2:  # ìµœì†Œ 2ì´ˆ ê°„ê²©
            time.sleep(2 - time_since_last)
        
        try:
            response = self.network_optimizer.smart_request(url)
            
            # ì„±ê³µ ì‹œ ì˜¤ë¥˜ ì¹´ìš´íŠ¸ ë¦¬ì…‹
            self.error_count = 0
            self.last_request_time = time.time()
            
            return self.parse_response(response)
            
        except Exception as e:
            self.error_count += 1
            
            # ì ì‘ì  ì§€ì—° ì ìš©
            delay = self.network_optimizer.adaptive_delay(
                response_time=5.0,  # ì˜¤ë¥˜ ì‹œ ê°€ì •ê°’
                error_count=self.error_count
            )
            
            print(f"ğŸ”„ ì˜¤ë¥˜ í›„ {delay:.1f}ì´ˆ ëŒ€ê¸°")
            return None
```

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

### ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸

#### ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
```python
import statistics
from datetime import datetime

class KKdayPerformanceBenchmark:
    """KKday í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(self):
        self.results = {}
    
    def run_comprehensive_benchmark(self):
        """ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        print("ğŸš€ KKday í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        
        # 1. ë‹¨ì¼ URL ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        self.results['single_url'] = self.benchmark_single_url()
        
        # 2. ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸  
        self.results['batch_processing'] = self.benchmark_batch_processing()
        
        # 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
        self.results['memory_usage'] = self.benchmark_memory_usage()
        
        # 4. ë™ì‹œ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        self.results['concurrent_processing'] = self.benchmark_concurrent_processing()
        
        # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        return self.generate_benchmark_report()
    
    def benchmark_single_url(self, iterations=10):
        """ë‹¨ì¼ URL ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        
        test_url = "https://www.kkday.com/ko/product/test"
        times = []
        
        print(f"ğŸ“Š ë‹¨ì¼ URL ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ({iterations}íšŒ)")
        
        for i in range(iterations):
            start_time = time.time()
            
            try:
                # ì‹¤ì œ í¬ë¡¤ë§ ì‹œë®¬ë ˆì´ì…˜
                result = simulate_single_crawl(test_url)
                duration = time.time() - start_time
                times.append(duration)
                
                print(f"  í…ŒìŠ¤íŠ¸ {i+1}: {duration:.2f}ì´ˆ")
                
            except Exception as e:
                print(f"  í…ŒìŠ¤íŠ¸ {i+1}: ì‹¤íŒ¨ - {e}")
        
        if times:
            return {
                'avg_time': statistics.mean(times),
                'min_time': min(times),
                'max_time': max(times),
                'std_dev': statistics.stdev(times) if len(times) > 1 else 0,
                'success_rate': len(times) / iterations * 100
            }
        else:
            return {'error': 'All tests failed'}
    
    def benchmark_batch_processing(self, batch_sizes=[10, 20, 50]):
        """ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        
        results = {}
        
        for batch_size in batch_sizes:
            print(f"ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ (í¬ê¸°: {batch_size})")
            
            # í…ŒìŠ¤íŠ¸ URL ìƒì„±
            test_urls = [f"https://www.kkday.com/ko/product/test{i}" 
                        for i in range(batch_size)]
            
            start_time = time.time()
            
            try:
                # ë°°ì¹˜ í¬ë¡¤ë§ ì‹œë®¬ë ˆì´ì…˜
                batch_results = simulate_batch_crawl(test_urls)
                
                total_time = time.time() - start_time
                throughput = batch_size / total_time  # ê°œ/ì´ˆ
                
                results[f'batch_{batch_size}'] = {
                    'total_time': total_time,
                    'throughput': throughput,
                    'avg_time_per_item': total_time / batch_size,
                    'success_count': len([r for r in batch_results if r])
                }
                
                print(f"  ì™„ë£Œ: {total_time:.2f}ì´ˆ ({throughput:.2f}ê°œ/ì´ˆ)")
                
            except Exception as e:
                results[f'batch_{batch_size}'] = {'error': str(e)}
                print(f"  ì‹¤íŒ¨: {e}")
        
        return results
    
    def benchmark_memory_usage(self, duration=60):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë²¤ì¹˜ë§ˆí¬"""
        
        print(f"ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ({duration}ì´ˆ)")
        
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024
        
        memory_samples = []
        start_time = time.time()
        
        # ì§€ì†ì ì¸ í¬ë¡¤ë§ ì‹œë®¬ë ˆì´ì…˜
        while time.time() - start_time < duration:
            # í¬ë¡¤ë§ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
            simulate_crawling_work()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡
            current_memory = process.memory_info().rss / 1024 / 1024
            memory_samples.append(current_memory)
            
            time.sleep(1)  # 1ì´ˆë§ˆë‹¤ ì¸¡ì •
        
        final_memory = process.memory_info().rss / 1024 / 1024
        
        return {
            'initial_memory_mb': initial_memory,
            'final_memory_mb': final_memory,
            'max_memory_mb': max(memory_samples),
            'avg_memory_mb': statistics.mean(memory_samples),
            'memory_growth_mb': final_memory - initial_memory,
            'memory_samples': len(memory_samples)
        }
    
    def benchmark_concurrent_processing(self, concurrent_levels=[1, 5, 10]):
        """ë™ì‹œ ì²˜ë¦¬ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        
        results = {}
        
        for level in concurrent_levels:
            print(f"ğŸ“Š ë™ì‹œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ (ë™ì‹œì„±: {level})")
            
            start_time = time.time()
            
            try:
                # ë™ì‹œ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
                concurrent_results = simulate_concurrent_crawling(level)
                
                total_time = time.time() - start_time
                
                results[f'concurrent_{level}'] = {
                    'total_time': total_time,
                    'requests_per_second': level / total_time * len(concurrent_results),
                    'success_rate': sum(1 for r in concurrent_results if r) / len(concurrent_results) * 100
                }
                
                print(f"  ì™„ë£Œ: {total_time:.2f}ì´ˆ")
                
            except Exception as e:
                results[f'concurrent_{level}'] = {'error': str(e)}
                print(f"  ì‹¤íŒ¨: {e}")
        
        return results
    
    def generate_benchmark_report(self):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report = f"""
# KKday í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸

**ì‹¤í–‰ ì‹œê°„**: {timestamp}
**ì‹œìŠ¤í…œ ì‚¬ì–‘**: {psutil.cpu_count()}ì½”ì–´, {psutil.virtual_memory().total / 1024**3:.1f}GB RAM

## ğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼

### 1. ë‹¨ì¼ URL ì²˜ë¦¬ ì„±ëŠ¥
"""
        
        if 'single_url' in self.results and 'avg_time' in self.results['single_url']:
            single = self.results['single_url']
            report += f"""
- **í‰ê·  ì²˜ë¦¬ì‹œê°„**: {single['avg_time']:.2f}ì´ˆ
- **ìµœì†Œ ì²˜ë¦¬ì‹œê°„**: {single['min_time']:.2f}ì´ˆ  
- **ìµœëŒ€ ì²˜ë¦¬ì‹œê°„**: {single['max_time']:.2f}ì´ˆ
- **í‘œì¤€í¸ì°¨**: {single['std_dev']:.2f}ì´ˆ
- **ì„±ê³µë¥ **: {single['success_rate']:.1f}%
"""
        
        report += "\n### 2. ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥\n"
        
        if 'batch_processing' in self.results:
            for batch_name, batch_data in self.results['batch_processing'].items():
                if 'throughput' in batch_data:
                    report += f"""
- **{batch_name}**: {batch_data['throughput']:.2f}ê°œ/ì´ˆ (ì´ {batch_data['total_time']:.2f}ì´ˆ)
"""
        
        report += "\n### 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\n"
        
        if 'memory_usage' in self.results:
            memory = self.results['memory_usage']
            if 'avg_memory_mb' in memory:
                report += f"""
- **ì´ˆê¸° ë©”ëª¨ë¦¬**: {memory['initial_memory_mb']:.1f}MB
- **ìµœì¢… ë©”ëª¨ë¦¬**: {memory['final_memory_mb']:.1f}MB
- **ìµœëŒ€ ë©”ëª¨ë¦¬**: {memory['max_memory_mb']:.1f}MB
- **í‰ê·  ë©”ëª¨ë¦¬**: {memory['avg_memory_mb']:.1f}MB
- **ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰**: {memory['memory_growth_mb']:.1f}MB
"""
        
        # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
        score = self.calculate_performance_score()
        report += f"\n## ğŸ† ì¢…í•© ì„±ëŠ¥ ì ìˆ˜: {score}/100\n"
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
        filename = f"benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“‹ ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ìƒì„±: {filename}")
        return report
    
    def calculate_performance_score(self):
        """ì¢…í•© ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°"""
        score = 100
        
        # ì‘ë‹µì‹œê°„ ì ìˆ˜ (40ì )
        if 'single_url' in self.results and 'avg_time' in self.results['single_url']:
            avg_time = self.results['single_url']['avg_time']
            if avg_time <= 3:
                time_score = 40
            elif avg_time <= 5:
                time_score = 30
            elif avg_time <= 10:
                time_score = 20
            else:
                time_score = 10
        else:
            time_score = 0
        
        # ë©”ëª¨ë¦¬ ì ìˆ˜ (30ì )
        if 'memory_usage' in self.results and 'max_memory_mb' in self.results['memory_usage']:
            max_memory = self.results['memory_usage']['max_memory_mb']
            if max_memory <= 1000:  # 1GB ì´í•˜
                memory_score = 30
            elif max_memory <= 2000:  # 2GB ì´í•˜
                memory_score = 20
            else:
                memory_score = 10
        else:
            memory_score = 0
        
        # ì²˜ë¦¬ëŸ‰ ì ìˆ˜ (30ì )
        throughput_score = 30  # ê¸°ë³¸ê°’
        if 'batch_processing' in self.results:
            # ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ì—ì„œ ìµœê³  ì²˜ë¦¬ëŸ‰ í™•ì¸
            max_throughput = 0
            for batch_data in self.results['batch_processing'].values():
                if isinstance(batch_data, dict) and 'throughput' in batch_data:
                    max_throughput = max(max_throughput, batch_data['throughput'])
            
            if max_throughput >= 1.0:  # 1ê°œ/ì´ˆ ì´ìƒ
                throughput_score = 30
            elif max_throughput >= 0.5:  # 0.5ê°œ/ì´ˆ ì´ìƒ
                throughput_score = 20
            else:
                throughput_score = 10
        
        final_score = min(100, time_score + memory_score + throughput_score)
        return final_score


# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ í•¨ìˆ˜ë“¤ (ì‹œë®¬ë ˆì´ì…˜)
def simulate_single_crawl(url):
    """ë‹¨ì¼ í¬ë¡¤ë§ ì‹œë®¬ë ˆì´ì…˜"""
    time.sleep(random.uniform(2, 5))  # 2-5ì´ˆ ì‹œë®¬ë ˆì´ì…˜
    return {"url": url, "data": "mock_data"}

def simulate_batch_crawl(urls):
    """ë°°ì¹˜ í¬ë¡¤ë§ ì‹œë®¬ë ˆì´ì…˜"""  
    results = []
    for url in urls:
        time.sleep(random.uniform(1, 3))  # ê°œë³„ ì²˜ë¦¬ ì‹œê°„
        results.append(simulate_single_crawl(url))
    return results

def simulate_crawling_work():
    """í¬ë¡¤ë§ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜"""
    # ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œë®¬ë ˆì´ì…˜
    data = [i for i in range(1000)]  # ì„ì‹œ ë°ì´í„° ìƒì„±
    time.sleep(0.5)
    del data

def simulate_concurrent_crawling(concurrent_level):
    """ë™ì‹œ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜"""
    results = []
    for i in range(concurrent_level * 2):  # ê° ë ˆë²¨ë‹¹ 2ê°œì”© ì²˜ë¦¬
        time.sleep(random.uniform(0.5, 2))
        results.append(True if random.random() > 0.1 else False)  # 90% ì„±ê³µë¥ 
    return results
```

## ğŸ›ï¸ ì„±ëŠ¥ íŠœë‹ ê°€ì´ë“œ

### ìë™ ì„±ëŠ¥ íŠœë‹

#### ì ì‘í˜• ì„±ëŠ¥ ì¡°ì •ê¸°
```python
class AdaptivePerformanceTuner:
    """ì ì‘í˜• ì„±ëŠ¥ ì¡°ì • ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.settings = {
            'batch_size': 20,
            'delay_time': 3.0,
            'max_workers': 4,
            'memory_threshold': 1500,  # MB
            'cpu_threshold': 70        # %
        }
        
        self.performance_history = deque(maxlen=50)
        self.adjustment_history = []
    
    def auto_tune_performance(self, current_metrics):
        """í˜„ì¬ ë©”íŠ¸ë¦­ì„ ê¸°ë°˜ìœ¼ë¡œ ì„±ëŠ¥ ìë™ ì¡°ì •"""
        
        adjustments = []
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ì¡°ì •
        if current_metrics['memory_mb'] > self.settings['memory_threshold']:
            # ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ
            old_batch = self.settings['batch_size']
            self.settings['batch_size'] = max(5, old_batch - 5)
            adjustments.append(f"ë°°ì¹˜í¬ê¸° {old_batch} â†’ {self.settings['batch_size']}")
        
        # CPU ì‚¬ìš©ë¥  ê¸°ë°˜ ì¡°ì •
        if current_metrics['cpu_percent'] > self.settings['cpu_threshold']:
            # ëŒ€ê¸°ì‹œê°„ ì¦ê°€
            old_delay = self.settings['delay_time']
            self.settings['delay_time'] = min(10.0, old_delay + 0.5)
            adjustments.append(f"ëŒ€ê¸°ì‹œê°„ {old_delay} â†’ {self.settings['delay_time']}ì´ˆ")
        
        # ì‘ë‹µì‹œê°„ ê¸°ë°˜ ì¡°ì •
        if current_metrics['response_time'] > 10:
            # ì›Œì»¤ ìˆ˜ ê°ì†Œ
            old_workers = self.settings['max_workers']
            self.settings['max_workers'] = max(1, old_workers - 1)
            adjustments.append(f"ì›Œì»¤ìˆ˜ {old_workers} â†’ {self.settings['max_workers']}")
        
        # ì„±ëŠ¥ì´ ì¢‹ì„ ë•Œ ì ì§„ì  ìµœì í™”
        if (current_metrics['memory_mb'] < self.settings['memory_threshold'] * 0.7 and 
            current_metrics['cpu_percent'] < self.settings['cpu_threshold'] * 0.7 and
            current_metrics['response_time'] < 5):
            
            # ë°°ì¹˜ í¬ê¸° ì¦ê°€ (ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ)
            if self.settings['batch_size'] < 50:
                old_batch = self.settings['batch_size']
                self.settings['batch_size'] += 2
                adjustments.append(f"ë°°ì¹˜í¬ê¸° ì¦ê°€ {old_batch} â†’ {self.settings['batch_size']}")
        
        # ì¡°ì • ë‚´ì—­ ê¸°ë¡
        if adjustments:
            self.adjustment_history.append({
                'timestamp': time.time(),
                'adjustments': adjustments,
                'metrics': current_metrics.copy()
            })
            
            print(f"ğŸ”§ ì„±ëŠ¥ ìë™ ì¡°ì •: {', '.join(adjustments)}")
        
        return self.settings
    
    def get_optimization_recommendations(self):
        """ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        if len(self.performance_history) < 10:
            return ["ë°ì´í„° ìˆ˜ì§‘ ì¤‘... ë” ë§ì€ ì„±ëŠ¥ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."]
        
        recommendations = []
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš© íŒ¨í„´ ë¶„ì„
        memory_values = [p['memory_mb'] for p in self.performance_history]
        avg_memory = statistics.mean(memory_values)
        max_memory = max(memory_values)
        
        if max_memory > 2000:
            recommendations.append("ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”: ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ ë˜ëŠ” ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•í™”")
        
        # ì‘ë‹µì‹œê°„ íŒ¨í„´ ë¶„ì„
        response_times = [p['response_time'] for p in self.performance_history]
        avg_response = statistics.mean(response_times)
        
        if avg_response > 8:
            recommendations.append("ğŸŒ ì‘ë‹µì‹œê°„ ê°œì„ : ì…€ë ‰í„° ìµœì í™” ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ì„¤ì • ì ê²€")
        
        # CPU ì‚¬ìš© íŒ¨í„´ ë¶„ì„
        cpu_values = [p['cpu_percent'] for p in self.performance_history]
        avg_cpu = statistics.mean(cpu_values)
        
        if avg_cpu > 80:
            recommendations.append("âš¡ CPU ë¶€í•˜ ê°ì†Œ: ëŒ€ê¸°ì‹œê°„ ì¦ê°€ ë˜ëŠ” ë™ì‹œì„± ì¶•ì†Œ")
        
        # ì„±ê³µë¥  ë¶„ì„
        success_rates = [p.get('success_rate', 100) for p in self.performance_history]
        avg_success = statistics.mean(success_rates)
        
        if avg_success < 90:
            recommendations.append("ğŸ¯ ì•ˆì •ì„± ê°œì„ : ì¬ì‹œë„ ë¡œì§ ê°•í™” ë˜ëŠ” ì˜¤ë¥˜ ì²˜ë¦¬ ê°œì„ ")
        
        if not recommendations:
            recommendations.append("âœ¨ í˜„ì¬ ì„±ëŠ¥ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤. í˜„ ìƒíƒœë¥¼ ìœ ì§€í•˜ì„¸ìš”.")
        
        return recommendations


# ìë™ íŠœë‹ í†µí•© í¬ë¡¤ëŸ¬
class AutoTuningKKdayCrawler:
    """ìë™ ì„±ëŠ¥ íŠœë‹ì´ í¬í•¨ëœ KKday í¬ë¡¤ëŸ¬"""
    
    def __init__(self, city_name):
        self.city_name = city_name
        self.tuner = AdaptivePerformanceTuner()
        self.monitor = KKdayPerformanceMonitor()
        
    def crawl_with_auto_tuning(self, urls):
        """ìë™ íŠœë‹ì´ ì ìš©ëœ í¬ë¡¤ë§"""
        
        self.monitor.start_monitoring()
        
        try:
            results = []
            
            for i, batch_start in enumerate(range(0, len(urls), self.tuner.settings['batch_size'])):
                batch_end = min(batch_start + self.tuner.settings['batch_size'], len(urls))
                batch_urls = urls[batch_start:batch_end]
                
                print(f"ğŸ”„ ë°°ì¹˜ {i+1} ì²˜ë¦¬ ì¤‘ ({len(batch_urls)}ê°œ)")
                
                # ë°°ì¹˜ í¬ë¡¤ë§
                batch_results = self.process_batch(batch_urls)
                results.extend(batch_results)
                
                # í˜„ì¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                current_metrics = {
                    'memory_mb': psutil.Process().memory_info().rss / 1024 / 1024,
                    'cpu_percent': psutil.cpu_percent(),
                    'response_time': self.calculate_avg_response_time(),
                    'success_rate': self.calculate_success_rate(batch_results)
                }
                
                # ìë™ íŠœë‹ ìˆ˜í–‰
                if i % 5 == 0:  # 5ë°°ì¹˜ë§ˆë‹¤ íŠœë‹
                    self.tuner.auto_tune_performance(current_metrics)
                
                # ì¡°ì •ëœ ëŒ€ê¸°ì‹œê°„ ì ìš©
                time.sleep(self.tuner.settings['delay_time'])
            
            return results
            
        finally:
            self.monitor.stop_monitoring()
            
            # ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸
            final_report = self.monitor.generate_performance_report()
            print("ğŸ“‹ ìë™ íŠœë‹ í¬ë¡¤ë§ ì™„ë£Œ")
```

---

**ë¬¸ì„œ ë²„ì „**: v1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2024-12-07  
**ë‹´ë‹¹ì**: DevOpsíŒ€  
**ë‹¤ìŒ ë¦¬ë·°**: 2024-12-21