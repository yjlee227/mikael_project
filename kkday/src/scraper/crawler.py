"""
ë©”ì¸ í¬ë¡¤ë§ ì—”ì§„
- ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ í†µí•© ê´€ë¦¬
- í˜ì´ì§€ë„¤ì´ì…˜ ë° ë°ì´í„° ìˆ˜ì§‘
- ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ ì‹œìŠ¤í…œ
"""

import time
import random
import re
from datetime import datetime

from ..config import CONFIG, SELENIUM_AVAILABLE
from ..utils.file_handler import create_product_data_structure, save_to_csv_kkday, get_dual_image_urls_kkday, download_and_save_image_kkday, ensure_directory_structure
from .driver_manager import setup_driver, go_to_main_page, find_and_fill_search, click_search_button, handle_kkday_cookie_popup, handle_popup, smart_scroll_selector
from .url_manager import collect_urls_from_page, get_pagination_urls, is_url_already_processed, mark_url_as_processed, go_to_next_page
from .parsers import extract_all_product_data, validate_product_data
from .ranking import save_url_with_rank, get_next_start_rank
from . import human_scroll_patterns

if SELENIUM_AVAILABLE:
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException

# =============================================================================
# ë©”ì¸ í¬ë¡¤ë§ í´ë˜ìŠ¤
# =============================================================================

class KKdayCrawler:
    """KKday í¬ë¡¤ë§ í†µí•© ì‹œìŠ¤í…œ"""

    def __init__(self, city_name="ì„œìš¸"):
        self.city_name = city_name
        self.driver = None
        self.stats = {
            "start_time": None,
            "end_time": None,
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "skip_count": 0,
            "urls_collected": 0,
            "current_rank": 0
        }

    def initialize(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        print(f"ğŸš€ KKday í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”: {self.city_name}")
        try:
            # ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ë³´
            ensure_directory_structure(self.city_name)
            
            # ë“œë¼ì´ë²„ ì„¤ì •
            self.driver = setup_driver()
            if not self.driver:
                raise Exception("ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨")
            time.sleep(random.uniform(1, 3))
            
            # [ìˆ˜ì •] ë„ì‹œë³„ ìƒí’ˆ ëª©ë¡ URLë¡œ ì§ì ‘ ì´ë™
            from urllib.parse import quote
            encoded_city_name = quote(self.city_name)
            target_url = f"https://www.kkday.com/ko/product/productlist/{encoded_city_name}"
            print(f"ìƒí’ˆ ëª©ë¡ í˜ì´ì§€ë¡œ ì§ì ‘ ì´ë™: {target_url}")
            
            self.driver.get(target_url)
            time.sleep(random.uniform(3, 5))  # í˜ì´ì§€ ë¡œë“œë¥¼ ìœ„í•œ ìµœì†Œ ëŒ€ê¸°
            
            # íŒì—… ì²˜ë¦¬ ë° ìŠ¤í¬ë¡¤
            handle_popup(self.driver)
            smart_scroll_selector(self.driver)
            time.sleep(random.uniform(2, 5))
            
            self.stats["start_time"] = datetime.now()
            print("âœ… í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if self.driver:
                pass
            return False

    def collect_urls(self, max_pages=3, max_products=None):
        """URL ìˆ˜ì§‘ (KLOOK ë°©ì‹ ì—…ê·¸ë ˆì´ë“œ: ë©”íƒ€ë°ì´í„° í¬í•¨)"""
        print(f"ğŸ”— URL ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€, ëª©í‘œ ìƒí’ˆ: {max_products or 'ì œí•œ ì—†ìŒ'})")
        time.sleep(random.uniform(2, 4))

        all_product_urls = []  # KLOOK ë°©ì‹: ë©”íƒ€ë°ì´í„° í¬í•¨ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        seen_urls = set()
        current_page = 1
        current_rank = 1  # ìˆœìœ„ ì¶”ì 

        try:
            while current_page <= max_pages:
                print(f"  ğŸ“„ {current_page}í˜ì´ì§€ íƒìƒ‰ ì¤‘... (í˜„ì¬ ìˆ˜ì§‘: {len(all_product_urls)}ê°œ)")

                # í˜„ì¬ í˜ì´ì§€ì—ì„œ URL ìˆ˜ì§‘
                page_urls = collect_urls_from_page(self.driver, self.city_name)
                if not page_urls:
                    print("  âš ï¸ í˜„ì¬ í˜ì´ì§€ì—ì„œ URLì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break

                # ìƒí’ˆ URLë§Œ í•„í„°ë§
                product_urls_on_page = self.filter_product_detail_urls(page_urls)

                # KLOOK ë°©ì‹: ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ URL ì¶”ê°€
                page_index = 1
                for url in product_urls_on_page:
                    if url not in seen_urls:
                        # KLOOK ìŠ¤íƒ€ì¼ URL ì—”íŠ¸ë¦¬ ìƒì„±
                        url_entry = {
                            "rank": current_rank,
                            "url": url,
                            "page": current_page,
                            "page_index": page_index,
                            "collected_at": datetime.now().isoformat(),
                            "is_duplicate": False
                        }

                        all_product_urls.append(url_entry)
                        seen_urls.add(url)
                        current_rank += 1
                        page_index += 1

                # ëª©í‘œ ìƒí’ˆ ìˆ˜ì— ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸
                if max_products and len(all_product_urls) >= max_products:
                    print(f"  ğŸ¯ ëª©í‘œ ìƒí’ˆ ìˆ˜({max_products}ê°œ)ì— ë„ë‹¬í•˜ì—¬ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break

                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                if current_page < max_pages:
                    if not go_to_next_page(self.driver):
                        print("  â„¹ï¸ ë” ì´ìƒ ë‹¤ìŒ í˜ì´ì§€ê°€ ì—†ì–´ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        break

                current_page += 1
                # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
                time.sleep(random.uniform(2, 5))

            # KLOOK ë°©ì‹: ë°ì´í„° ì˜ì†ì„± ì‹œìŠ¤í…œìœ¼ë¡œ ì €ì¥
            from ..utils.data_persistence import KKdayDataPersistence

            # ìˆ˜ì§‘ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            collection_info = {
                "target_products": max_products or len(all_product_urls),
                "max_pages": max_pages,
                "pages_processed": current_page - 1
            }

            # JSON í˜•íƒœë¡œ ì €ì¥
            persistence = KKdayDataPersistence()
            persistence.save_url_collection_data(
                city_name=self.city_name,
                tab="ì „ì²´",
                url_data=all_product_urls,
                collection_info=collection_info
            )

            # Stage 1 ìƒíƒœ ì €ì¥
            stage1_data = {
                "status": "success",
                "url_count": len(all_product_urls),
                "new_count": len(all_product_urls)
            }
            persistence.save_status_data(self.city_name, "ì „ì²´", stage1_data=stage1_data)

            # ë¯¸ì²˜ë¦¬ URL í•„í„°ë§ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
            unprocessed_urls = []
            for url_entry in all_product_urls:
                url = url_entry["url"]
                if not is_url_already_processed(url, self.city_name):
                    unprocessed_urls.append(url)  # ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•´ URLë§Œ ë°˜í™˜

            self.stats["urls_collected"] = len(all_product_urls)
            print(f"âœ… URL ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_product_urls)}ê°œ ìƒí’ˆ URL, ë¯¸ì²˜ë¦¬ {len(unprocessed_urls)}ê°œ")
            
            # max_productsì— ë§ì¶° ìµœì¢… ê²°ê³¼ ìŠ¬ë¼ì´ì‹± (ì•ˆì „ì¥ì¹˜)
            if max_products:
                return unprocessed_urls[:max_products]
            return unprocessed_urls

        except Exception as e:
            print(f"âŒ URL ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return []

    def filter_product_detail_urls(self, urls):
        """ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ URLë§Œ í•„í„°ë§ (ëª©ë¡ í˜ì´ì§€ ì œì™¸)"""
        # ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ íŒ¨í„´ (ë” í¬ê´„ì ìœ¼ë¡œ ìˆ˜ì •)
        product_detail_patterns = [
            r'/ko/product/\d+$',                           # /ko/product/287674
            r'/ko/product/\d+-[\w\-]+',                    # /ko/product/146520-krabi-kayaking
            r'/product/\d+',                               # /product/287674 (ì–¸ì–´ ìƒëµ)
            r'/product/\d+-[\w\-]+',                       # /product/146520-krabi-kayaking
        ]

        filtered_urls = []
        excluded_count = 0

        for url in urls:
            # ëª©ë¡ í˜ì´ì§€ íŒ¨í„´ ì œì™¸
            if 'productlist' in url:
                excluded_count += 1
                print(f"  ğŸš« ëª©ë¡ í˜ì´ì§€ ì œì™¸: {url}")
                continue

            # ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ íŒ¨í„´ í™•ì¸
            is_product_detail = any(re.search(pattern, url) for pattern in product_detail_patterns)

            if is_product_detail:
                filtered_urls.append(url)
            else:
                excluded_count += 1
                print(f"  ğŸš« ë¹„ìƒí’ˆ í˜ì´ì§€ ì œì™¸: {url}")

        print(f"âœ… URL í•„í„°ë§ ì™„ë£Œ: {len(filtered_urls)}ê°œ ìœ ì§€, {excluded_count}ê°œ ì œì™¸")
        return filtered_urls

    def get_next_available_rank(self):
        """ë„ì‹œë³„ ë‹¤ìŒ ì‚¬ìš© ê°€ëŠ¥í•œ ìˆœìœ„ ì¡°íšŒ"""
        try:
            next_rank = get_next_start_rank(self.city_name)
            print(f"ğŸ“Š {self.city_name} ë‹¤ìŒ ìˆœìœ„: {next_rank}")
            return next_rank
        except Exception as e:
            print(f"âš ï¸ ìˆœìœ„ ì¡°íšŒ ì‹¤íŒ¨, 1ë¶€í„° ì‹œì‘: {e}")
            return 1

    def crawl_product(self, url, rank=None):
        """ê°œë³„ ìƒí’ˆ í¬ë¡¤ë§"""
        print(f"ğŸ” ìƒí’ˆ í¬ë¡¤ë§ ì‹œì‘: ìˆœìœ„ {rank}")
        try:
            # ìƒí’ˆ í˜ì´ì§€ ì´ë™
            self.driver.get(url)
            time.sleep(random.uniform(3, 8))
            
            # [ì¶”ê°€] ì¸ê°„ í–‰ë™ ê¸°ë°˜ ìŠ¤í¬ë¡¤ ì‹¤í–‰ 
            print("   - ğŸ¤– ì¸ê°„ í–‰ë™ ê¸°ë°˜ ìŠ¤í¬ë¡¤ ì‹œì‘...")
            try:
                human_scroll_patterns.simulate_human_scroll(self.driver)
            except Exception as e:
                print(f"   - âš ï¸ ìŠ¤í¬ë¡¤ íŒ¨í„´ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:{e}")
                # ìŠ¤í¬ë¡¤ì— ì‹¤íŒ¨í•´ë„ ë°ì´í„° ìˆ˜ì§‘ì€ ê³„ì† ì‹œë„
                pass 
            
            # ë°ì´í„° ì¶”ì¶œ
            product_data = extract_all_product_data(self.driver, url, rank, city_name=self.city_name)
            time.sleep(random.uniform(4, 9))

            # ë°ì´í„° ê²€ì¦
            if not validate_product_data(product_data):
                print(f"âš ï¸ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: ìˆœìœ„ {rank}")
                self.stats["error_count"] += 1
                return False
            
            # ê¸°ë³¸ êµ¬ì¡°ì— ë§ì¶° ë°ì´í„° ë³‘í•©
            base_data = create_product_data_structure(self.city_name, self.stats["total_processed"] + 1, rank)
            base_data.update(product_data)

            # ì¶”ê°€ í•„ë“œ ì„¤ì • (í‘œì¤€ 30ê°œ ì»¬ëŸ¼ ì§€ì›)
            if "ì¹´í…Œê³ ë¦¬" in product_data:
                # ì¹´í…Œê³ ë¦¬ì—ì„œ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ë¶„ë¥˜ë¡œ ì‚¬ìš©
                category = product_data.get("ì¹´í…Œê³ ë¦¬", "")
                base_data["ë¶„ë¥˜"] = category.split(" > ")[-1] if category else ""

            # ì œíœ´ë§í¬ëŠ” í–¥í›„ KKDAY ì œíœ´ í”„ë¡œê·¸ë¨ ì„¤ì • í›„ ì¶”ê°€ ì˜ˆì •
            base_data["ì œíœ´ë§í¬"] = ""
            
            # ì´ë¯¸ì§€ ì²˜ë¦¬
            try:
                main_img_url, thumb_img_url = get_dual_image_urls_kkday(self.driver)
                # íŒŒì¼ëª…ì— ìˆœì°¨ì ì¸ rankë¥¼ ì‚¬ìš© (ì—†ìœ¼ë©´ 0ë²ˆ)
                image_identifier = rank if rank is not None else 0
                
                if main_img_url:
                    if CONFIG.get("SAVE_IMAGES", False):
                        print("    ğŸ“¥ ë©”ì¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                        time.sleep(random.uniform(0.5, 1.5))  # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì „ ì§§ì€ ëŒ€ê¸°

                        main_img_filename = download_and_save_image_kkday(
                            main_img_url,
                            image_identifier,
                            self.city_name,
                            image_type="main"
                        )
                        base_data["ë©”ì¸ì´ë¯¸ì§€"] = main_img_filename if main_img_filename else "ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"

                        # ì „ì²´ ê²½ë¡œë„ í•¨ê»˜ ì €ì¥
                        if main_img_filename:
                            from ..utils.file_handler import get_smart_image_path
                            main_img_path = get_smart_image_path(self.city_name, image_identifier, "main")
                            base_data["ë©”ì¸ì´ë¯¸ì§€_ê²½ë¡œ"] = main_img_path

                        time.sleep(random.uniform(1, 2))

                    else:
                        base_data["ë©”ì¸ì´ë¯¸ì§€"] = main_img_url
                        
                if thumb_img_url:
                    if CONFIG.get("SAVE_IMAGES", False):
                        print("    ğŸ“¥ ì¸ë„¤ì¼ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                        thumb_img_filename = download_and_save_image_kkday(
                            thumb_img_url,
                            image_identifier,
                            self.city_name,
                            image_type="thumb"
                        )
                        base_data["ì¸ë„¤ì¼ì´ë¯¸ì§€"] = thumb_img_filename if thumb_img_filename else "ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"

                        # ì „ì²´ ê²½ë¡œë„ í•¨ê»˜ ì €ì¥
                        if thumb_img_filename:
                            from ..utils.file_handler import get_smart_image_path
                            thumb_img_path = get_smart_image_path(self.city_name, image_identifier, "thumb")
                            base_data["ì¸ë„¤ì¼ì´ë¯¸ì§€_ê²½ë¡œ"] = thumb_img_path

                        time.sleep(random.uniform(1, 2))
                    else:
                        base_data["ì¸ë„¤ì¼ì´ë¯¸ì§€"] = thumb_img_url
                        
            except Exception as e:
                import traceback
                print(f"  âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                traceback.print_exc()
            
            # CSV ì €ì¥
            if save_to_csv_kkday(base_data, self.city_name):
                # ìˆœìœ„ ì •ë³´ ì €ì¥ (product_id í¬í•¨)
                save_url_with_rank(url, rank, self.city_name, base_data["ìƒí’ˆë²ˆí˜¸"])
                
                # URL ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
                mark_url_as_processed(url, self.city_name, base_data["ìƒí’ˆë²ˆí˜¸"], rank)
                
                self.stats["success_count"] += 1
                self.stats["current_rank"] = rank
                print(f"âœ… ìƒí’ˆ í¬ë¡¤ë§ ì™„ë£Œ: ìˆœìœ„ {rank}")
                return True
            else:
                self.stats["error_count"] += 1
                return False
                
        except Exception as e:
            print(f"âŒ ìƒí’ˆ í¬ë¡¤ë§ ì‹¤íŒ¨ (ìˆœìœ„ {rank}): {e}")
            self.stats["error_count"] += 1
            return False
        finally:
            self.stats["total_processed"] += 1
    
    def load_urls_from_json(self, tab="ì „ì²´"):
        """JSON íŒŒì¼ì—ì„œ URL ë¡œë“œ (KLOOK ë°©ì‹ í˜¸í™˜)"""
        try:
            from ..utils.data_persistence import KKdayDataPersistence
            persistence = KKdayDataPersistence()

            # JSON ë°ì´í„°ì—ì„œ URL ì¶”ì¶œ
            urls = persistence.get_urls_for_stage2(self.city_name, tab)

            if urls:
                print(f"âœ… JSONì—ì„œ {len(urls)}ê°œ URL ë¡œë“œ ì™„ë£Œ")
                return urls
            else:
                print("âš ï¸ JSON íŒŒì¼ì—ì„œ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []

        except Exception as e:
            print(f"âŒ JSON URL ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def crawl_products_batch(self, urls):
        """ë°°ì¹˜ ìƒí’ˆ í¬ë¡¤ë§ (JSON/TXT í˜¸í™˜)"""
        # Stage 2 ìƒíƒœ ì´ˆê¸°í™”
        from ..utils.data_persistence import KKdayDataPersistence
        persistence = KKdayDataPersistence()

        # ì‹œì‘ ìˆœìœ„ ìë™ ê³„ì‚°
        start_rank = self.get_next_available_rank()
        print(f"ğŸ“¦ ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘: {len(urls)}ê°œ ìƒí’ˆ (ì‹œì‘ ìˆœìœ„: {start_rank})")

        current_rank = start_rank
        stage2_success = True

        try:
            for i, url in enumerate(urls):
                time.sleep(random.uniform(2, 4))
                print(f"\n{'='*50}")
                print(f"ì§„í–‰ë¥ : {i+1}/{len(urls)} ({((i+1)/len(urls)*100):.1f}%)")

                # ì´ë¯¸ ì²˜ë¦¬ëœ URLì¸ì§€ í™•ì¸
                if is_url_already_processed(url, self.city_name):
                    print(f"â­ï¸ ì´ë¯¸ ì²˜ë¦¬ëœ URL, ê±´ë„ˆëœ€")
                    self.stats["skip_count"] += 1
                    continue

                # ìƒí’ˆ í¬ë¡¤ë§
                success = self.crawl_product(url, current_rank)

                if success:
                    current_rank += 1
                else:
                    stage2_success = False

                # ì§„í–‰ìƒí™© ì¶œë ¥
                self.print_progress()

                # ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€ê¸°
                delay = random.uniform(
                CONFIG.get("MEDIUM_MIN_DELAY", 3),
                CONFIG.get("MEDIUM_MAX_DELAY", 8)
            )
            print(f"â³ {delay:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(delay)

            # ì¤‘ê°„ íœ´ì‹ (10ê°œë§ˆë‹¤)
            if (i + 1) % 10 == 0:
                long_delay = random.uniform(
                    CONFIG.get("LONG_MIN_DELAY", 15),
                    CONFIG.get("LONG_MAX_DELAY", 30)
                )
                print(f"ğŸ˜´ ê¸´ íœ´ì‹: {long_delay:.1f}ì´ˆ...")
                time.sleep(long_delay)

            # Stage 2 ì™„ë£Œ ìƒíƒœ ì €ì¥
            stage2_data = {
                "status": "success" if stage2_success else "partial",
                "data": {
                    "total_processed": self.stats["total_processed"],
                    "success_count": self.stats["success_count"],
                    "error_count": self.stats["error_count"],
                    "skip_count": self.stats["skip_count"]
                }
            }
            persistence.save_status_data(self.city_name, "ì „ì²´", stage2_data=stage2_data)

            print("\nğŸ“¦ ë°°ì¹˜ í¬ë¡¤ë§ ì™„ë£Œ")
            print(f"âœ… Stage 2 ìƒíƒœ ì €ì¥: {'ì„±ê³µ' if stage2_success else 'ë¶€ë¶„ ì„±ê³µ'}")
            return True

        except Exception as e:
            print(f"âŒ ë°°ì¹˜ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì‹¤íŒ¨ ìƒíƒœ ì €ì¥
            stage2_data = {
                "status": "failed",
                "data": {
                    "error": str(e),
                    "total_processed": self.stats.get("total_processed", 0),
                    "success_count": self.stats.get("success_count", 0)
                }
            }
            persistence.save_status_data(self.city_name, "ì „ì²´", stage2_data=stage2_data)
            return False

    def run_full_crawling(self, max_pages=3, max_products=None):
        """ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰"""
        print(f"ğŸ¯ {self.city_name} ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")

        try:
            # 1. ì´ˆê¸°í™”
            if not self.initialize():
                return False

            # 2. URL ìˆ˜ì§‘
            urls = self.collect_urls(max_pages)
            if not urls:
                print("âš ï¸ ìˆ˜ì§‘í•  URLì´ ì—†ìŠµë‹ˆë‹¤.")
                return False

            # 2.1 ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ URLë§Œ í•„í„°ë§ (ëª©ë¡ í˜ì´ì§€ ì œì™¸)
            product_urls = self.filter_product_detail_urls(urls)
            if not product_urls:
                print("âš ï¸ ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ URLì´ ì—†ìŠµë‹ˆë‹¤.")
                return False

            print(f"ğŸ“Š ì „ì²´ URL: {len(urls)}ê°œ, ìƒí’ˆ ìƒì„¸ URL: {len(product_urls)}ê°œ")

            # 3. ìµœëŒ€ ìƒí’ˆ ìˆ˜ ì œí•œ
            max_products_config = CONFIG.get("MAX_PRODUCTS_PER_CITY", None)
            if max_products:
                product_urls = product_urls[:max_products]
            elif max_products_config:
                product_urls = product_urls[:max_products_config]

            print(f"ğŸ“Š í¬ë¡¤ë§í•  ìƒí’ˆ ìˆ˜: {len(product_urls)}ê°œ")

            # 4. ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰ (ìˆœìœ„ëŠ” ë‚´ë¶€ì—ì„œ ìë™ ê³„ì‚°)
            success = self.crawl_products_batch(product_urls)

            # 5. ìµœì¢… í†µê³„
            self.stats["end_time"] = datetime.now()
            self.print_final_stats()

            # ğŸ†• êµ­ê°€ë³„ í†µí•© CSV ìë™ ìƒì„± (ì—¬ê¸°ì— ì¶”ê°€)
            from ..utils.file_handler import auto_create_country_csv_after_crawling
            auto_create_country_csv_after_crawling(self.city_name)

            return success

        except Exception as e:
            print(f"âŒ ì „ì²´ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return False
        finally:
            if self.driver:
                pass

    def print_progress(self):
        """ì§„í–‰ìƒí™© ì¶œë ¥"""
        total = self.stats["total_processed"]
        success = self.stats["success_count"]
        error = self.stats["error_count"]
        skip = self.stats["skip_count"]

        if total > 0:
            success_rate = (success / total) * 100
            print(f"ğŸ“Š ì§„í–‰ìƒí™©: ì„±ê³µ {success}, ì‹¤íŒ¨ {error}, ê±´ë„ˆëœ€ {skip}, ì„±ê³µë¥  {success_rate:.1f}%")

    def print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        print("\n" + "="*60)
        print(f"ğŸ‰ {self.city_name} í¬ë¡¤ë§ ì™„ë£Œ!")
        print("="*60)

        if self.stats["start_time"] and self.stats["end_time"]:
            duration = self.stats["end_time"] - self.stats["start_time"]
            print(f"â±ï¸ ì†Œìš”ì‹œê°„: {duration}")

        print(f"ğŸ“Š ì²˜ë¦¬ í†µê³„:")
        print(f"   â€¢ ì „ì²´ ì²˜ë¦¬: {self.stats['total_processed']}ê°œ")
        print(f"   â€¢ ì„±ê³µ: {self.stats['success_count']}ê°œ")
        print(f"   â€¢ ì‹¤íŒ¨: {self.stats['error_count']}ê°œ")
        print(f"   â€¢ ê±´ë„ˆëœ€: {self.stats['skip_count']}ê°œ")
        print(f"   â€¢ URL ìˆ˜ì§‘: {self.stats['urls_collected']}ê°œ")
        print(f"   â€¢ ë§ˆì§€ë§‰ ìˆœìœ„: {self.stats['current_rank']}")

        if self.stats["total_processed"] > 0:
            success_rate = (self.stats["success_count"] / self.stats["total_processed"]) * 100
            print(f"   â€¢ ì„±ê³µë¥ : {success_rate:.1f}%")
