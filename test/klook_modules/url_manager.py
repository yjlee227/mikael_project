"""
ğŸš€ ê·¸ë£¹ 3: KLOOK ì „ìš© URL íŒ¨í„´ + hashlib í†µí•© ê°„ì†Œí™”ëœ ìƒíƒœ ê´€ë¦¬ ì‹œìŠ¤í…œ
- KLOOK /activity/ íŒ¨í„´ ì™„ì „ ë³€ê²½
- hashlib ìµœì í™” í™œì„±í™”
- URL ì¤‘ë³µ ë°©ì§€ ë° ìƒíƒœ ê´€ë¦¬
"""

import os
import re
import hashlib
import json
import time
from datetime import datetime
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

# config ëª¨ë“ˆì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë“¤ import
from .config import CONFIG, get_city_code, is_url_processed_fast, mark_url_processed_fast

# =============================================================================
# ğŸ”— KLOOK URL íŒ¨í„´ ë° ê²€ì¦ ì‹œìŠ¤í…œ
# =============================================================================

def is_valid_klook_url(url):
    """âœ… KLOOK URL ìœ íš¨ì„± ê²€ì‚¬ (activity íŒ¨í„´)"""
    if not url or not isinstance(url, str):
        return False
    
    # KLOOK ë„ë©”ì¸ ì²´í¬
    klook_domains = [
        'klook.com',
        'www.klook.com', 
        'm.klook.com'
    ]
    
    parsed = urlparse(url)
    domain_valid = any(domain in parsed.netloc.lower() for domain in klook_domains)
    
    if not domain_valid:
        return False
    
    # /activity/ íŒ¨í„´ ì²´í¬ (KLOOK í‘œì¤€)
    activity_patterns = [
        r'/activity/\d+',           # /activity/123456
        r'/ko/activity/\d+',        # /ko/activity/123456  
        r'/en/activity/\d+',        # /en/activity/123456
        r'/activity/[^/]+',         # /activity/slug-name
    ]
    
    path_valid = any(re.search(pattern, url) for pattern in activity_patterns)
    
    # ì œì™¸í•  íŒ¨í„´ë“¤
    excluded_patterns = [
        r'/search',
        r'/category',
        r'/city',
        r'/user',
        r'/account',
        r'/cart',
        r'/checkout'
    ]
    
    excluded = any(re.search(pattern, url) for pattern in excluded_patterns)
    
    return path_valid and not excluded

def extract_klook_activity_id(url):
    """KLOOK activity ID ì¶”ì¶œ"""
    if not url:
        return None
    
    # /activity/ìˆ«ì íŒ¨í„´ì—ì„œ ID ì¶”ì¶œ
    id_patterns = [
        r'/activity/(\d+)',
        r'/ko/activity/(\d+)', 
        r'/en/activity/(\d+)'
    ]
    
    for pattern in id_patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    
    return None

def normalize_klook_url(url):
    """âœ… KLOOK URL ì •ê·œí™” (ì¤‘ë³µ ë°©ì§€ìš©)"""
    if not url:
        return url
    
    try:
        parsed = urlparse(url)
        
        # ê¸°ë³¸ ì •ê·œí™”
        normalized_url = f"{parsed.scheme}://{parsed.netloc.lower()}{parsed.path}"
        
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì •ë¦¬ (í•„ìˆ˜ë§Œ ìœ ì§€)
        if parsed.query:
            query_params = parse_qs(parsed.query)
            essential_params = {}
            
            # KLOOKì—ì„œ í•„ìˆ˜ì ì¸ íŒŒë¼ë¯¸í„°ë“¤ë§Œ ìœ ì§€
            essential_keys = ['currency', 'locale', 'aid']  # í•„ìš”ì‹œ ì¶”ê°€
            
            for key in essential_keys:
                if key in query_params:
                    essential_params[key] = query_params[key]
            
            if essential_params:
                query_string = urlencode(essential_params, doseq=True)
                normalized_url += f"?{query_string}"
        
        return normalized_url
        
    except Exception as e:
        print(f"âš ï¸ URL ì •ê·œí™” ì‹¤íŒ¨: {e}")
        return url

# =============================================================================
# ğŸ“Š URL ìƒíƒœ ê´€ë¦¬ ì‹œìŠ¤í…œ (hashlib í†µí•©)
# =============================================================================

def is_url_already_processed(url, city_name):
    """âœ… URL ì¤‘ë³µ ì²´í¬ (hashlib ì´ˆê³ ì† + CSV í˜¸í™˜ì„±)"""
    if not url:
        return True  # ë¹ˆ URLì€ ì²˜ë¦¬ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
    
    # 1. URL ì •ê·œí™”
    normalized_url = normalize_klook_url(url)
    
    # 2. hashlib ì´ˆê³ ì† ì²´í¬ (0.001ì´ˆ)
    if CONFIG.get("USE_HASH_SYSTEM", True):
        if is_url_processed_fast(normalized_url, city_name):
            return True
    
    # 3. ê¸°ì¡´ CSV í˜¸í™˜ì„± ì²´í¬ (í•„ìš”ì‹œì—ë§Œ)
    if CONFIG.get("KEEP_CSV_SYSTEM", True):
        try:
            from .config import get_completed_urls_from_csv
            completed_urls = get_completed_urls_from_csv(city_name)
            if normalized_url in completed_urls:
                # CSVì—ëŠ” ìˆì§€ë§Œ í•´ì‹œì— ì—†ìœ¼ë©´ í•´ì‹œì—ë„ ì¶”ê°€
                if CONFIG.get("USE_HASH_SYSTEM", True):
                    mark_url_processed_fast(normalized_url, city_name, "csv_sync")
                return True
        except Exception as e:
            print(f"âš ï¸ CSV í˜¸í™˜ì„± ì²´í¬ ì‹¤íŒ¨: {e}")
    
    return False

def mark_url_as_processed(url, city_name, product_number=None, rank=None):
    """âœ… URLì„ ì²˜ë¦¬ ì™„ë£Œë¡œ í‘œì‹œ (hashlib + V2 3-tier + ìˆœìœ„ ì •ë³´)"""
    if not url:
        return False
    
    normalized_url = normalize_klook_url(url)
    
    try:
        # 1. hashlib ì‹œìŠ¤í…œì— ê¸°ë¡ (ì´ˆê³ ì†) - ìˆœìœ„ ì •ë³´ í¬í•¨
        if CONFIG.get("USE_HASH_SYSTEM", True):
            mark_url_processed_fast(normalized_url, city_name, product_number, rank)
        
        # 2. V2 3-tier ì‹œìŠ¤í…œì— ê¸°ë¡
        if CONFIG.get("USE_V2_URL_SYSTEM", True):
            from .config import save_url_to_log
            save_url_to_log(city_name, normalized_url)
        
        return True
        
    except Exception as e:
        print(f"âŒ URL ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ ì‹¤íŒ¨: {e}")
        return False

def get_unprocessed_urls(url_list, city_name):
    """ë¯¸ì²˜ë¦¬ URLë§Œ í•„í„°ë§"""
    if not url_list:
        return []
    
    unprocessed = []
    total_count = len(url_list)
    processed_count = 0
    
    print(f"ğŸ” {total_count}ê°œ URL ì¤‘ë³µ ê²€ì‚¬ ì¤‘...")
    
    for url in url_list:
        if is_valid_klook_url(url):
            if not is_url_already_processed(url, city_name):
                unprocessed.append(url)
            else:
                processed_count += 1
    
    print(f"   ğŸ“Š ê²°ê³¼: ë¯¸ì²˜ë¦¬ {len(unprocessed)}ê°œ, ì¤‘ë³µ ì œì™¸ {processed_count}ê°œ")
    return unprocessed

# =============================================================================
# ğŸ“‚ URL ìˆ˜ì§‘ ë° ì €ì¥ ì‹œìŠ¤í…œ
# =============================================================================

def save_urls_to_collection(urls, city_name, source="manual"):
    """URLì„ ìˆ˜ì§‘ í´ë”ì— ì €ì¥ (V2 3-tier ì‹œìŠ¤í…œ)"""
    if not urls:
        return False
    
    try:
        # V2 ìˆ˜ì§‘ í´ë”ì— ì €ì¥
        collection_dir = CONFIG.get("V2_URL_COLLECTED", "url_collected")
        os.makedirs(collection_dir, exist_ok=True)
        
        city_code = get_city_code(city_name)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{city_code}_{source}_{timestamp}.json"
        filepath = os.path.join(collection_dir, filename)
        
        # JSON í˜•íƒœë¡œ ì €ì¥ (ë©”íƒ€ë°ì´í„° í¬í•¨)
        data = {
            "city_name": city_name,
            "city_code": city_code,
            "source": source,
            "collected_at": datetime.now().isoformat(),
            "total_urls": len(urls),
            "urls": urls
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ URL ìˆ˜ì§‘ ì €ì¥ ì™„ë£Œ: {filename} ({len(urls)}ê°œ)")
        return True
        
    except Exception as e:
        print(f"âŒ URL ìˆ˜ì§‘ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

def load_urls_from_collection(city_name, source_filter=None):
    """ìˆ˜ì§‘ í´ë”ì—ì„œ URL ë¡œë“œ"""
    try:
        collection_dir = CONFIG.get("V2_URL_COLLECTED", "url_collected")
        if not os.path.exists(collection_dir):
            return []
        
        city_code = get_city_code(city_name)
        all_urls = []
        
        # í•´ë‹¹ ë„ì‹œì˜ ëª¨ë“  ìˆ˜ì§‘ íŒŒì¼ ì°¾ê¸°
        for filename in os.listdir(collection_dir):
            if filename.startswith(city_code) and filename.endswith('.json'):
                if source_filter and source_filter not in filename:
                    continue
                
                filepath = os.path.join(collection_dir, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if 'urls' in data:
                            all_urls.extend(data['urls'])
                except Exception as e:
                    print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {filename} - {e}")
        
        # ì¤‘ë³µ ì œê±°
        unique_urls = list(set(all_urls))
        print(f"ğŸ“‚ ìˆ˜ì§‘ëœ URL ë¡œë“œ: {len(unique_urls)}ê°œ (ì¤‘ë³µ ì œê±° í›„)")
        return unique_urls
        
    except Exception as e:
        print(f"âŒ URL ìˆ˜ì§‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

# =============================================================================
# ğŸ”— URL ë¶„ì„ ë° í†µê³„
# =============================================================================

def analyze_url_patterns(urls):
    """URL íŒ¨í„´ ë¶„ì„"""
    if not urls:
        return {}
    
    analysis = {
        "total_urls": len(urls),
        "valid_klook_urls": 0,
        "activity_ids": [],
        "domains": {},
        "languages": {"ko": 0, "en": 0, "other": 0},
        "patterns": {}
    }
    
    for url in urls:
        if is_valid_klook_url(url):
            analysis["valid_klook_urls"] += 1
            
            # Activity ID ì¶”ì¶œ
            activity_id = extract_klook_activity_id(url)
            if activity_id:
                analysis["activity_ids"].append(activity_id)
            
            # ë„ë©”ì¸ ë¶„ì„
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            analysis["domains"][domain] = analysis["domains"].get(domain, 0) + 1
            
            # ì–¸ì–´ ë¶„ì„
            if '/ko/' in url:
                analysis["languages"]["ko"] += 1
            elif '/en/' in url:
                analysis["languages"]["en"] += 1
            else:
                analysis["languages"]["other"] += 1
    
    # ê³ ìœ  Activity ID ìˆ˜
    analysis["unique_activities"] = len(set(analysis["activity_ids"]))
    
    return analysis

def get_url_collection_stats(city_name):
    """URL ìˆ˜ì§‘ í†µê³„"""
    try:
        collection_dir = CONFIG.get("V2_URL_COLLECTED", "url_collected")
        if not os.path.exists(collection_dir):
            return {"total_files": 0, "total_urls": 0}
        
        city_code = get_city_code(city_name)
        stats = {
            "total_files": 0,
            "total_urls": 0,
            "sources": {},
            "latest_collection": None
        }
        
        latest_time = 0
        
        for filename in os.listdir(collection_dir):
            if filename.startswith(city_code) and filename.endswith('.json'):
                stats["total_files"] += 1
                
                filepath = os.path.join(collection_dir, filename)
                try:
                    # íŒŒì¼ ì •ë³´
                    file_time = os.path.getmtime(filepath)
                    if file_time > latest_time:
                        latest_time = file_time
                        stats["latest_collection"] = datetime.fromtimestamp(file_time).strftime("%Y-%m-%d %H:%M:%S")
                    
                    # ë‚´ìš© ë¶„ì„
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        url_count = data.get('total_urls', 0)
                        source = data.get('source', 'unknown')
                        
                        stats["total_urls"] += url_count
                        stats["sources"][source] = stats["sources"].get(source, 0) + url_count
                        
                except Exception as e:
                    print(f"âš ï¸ í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {filename} - {e}")
        
        return stats
        
    except Exception as e:
        print(f"âŒ URL ìˆ˜ì§‘ í†µê³„ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

print("âœ… ê·¸ë£¹ 3 ì™„ë£Œ: KLOOK ì „ìš© URL íŒ¨í„´ + hashlib í†µí•© ê°„ì†Œí™”ëœ ìƒíƒœ ê´€ë¦¬ ì‹œìŠ¤í…œ!")
print("ğŸš€ hashlib ìµœì í™”: í™œì„±í™”")
print("ğŸ§¹ KLOOK /activity/ íŒ¨í„´ìœ¼ë¡œ ì™„ì „ ë³€ê²½ ì™„ë£Œ")
print("   ğŸ”— URL ê²€ì¦:")
print("   - is_valid_klook_url(): KLOOK activity URL ê²€ì¦")
print("   - normalize_klook_url(): URL ì •ê·œí™”")
print("   ğŸ“Š ìƒíƒœ ê´€ë¦¬:")
print("   - is_url_already_processed(): hashlib ì´ˆê³ ì† ì¤‘ë³µ ì²´í¬")
print("   - mark_url_as_processed(): ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ")
print("   ğŸ“‚ ìˆ˜ì§‘ ì‹œìŠ¤í…œ:")
print("   - save_urls_to_collection(): V2 3-tier ìˆ˜ì§‘ ì €ì¥")
print("   - load_urls_from_collection(): ìˆ˜ì§‘ URL ë¡œë“œ")
print("   ğŸ“ˆ ë¶„ì„ ë„êµ¬:")
print("   - analyze_url_patterns(): URL íŒ¨í„´ ë¶„ì„")
print("   - get_url_collection_stats(): ìˆ˜ì§‘ í†µê³„")