# =============================================================================
# ğŸ¯ ìš°ë¦¬ ì‹œë‚˜ë¦¬ì˜¤ì— ë§ëŠ” í•¨ìˆ˜ ë§¤í•‘ ë° ë¡œì§ ì„¤ê³„
# =============================================================================

def execute_tab_based_sequential_crawling(city_name, driver, target_products_per_tab=50):
    """
    ğŸš€ íƒ­ë³„ ìˆœì°¨ í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ë§ (ìš°ë¦¬ ì‹œë‚˜ë¦¬ì˜¤ ì™„ë²½ êµ¬í˜„)
    
    ì‹œë‚˜ë¦¬ì˜¤:
    1. ì „ì²´â†’íˆ¬ì–´&ì•¡í‹°ë¹„í‹°â†’í‹°ì¼“&ì…ì¥ê¶Œâ†’êµí†µâ†’ê¸°íƒ€ ìˆœì„œ
    2. ê° íƒ­ë§ˆë‹¤ í˜ì´ì§€ë³„ 15ê°œ URL ìˆ˜ì§‘ â†’ ì¦‰ì‹œ ìƒì„¸í¬ë¡¤ë§
    3. 10ê°œë§ˆë‹¤ CSV ì €ì¥
    4. ë­í‚¹ ìˆœì„œ ë³´ì¥
    """
    print(f"ğŸ¯ {city_name} íƒ­ë³„ ìˆœì°¨ í¬ë¡¤ë§ ì‹œì‘!")
    print("=" * 60)
    
    # íƒ­ ìˆœì„œ ì •ì˜ (ë­í‚¹ ìš°ì„ ìˆœìœ„)
    tab_sequence = [
        ("ì „ì²´", 50),
        ("íˆ¬ì–´&ì•¡í‹°ë¹„í‹°", 50), 
        ("í‹°ì¼“&ì…ì¥ê¶Œ", 50),
        ("êµí†µ", 50),
        ("ê¸°íƒ€", 50)
    ]
    
    all_results = []
    overall_rank = 1  # ì „ì²´ ë­í‚¹
    
    # 1. íƒ­ ê°ì§€
    print("ğŸ” íƒ­ êµ¬ì¡° ê°ì§€...")
    detected_tabs = detect_tabs_with_enhanced_fallback(driver)
    if not detected_tabs:
        print("âŒ íƒ­ ê°ì§€ ì‹¤íŒ¨")
        return []
    
    continent, country = get_city_info(city_name) or ("ì•„ì‹œì•„", "ëŒ€í•œë¯¼êµ­")
    
    # 2. ê° íƒ­ë³„ ìˆœì°¨ ì²˜ë¦¬
    for tab_index, (tab_name, target_count) in enumerate(tab_sequence):
        print(f"\nğŸ”„ [{tab_index+1}/5] '{tab_name}' íƒ­ í¬ë¡¤ë§ ì‹œì‘ (ëª©í‘œ: {target_count}ê°œ)")
        
        # 2-1. íƒ­ í´ë¦­
        if not click_tab_enhanced(driver, tab_name, detected_tabs):
            print(f"âŒ '{tab_name}' íƒ­ í´ë¦­ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
            continue
        
        # 2-2. í•´ë‹¹ íƒ­ì—ì„œ í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ë§
        tab_results = crawl_single_tab_with_ranking(
            driver, city_name, tab_name, target_count, 
            continent, country, overall_rank
        )
        
        all_results.extend(tab_results)
        overall_rank += len(tab_results)
        
        print(f"âœ… '{tab_name}' íƒ­ ì™„ë£Œ: {len(tab_results)}ê°œ í¬ë¡¤ë§")
    
    print(f"\nğŸ‰ ì „ì²´ íƒ­ë³„ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(all_results)}ê°œ ìƒí’ˆ")
    return all_results


def crawl_single_tab_with_ranking(driver, city_name, tab_name, target_count, continent, country, start_rank):
    """
    ğŸ¯ ë‹¨ì¼ íƒ­ì—ì„œ ë­í‚¹ ìˆœì„œ ë³´ì¥ í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ë§
    
    í•µì‹¬:
    - í˜ì´ì§€ë‹¹ 15ê°œ URL ìˆ˜ì§‘
    - í™”ë©´ ìˆœì„œëŒ€ë¡œ í¬ë¡¤ë§ (ë­í‚¹ ë³´ì¥)
    - 10ê°œë§ˆë‹¤ CSV ì €ì¥
    - ìƒì„¸â†’ëª©ë¡ ë°˜ë³µ
    """
    print(f"ğŸ“‹ '{tab_name}' íƒ­ ìƒì„¸ í¬ë¡¤ë§ ì‹œì‘")
    
    tab_results = []
    current_page = 1
    current_rank = start_rank
    
    while len(tab_results) < target_count:
        print(f"  ğŸ“„ í˜ì´ì§€ {current_page} ì²˜ë¦¬ ì¤‘...")
        
        # Step 1: í˜„ì¬ í˜ì´ì§€ URL ìˆ˜ì§‘ (15ê°œ)
        page_urls = collect_basic_urls_from_current_view(driver)
        if not page_urls:
            print(f"  âš ï¸ í˜ì´ì§€ {current_page}ì—ì„œ URL ì—†ìŒ, ì¤‘ë‹¨")
            break
        
        print(f"  âœ… {len(page_urls)}ê°œ URL ìˆ˜ì§‘ (KLOOK í˜ì´ì§€ë‹¹ 15ê°œ ê¸°ì¤€)")
        
        # Step 2: URLë“¤ì„ í™”ë©´ ìˆœì„œëŒ€ë¡œ í¬ë¡¤ë§ (ë­í‚¹ ë³´ì¥)
        for url_index, product_url in enumerate(page_urls):
            if len(tab_results) >= target_count:
                print(f"  ğŸ¯ ëª©í‘œ {target_count}ê°œ ë‹¬ì„±!")
                break
            
            print(f"    ğŸ“¦ [{current_rank}ìœ„] ìƒí’ˆ í¬ë¡¤ë§ ({current_page}í˜ì´ì§€ {url_index+1}ë²ˆì§¸)")
            
            # Step 3: ìƒì„¸í˜ì´ì§€ í¬ë¡¤ë§
            result = crawl_single_product_with_stop_check(
                driver, product_url, current_rank, city_name, 
                continent, country, current_page
            )
            
            if result:
                # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                result['íƒ­ëª…'] = tab_name
                result['íƒ­ë‚´_ë­í‚¹'] = len(tab_results) + 1
                result['ì „ì²´_ë­í‚¹'] = current_rank
                result['í˜ì´ì§€'] = current_page
                
                tab_results.append(result)
                print(f"    âœ… ì™„ë£Œ: {result.get('ìƒí’ˆëª…', 'Unknown')[:25]}...")
                
                # Step 4: 10ê°œë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
                if len(tab_results) % 10 == 0:
                    save_batch_data(tab_results[-10:], f"{city_name}_{tab_name}")
                    print(f"    ğŸ’¾ ì¤‘ê°„ ì €ì¥: {len(tab_results)}ê°œ")
            
            current_rank += 1
            time.sleep(2)  # ìš”ì²­ ê°„ê²©
        
        # Step 5: ë‹¤ìŒ í˜ì´ì§€ ì´ë™
        if len(tab_results) < target_count:
            print(f"  ğŸ”„ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™...")
            next_result = click_next_page_enhanced(driver, current_page)
            
            if not next_result[0]:  # ë‹¤ìŒ í˜ì´ì§€ ì—†ìŒ
                print(f"  âš ï¸ ë‹¤ìŒ í˜ì´ì§€ ì—†ìŒ: {next_result[1]}")
                break
            
            current_page += 1
            time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
    
    # ìµœì¢… ì €ì¥ (ë‚˜ë¨¸ì§€)
    remaining = len(tab_results) % 10
    if remaining > 0:
        save_batch_data(tab_results[-remaining:], f"{city_name}_{tab_name}")
        print(f"  ğŸ’¾ ìµœì¢… ì €ì¥: {remaining}ê°œ")
    
    return tab_results


# =============================================================================
# ğŸ¯ Group 12ì—ì„œ ì‚¬ìš©í•  í†µí•© í•¨ìˆ˜
# =============================================================================

def execute_unified_klook_crawling_workflow(city_name, driver, total_target=250):
    """
    Group 12 ìœ„ì ¯ì—ì„œ í˜¸ì¶œí•  í†µí•© ì›Œí¬í”Œë¡œìš°
    
    Args:
        city_name (str): ë„ì‹œëª…
        driver: ì›¹ë“œë¼ì´ë²„
        total_target (int): ì „ì²´ ëª©í‘œ ìƒí’ˆ ìˆ˜ (ê¸°ë³¸ 250ê°œ = íƒ­ë‹¹ 50ê°œì”©)
    
    Returns:
        dict: ì‹¤í–‰ ê²°ê³¼
    """
    print(f"ğŸš€ í†µí•© KLOOK í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš° ì‹œì‘: {city_name}")
    print(f"ğŸ¯ ëª©í‘œ: {total_target}ê°œ ìƒí’ˆ (íƒ­ë‹¹ {total_target//5}ê°œì”©)")
    
    try:
        # 1. ë¸Œë¼ìš°ì € ì´ˆê¸°í™”ëŠ” ì´ë¯¸ ì™„ë£Œëœ ìƒíƒœ
        
        # 2. ë©”ì¸í˜ì´ì§€ â†’ ê²€ìƒ‰ â†’ ê²°ê³¼í˜ì´ì§€ (ì´ë¯¸ ì™„ë£Œëœ ìƒíƒœ)
        
        # 3. íƒ­ë³„ ìˆœì°¨ í¬ë¡¤ë§ ì‹¤í–‰
        all_results = execute_tab_based_sequential_crawling(
            city_name, driver, target_target//5
        )
        
        # 4. ê²°ê³¼ ë°˜í™˜
        return {
            "success": True,
            "total_crawled": len(all_results),
            "results": all_results,
            "execution_time": datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"âŒ í†µí•© ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {e}")
        return {
            "success": False,
            "error": str(e),
            "total_crawled": 0,
            "results": []
        }


# =============================================================================
# ğŸ¯ ê¸°ì¡´ í•¨ìˆ˜ë“¤ê³¼ì˜ ë§¤í•‘ (FUNCTION_REFERENCE.md ê¸°ì¤€)
# =============================================================================

FUNCTION_MAPPING = {
    # ìš°ë¦¬ ì‹œë‚˜ë¦¬ì˜¤ -> ê¸°ì¡´ í•¨ìˆ˜ëª…
    "ë¸Œë¼ìš°ì €_ì´ˆê¸°í™”": "setup_driver",
    "ë©”ì¸í˜ì´ì§€_ì´ë™": "go_to_main_page", 
    "ê²€ìƒ‰ì–´_ì…ë ¥": "find_and_fill_search",
    "ê²€ìƒ‰_ë²„íŠ¼_í´ë¦­": "click_search_button",
    "íŒì—…_ì²˜ë¦¬": "handle_popup",
    "íƒ­_ê°ì§€": "detect_tabs_with_enhanced_fallback",
    "íƒ­_í´ë¦­": "click_tab_enhanced",
    "URL_ìˆ˜ì§‘": "collect_basic_urls_from_current_view",
    "ìƒí’ˆ_í¬ë¡¤ë§": "crawl_single_product_with_stop_check",
    "ë‹¤ìŒí˜ì´ì§€_í´ë¦­": "click_next_page_enhanced", 
    "ë°ì´í„°_ì €ì¥": "save_batch_data",
    "ì •ì§€_ì²´í¬": "check_stop_flag",
    "ë„ì‹œ_ì •ë³´": "get_city_info"
}

print("âœ… ì‹œë‚˜ë¦¬ì˜¤ ë§¤í•‘ ì™„ë£Œ!")
print("ğŸ”§ ê¸°ì¡´ í•¨ìˆ˜ë“¤ì„ í™œìš©í•œ ì™„ë²½í•œ ì›Œí¬í”Œë¡œìš° ì„¤ê³„")
print("ğŸ’¡ Group 12ì—ì„œ execute_unified_klook_crawling_workflow() í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤!")