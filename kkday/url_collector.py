import sys
import os
import time
import random
import inspect

# ======================================================================
# ğŸ•µï¸ ìŠ¤í¬ë¦½íŠ¸ ìì²´ ì§„ë‹¨ ì‹œì‘ ğŸ•µï¸
# ======================================================================
print("="*70)
print("ğŸ•µï¸  ìŠ¤í¬ë¦½íŠ¸ ìì²´ ì§„ë‹¨ ì‹œì‘ ğŸ•µï¸")
try:
    # Get the full path of the currently running script
    my_path = inspect.getframeinfo(inspect.currentframe()).filename
    print(f"   - ì‹¤í–‰ ì¤‘ì¸ íŒŒì¼ ê²½ë¡œ: {my_path}")
    with open(my_path, 'r', encoding='utf-8') as f:
        for line in f:
            if 'TARGET_PRODUCTS' in line and not line.strip().startswith('#'):
                print(f"   - íŒŒì¼ì—ì„œ ì½ì€ ì„¤ì •: {line.strip()}")
                break
except Exception as e:
    print(f"   - ì§„ë‹¨ ì¤‘ ì˜¤ë¥˜: {e}")
print("="*70)
# ======================================================================

# Add src to path to find the crawler modules
sys.path.append('./src')
sys.path.append('.')

try:
    from src.scraper.crawler import KKdayCrawler
    print("âœ… KKday ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ KKday ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    raise

# ===== ğŸ¯ ì‚¬ìš©ì ì„¤ì • ì˜ì—­ =====
CITY_NAME = "ì¹˜ì•™ë§ˆì´"
TARGET_PRODUCTS = 2  # ìˆ˜ì§‘í•  ìƒí’ˆ URL ìˆ˜
MAX_PAGES = 5       # URLì„ ì¶©ë¶„íˆ ìˆ˜ì§‘í•˜ê¸° ìœ„í•´ í˜ì´ì§€ ìˆ˜ë¥¼ ë„‰ë„‰í•˜ê²Œ ì„¤ì •
OUTPUT_FILE = "kkday_urls.txt"

print("="*70)
print("ğŸš€ KKday URL ìˆ˜ì§‘ê¸° ì‹œì‘")
print("="*70)
print(f"   ğŸ™ï¸ ë„ì‹œ: {CITY_NAME}")
print(f"   ğŸ¯ ëª©í‘œ URL: {TARGET_PRODUCTS}ê°œ")
print(f"   ğŸ“„ ìµœëŒ€ í˜ì´ì§€: {MAX_PAGES}ê°œ")
print(f"   ğŸ’¾ ì¶œë ¥ íŒŒì¼: {OUTPUT_FILE}")

crawler = None
collected_urls = []

try:
    # 1. í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    print("\nğŸ—ï¸ KKday í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”...")
    crawler = KKdayCrawler(city_name=CITY_NAME)
    if not crawler.initialize():
        raise Exception("í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨")

    # 2. íš¨ìœ¨ì ì¸ URL ìˆ˜ì§‘ (ëª©í‘œ ìˆ˜ëŸ‰ ë„ë‹¬ ì‹œ ì¤‘ë‹¨)
    print("\nğŸ”— URL ìˆ˜ì§‘ ì¤‘...")
    collected_urls = crawler.collect_urls(
        max_pages=MAX_PAGES,
        max_products=TARGET_PRODUCTS
    )

    if not collected_urls:
        print("âš ï¸ ìˆ˜ì§‘ëœ URLì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # 3. íŒŒì¼ì— ì €ì¥
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            for url in collected_urls:
                f.write(url + '\n')
        
        print(f"âœ… URL {len(collected_urls)}ê°œë¥¼ '{OUTPUT_FILE}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

except Exception as e:
    print(f"âŒ URL ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
finally:
    if crawler and crawler.driver:
        print("\nğŸŒ ë“œë¼ì´ë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        crawler.driver.quit()
    print("ğŸ URL ìˆ˜ì§‘ê¸° ì¢…ë£Œ")