{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ KKday í¬ë¡¤ëŸ¬ v1.0\n",
    "## í†µí•© ìƒí’ˆ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ\n",
    "\n",
    "### ğŸ“‹ ì£¼ìš” ê¸°ëŠ¥:\n",
    "- âœ… **ì„œìš¸ í˜ì´ì§€ ì§„ì…ìœ¼ë¡œ ë´‡ íƒì§€ íšŒí”¼**\n",
    "- âœ… **ê²€ìƒ‰ë°•ìŠ¤ í™œìš©í•œ ë„ì‹œë³„ í¬ë¡¤ë§**\n",
    "- âœ… **ì¤‘ì•™í™” ì…€ë ‰í„° ì‹œìŠ¤í…œ** (fallback ì§€ì›)\n",
    "- âœ… **ê°œë³„ íŒŒì„œ í•¨ìˆ˜** (ë³µì¡í•œ ì†ì„± ì²˜ë¦¬)\n",
    "- âœ… **ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬** ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜\n",
    "- âœ… **24ê°œ CSV ì»¬ëŸ¼** ì™„ì „ ì§€ì›\n",
    "- âœ… **ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ** ë° ê²½ë¡œ ê´€ë¦¬\n",
    "- âœ… **URL ì¤‘ë³µ ì²˜ë¦¬** ë° ìˆœìœ„ ì—°ì†ì„± ë³´ì¥\n",
    "\n",
    "### ğŸ”¥ **v1.0 í•µì‹¬ íŠ¹ì§•:**\n",
    "- **KKdayCrawler í´ë˜ìŠ¤**: í†µí•© í¬ë¡¤ë§ ì‹œìŠ¤í…œ\n",
    "- **ì¤‘ì•™í™” + ê°œë³„ íŒŒì„œ**: ìµœì  í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜\n",
    "- **ë”ë¯¸ í…ŒìŠ¤íŠ¸ ê²€ì¦**: 100% ë¡œì§ ê²€ì¦ ì™„ë£Œ\n",
    "- **TimeoutException ì²˜ë¦¬**: ì•ˆì •ì ì¸ ëŒ€ê¸° ì‹œìŠ¤í…œ\n",
    "- **StaleElement ì²˜ë¦¬**: DOM ë³€í™” ëŒ€ì‘\n",
    "\n",
    "### ğŸ¯ ì‚¬ìš©ë²•:\n",
    "1. **ì•„ë˜ 1ë²ˆ ì…€ì—ì„œ ë„ì‹œëª… ë° ëª©í‘œ ìˆ˜ëŸ‰ ì„¤ì •**\n",
    "2. **Run All ì‹¤í–‰** (ì™„ì „ ìë™í™”)\n",
    "3. **ê²°ê³¼ ë¶„ì„** (ìë™ í†µê³„ ìƒì„±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ğŸ¯ ì‚¬ìš©ì ì„¤ì • ì˜ì—­ =====\n",
    "\n",
    "# 1. í¬ë¡¤ë§í•  ë„ì‹œëª… ì…ë ¥\n",
    "CITY_NAME = \"ë°©ì½•\"   #ğŸ”¥ğŸ”¥ ë„ì‹œëª… ì…ë ¥ ğŸ”¥ğŸ”¥\n",
    "\n",
    "# 2. ìˆ˜ì§‘í•  ìƒí’ˆ ìˆ˜ ì„¤ì •\n",
    "TARGET_PRODUCTS = 2  # ìˆ˜ì§‘í•  ìƒí’ˆ ìˆ˜\n",
    "\n",
    "# 3. í¬ë¡¤ë§ ë²”ìœ„ ì„¤ì •\n",
    "MAX_PAGES = 3  # ìµœëŒ€ ê²€ìƒ‰í•  í˜ì´ì§€ ìˆ˜\n",
    "\n",
    "# 4. ì´ë¯¸ì§€ ì €ì¥ ì—¬ë¶€\n",
    "SAVE_IMAGES = True  # True: ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ, False: URLë§Œ ì €ì¥\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸš€ KKday í¬ë¡¤ëŸ¬ v1.0 ì‹œì‘\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===== í™˜ê²½ ì„¤ì • ë° ëª¨ë“ˆ Import =====\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# í˜„ì¬ kkday í´ë”ì—ì„œ src í´ë”ì— ì ‘ê·¼\n",
    "sys.path.append('./src')\n",
    "sys.path.append('.')\n",
    "\n",
    "# KKday í”„ë¡œì íŠ¸ ëª¨ë“ˆ import\n",
    "try:\n",
    "    from src.scraper.crawler import KKdayCrawler\n",
    "    from src.config import CONFIG\n",
    "    from src.utils.file_handler import create_product_data_structure, ensure_directory_structure\n",
    "    print(\"âœ… KKday ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ KKday ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ’¡ src/ í´ë” êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    raise\n",
    "\n",
    "# ì˜ì¡´ì„± í™•ì¸\n",
    "try:\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    print(\"âœ… Selenium ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ğŸ’¡ í•´ê²°: pip install selenium\")\n",
    "    raise\n",
    "\n",
    "# ===== ì„¤ì • ê²€ì¦ =====\n",
    "print(\"\\nğŸ“‹ í¬ë¡¤ë§ ì„¤ì •:\")\n",
    "print(f\"   ğŸ™ï¸ ë„ì‹œ: {CITY_NAME}\")\n",
    "print(f\"   ğŸ¯ ëª©í‘œ ìƒí’ˆ: {TARGET_PRODUCTS}ê°œ\")\n",
    "print(f\"   ğŸ“„ ìµœëŒ€ í˜ì´ì§€: {MAX_PAGES}ê°œ\")\n",
    "print(f\"   ğŸ“¸ ì´ë¯¸ì§€ ì €ì¥: {'âœ…' if SAVE_IMAGES else 'âŒ'}\")\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ë³´\n",
    "try:\n",
    "    ensure_directory_structure(CITY_NAME)\n",
    "    print(f\"   ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ë³´ ì™„ë£Œ\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ë³´ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "print(\"\\nğŸ¯ ì„¤ì • ì™„ë£Œ - í¬ë¡¤ë§ ì‹œì‘ ì¤€ë¹„!\")\n",
    "print(\"ğŸ’¡ ì§„í–‰ìƒí™©ì€ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ğŸš€ ë©”ì¸ í¬ë¡¤ë§ ì‹¤í–‰ =====\n",
    "print(f\"ğŸš€ '{CITY_NAME}' í¬ë¡¤ë§ ì‹œì‘!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# í¬ë¡¤ë§ ì‹œì‘ ì‹œê°„ ê¸°ë¡\n",
    "start_time = datetime.now()\n",
    "print(f\"â° ì‹œì‘ ì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# KKday í¬ë¡¤ëŸ¬ ìƒì„± ë° ì‹¤í–‰\n",
    "crawler = None\n",
    "crawling_success = False\n",
    "\n",
    "try:\n",
    "    # 1. KKday í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”\n",
    "    print(f\"\\nğŸ—ï¸ KKday í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”...\")\n",
    "    crawler = KKdayCrawler(city_name=CITY_NAME)\n",
    "    \n",
    "    # 2. ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰\n",
    "    print(f\"\\nğŸ¯ ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰ (ìµœëŒ€ {MAX_PAGES}í˜ì´ì§€, {TARGET_PRODUCTS}ê°œ ìƒí’ˆ)\")\n",
    "    crawling_success = crawler.run_full_crawling(\n",
    "        max_pages=MAX_PAGES,\n",
    "        max_products=TARGET_PRODUCTS\n",
    "    )\n",
    "    \n",
    "    if crawling_success:\n",
    "        print(\"\\nğŸ‰ í¬ë¡¤ë§ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆì§€ë§Œ ì¼ë¶€ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâ¹ï¸ ì‚¬ìš©ìê°€ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    crawling_success = False\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    crawling_success = False\n",
    "\n",
    "finally:\n",
    "    # í¬ë¡¤ë§ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡\n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nâ° ì¢…ë£Œ ì‹œê°„: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {duration}\")\n",
    "    \n",
    "    # í¬ë¡¤ëŸ¬ í†µê³„ ì¶œë ¥ (í¬ë¡¤ëŸ¬ê°€ ìˆëŠ” ê²½ìš°)\n",
    "    if crawler and hasattr(crawler, 'stats'):\n",
    "        print(f\"\\nğŸ“Š í¬ë¡¤ë§ í†µê³„:\")\n",
    "        stats = crawler.stats\n",
    "        print(f\"   â€¢ ì „ì²´ ì²˜ë¦¬: {stats.get('total_processed', 0)}ê°œ\")\n",
    "        print(f\"   â€¢ ì„±ê³µ: {stats.get('success_count', 0)}ê°œ\")\n",
    "        print(f\"   â€¢ ì‹¤íŒ¨: {stats.get('error_count', 0)}ê°œ\")\n",
    "        print(f\"   â€¢ ê±´ë„ˆëœ€: {stats.get('skip_count', 0)}ê°œ\")\n",
    "        \n",
    "        if stats.get('total_processed', 0) > 0:\n",
    "            success_rate = (stats.get('success_count', 0) / stats.get('total_processed', 1)) * 100\n",
    "            print(f\"   â€¢ ì„±ê³µë¥ : {success_rate:.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ğŸ í¬ë¡¤ë§ ì‹¤í–‰ ë‹¨ê³„ ì™„ë£Œ\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ë¶„ì„ =====\n",
    "print(f\"ğŸ“Š '{CITY_NAME}' í¬ë¡¤ë§ ê²°ê³¼ ë¶„ì„\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "# 'src.config'ì—ì„œ ë„ì‹œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ë¥¼ import í•©ë‹ˆë‹¤.\n",
    "from src.config import get_city_location\n",
    "\n",
    "try:\n",
    "    # 1. í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ\n",
    "    print(\"\\nğŸ” í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ...\")\n",
    "    print(\"   (ìƒíƒœ ìš”ì•½ì€ ì´ì „ ì…€ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì„¸ìš”)\")\n",
    "\n",
    "    # 2. CSV íŒŒì¼ ë¶„ì„\n",
    "    print(\"\\nğŸ“‹ CSV ë°ì´í„° ë¶„ì„...\")\n",
    "\n",
    "    csv_path = None\n",
    "    df = None\n",
    "    csv_files_found = False\n",
    "    expected_csv_path = \"ê²½ë¡œ ë¯¸ì„¤ì •\"  # ë¡œê·¸ ì¶œë ¥ì„ ìœ„í•œ ë³€ìˆ˜\n",
    "\n",
    "    try:\n",
    "        continent, country = get_city_location(CITY_NAME)\n",
    "        is_city_state = CITY_NAME == country or CITY_NAME in [\"í™ì½©\", \"ì‹±ê°€í¬ë¥´\", \"ë§ˆì¹´ì˜¤\", \"ê´Œ\"]\n",
    "\n",
    "        if is_city_state:\n",
    "            # ë„ì‹œêµ­ê°€: ëŒ€ë¥™ ì§í•˜ì— í†µí•© íŒŒì¼\n",
    "            expected_csv_path = os.path.join(\"data\", continent, f\"{CITY_NAME}_í†µí•©_kkday_products.csv\")\n",
    "            if os.path.exists(expected_csv_path):\n",
    "                csv_path = expected_csv_path\n",
    "        else:\n",
    "            # ì¼ë°˜ êµ­ê°€: êµ­ê°€ í´ë” ì•„ë˜ì— í†µí•© íŒŒì¼\n",
    "            expected_csv_path = os.path.join(\"data\", continent, country, f\"{country}_í†µí•©_kkday_products.csv\")\n",
    "            if os.path.exists(expected_csv_path):\n",
    "                csv_path = expected_csv_path\n",
    "            else:\n",
    "                # í´ë°±: ë„ì‹œë³„ íŒŒì¼ í™•ì¸ (í†µí•© íŒŒì¼ì´ ìƒì„± ì•ˆëì„ ê²½ìš°)\n",
    "                fallback_path = os.path.join(\"data\", continent, country, CITY_NAME, f\"kkday_{CITY_NAME}_products.csv\")\n",
    "                if os.path.exists(fallback_path):\n",
    "                    csv_path = fallback_path\n",
    "                    expected_csv_path = fallback_path  # ë¡œê·¸ ì¶œë ¥ì„ ìœ„í•´ ê²½ë¡œ ì—…ë°ì´íŠ¸\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ CSV ê²½ë¡œ ê²°ì • ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "    if csv_path and os.path.exists(csv_path):\n",
    "        try:\n",
    "            print(f\"   ğŸ“„ ë¶„ì„í•  CSV íŒŒì¼: {csv_path}\")\n",
    "            df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
    "            csv_files_found = True\n",
    "            print(f\"\\nğŸ“ˆ ë°ì´í„° í†µê³„:\")\n",
    "            print(f\"   â€¢ ì´ ìƒí’ˆ ìˆ˜: {len(df)}ê°œ\")\n",
    "            print(f\"   â€¢ CSV ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ\")\n",
    "            print(f\"   â€¢ íŒŒì¼ í¬ê¸°: {os.path.getsize(csv_path):,} bytes\")\n",
    "\n",
    "            essential_fields = ['ìƒí’ˆëª…', 'ê°€ê²©', 'í‰ì ', 'URL', 'ìˆœìœ„']\n",
    "            print(f\"\\nâœ… í•„ìˆ˜ í•„ë“œ ì™„ì„±ë„:\")\n",
    "            for field in essential_fields:\n",
    "                if field in df.columns:\n",
    "                    if len(df) > 0:\n",
    "                        valid_count = len(df[df[field].notna() & (df[field] != '') & (df[field] != 'ì •ë³´ ì—†ìŒ')])\n",
    "                        completion_rate = (valid_count / len(df)) * 100\n",
    "                        status = \"âœ…\" if completion_rate >= 80 else \"âš ï¸\" if completion_rate >= 50 else \"âŒ\"\n",
    "                        print(f\"   {status} {field}: {completion_rate:.1f}% ({valid_count}/{len(df)})\")\n",
    "                    else:\n",
    "                        print(f\"   - {field}: ë°ì´í„° ì—†ìŒ\")\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"   â„¹ï¸ pandasê°€ ì—†ì–´ ìƒì„¸ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ CSV íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "            df = None\n",
    "            csv_files_found = False\n",
    "    else:\n",
    "        print(f\"   âš ï¸ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì˜ˆìƒ ê²½ë¡œ: {expected_csv_path})\")\n",
    "\n",
    "    # 3. ì´ë¯¸ì§€ íŒŒì¼ í™•ì¸\n",
    "    print(f\"\\nğŸ–¼ï¸ ì´ë¯¸ì§€ íŒŒì¼ í™•ì¸...\")\n",
    "    image_dir = None\n",
    "    images_found = False\n",
    "    expected_img_dir = \"ê²½ë¡œ ë¯¸ì„¤ì •\"  # ë¡œê·¸ ì¶œë ¥ì„ ìœ„í•œ ë³€ìˆ˜\n",
    "\n",
    "    try:\n",
    "        continent, country = get_city_location(CITY_NAME)\n",
    "        is_city_state = CITY_NAME == country\n",
    "\n",
    "        if is_city_state:\n",
    "            image_dir = os.path.join(\"kkday_img\", continent, country)\n",
    "        else:\n",
    "            image_dir = os.path.join(\"kkday_img\", continent, country, CITY_NAME)\n",
    "        expected_img_dir = image_dir\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ ì´ë¯¸ì§€ ê²½ë¡œ ê²°ì • ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "    if image_dir and os.path.exists(image_dir):\n",
    "        image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        images_found = len(image_files) > 0\n",
    "        print(f\"   ğŸ“¸ ì €ì¥ëœ ì´ë¯¸ì§€: {len(image_files)}ê°œ\")\n",
    "        if images_found:\n",
    "            total_size = sum(os.path.getsize(os.path.join(image_dir, f)) for f in image_files)\n",
    "            print(f\"   ğŸ’¾ ì´ í¬ê¸°: {total_size/1024/1024:.2f} MB\")\n",
    "            main_images = [f for f in image_files if '_thumb' not in f]\n",
    "            thumb_images = [f for f in image_files if '_thumb' in f]\n",
    "            print(f\"   ğŸ–¼ï¸ ë©”ì¸ ì´ë¯¸ì§€: {len(main_images)}ê°œ\")\n",
    "            print(f\"   ğŸ” ì¸ë„¤ì¼: {len(thumb_images)}ê°œ\")\n",
    "    else:\n",
    "        print(f\"   ğŸ“¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì—†ìŒ. (ì˜ˆìƒ ê²½ë¡œ: {expected_img_dir})\")\n",
    "\n",
    "    # 4. í¬ë¡¤ë§ í’ˆì§ˆ í‰ê°€\n",
    "    print(f\"\\nğŸ¯ í¬ë¡¤ë§ í’ˆì§ˆ í‰ê°€:\")\n",
    "    quality_score = 0\n",
    "    max_score = 5\n",
    "\n",
    "    if 'crawling_success' in locals() and crawling_success:\n",
    "        quality_score += 1\n",
    "        print(f\"   âœ… í¬ë¡¤ë§ ì‹¤í–‰: ì„±ê³µ\")\n",
    "    else:\n",
    "        print(f\"   âŒ í¬ë¡¤ë§ ì‹¤í–‰: ì‹¤íŒ¨\")\n",
    "\n",
    "    if csv_files_found:\n",
    "        quality_score += 1\n",
    "        print(f\"   âœ… CSV ìƒì„±: ì„±ê³µ\")\n",
    "    else:\n",
    "        print(f\"   âŒ CSV ìƒì„±: ì‹¤íŒ¨\")\n",
    "\n",
    "    if df is not None and not df.empty:\n",
    "        quality_score += 1\n",
    "        print(f\"   âœ… ë°ì´í„° ìˆ˜ì§‘: {len(df)}ê°œ ìƒí’ˆ\")\n",
    "\n",
    "        essential_completion = 0\n",
    "        for field in ['ìƒí’ˆëª…', 'ê°€ê²©', 'í‰ì ']:\n",
    "            if field in df.columns:\n",
    "                if len(df) > 0:\n",
    "                    valid_count = len(df[df[field].notna() & (df[field] != '')])\n",
    "                    if (valid_count / len(df) >= 0.8):\n",
    "                        essential_completion += 1\n",
    "\n",
    "        if essential_completion >= 2:\n",
    "            quality_score += 1\n",
    "            print(f\"   âœ… ë°ì´í„° í’ˆì§ˆ: ìš°ìˆ˜\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸ ë°ì´í„° í’ˆì§ˆ: ë³´í†µ\")\n",
    "    else:\n",
    "        print(f\"   âŒ ë°ì´í„° ìˆ˜ì§‘: ì‹¤íŒ¨\")\n",
    "\n",
    "    if SAVE_IMAGES and images_found:\n",
    "        quality_score += 1\n",
    "        print(f\"   âœ… ì´ë¯¸ì§€ ì €ì¥: ì„±ê³µ\")\n",
    "    elif not SAVE_IMAGES:\n",
    "        quality_score += 1\n",
    "        print(f\"   âœ… ì´ë¯¸ì§€ ì„¤ì •: URLë§Œ ì €ì¥ (ì„¤ì •ëŒ€ë¡œ)\")\n",
    "    else:\n",
    "        print(f\"   âŒ ì´ë¯¸ì§€ ì €ì¥: ì‹¤íŒ¨\")\n",
    "\n",
    "    quality_percentage = (quality_score / max_score) * 100\n",
    "    status_emoji = \"ğŸ‰ ìš°ìˆ˜\" if quality_percentage >= 80 else \"ğŸ‘ ì–‘í˜¸\" if quality_percentage >= 60 else \"âš ï¸ ë³´í†µ\" if quality_percentage >= 40 else \"âŒ ë¯¸í¡\"\n",
    "    print(f\"\\nğŸ† ì¢…í•© í’ˆì§ˆ ì ìˆ˜: {quality_score}/{max_score} ({quality_percentage:.1f}%) {status_emoji}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ê²°ê³¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ğŸ“Š ê²°ê³¼ ë¶„ì„ ì™„ë£Œ\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ ìš”ì•½ =====\n",
    "print(f\"ğŸ‰ KKday í¬ë¡¤ëŸ¬ v1.0 ì‹¤í–‰ ì™„ë£Œ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ìµœì¢… ì‹¤í–‰ ìš”ì•½\n",
    "print(f\"ğŸ“‹ ì‹¤í–‰ ìš”ì•½:\")\n",
    "print(f\"   ğŸ™ï¸ í¬ë¡¤ë§ ë„ì‹œ: {CITY_NAME}\")\n",
    "print(f\"   ğŸ¯ ëª©í‘œ ìƒí’ˆ: {TARGET_PRODUCTS}ê°œ\")\n",
    "print(f\"   ğŸ“„ ìµœëŒ€ í˜ì´ì§€: {MAX_PAGES}ê°œ\")\n",
    "print(f\"   ğŸ“¸ ì´ë¯¸ì§€ ì €ì¥: {'í™œì„±í™”' if SAVE_IMAGES else 'ë¹„í™œì„±í™”'}\")\n",
    "\n",
    "# ì‹¤í–‰ ì‹œê°„\n",
    "if 'start_time' in locals() and 'end_time' in locals():\n",
    "    print(f\"   â±ï¸ ì‹¤í–‰ ì‹œê°„: {end_time - start_time}\")\n",
    "\n",
    "# ì„±ê³µ/ì‹¤íŒ¨ ìƒíƒœ\n",
    "if crawling_success:\n",
    "    print(f\"   âœ… í¬ë¡¤ë§ ìƒíƒœ: ì„±ê³µ\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ í¬ë¡¤ë§ ìƒíƒœ: ë¶€ë¶„ ì„±ê³µ ë˜ëŠ” ì‹¤íŒ¨\")\n",
    "\n",
    "# ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´\n",
    "print(f\"\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(f\"   1ï¸âƒ£ ìˆ˜ì§‘ëœ CSV ë°ì´í„° í™•ì¸ ë° ê²€í† \")\n",
    "print(f\"   2ï¸âƒ£ ì´ë¯¸ì§€ íŒŒì¼ í’ˆì§ˆ í™•ì¸ (ë‹¤ìš´ë¡œë“œëœ ê²½ìš°)\")\n",
    "print(f\"   3ï¸âƒ£ ë°ì´í„° í›„ì²˜ë¦¬ ë° ë¶„ì„\")\n",
    "print(f\"   4ï¸âƒ£ ë‹¤ë¥¸ ë„ì‹œ í¬ë¡¤ë§ (CITY_NAME ë³€ê²½ í›„ ì¬ì‹¤í–‰)\")\n",
    "\n",
    "# íŒŒì¼ ìœ„ì¹˜ ì•ˆë‚´\n",
    "print(f\"\\nğŸ“ ìƒì„±ëœ íŒŒì¼ ìœ„ì¹˜:\")\n",
    "print(f\"   ğŸ“„ CSV: data/{CITY_NAME}/\")\n",
    "print(f\"   ğŸ–¼ï¸ ì´ë¯¸ì§€: images/{CITY_NAME}/\")\n",
    "print(f\"   ğŸ“Š ë­í‚¹: ranking_data/ (í•´ë‹¹í•˜ëŠ” ê²½ìš°)\")\n",
    "\n",
    "# ë¬¸ì œ í•´ê²° ì•ˆë‚´\n",
    "if not crawling_success:\n",
    "    print(f\"\\nğŸ”§ ë¬¸ì œ í•´ê²°:\")\n",
    "    print(f\"   â€¢ ì¸í„°ë„· ì—°ê²° í™•ì¸\")\n",
    "    print(f\"   â€¢ Chrome ë¸Œë¼ìš°ì € ë° ChromeDriver ë²„ì „ í™•ì¸\")\n",
    "    print(f\"   â€¢ ë°©í™”ë²½/ë³´ì•ˆ í”„ë¡œê·¸ë¨ ì„¤ì • í™•ì¸\")\n",
    "    print(f\"   â€¢ ë‹¤ë¥¸ ë„ì‹œëª…ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì‹œë„\")\n",
    "    print(f\"   â€¢ TARGET_PRODUCTSë¥¼ ë” ì ì€ ìˆ˜ë¡œ ì„¤ì •\")\n",
    "\n",
    "print(f\"\\nğŸš€ KKday í¬ë¡¤ëŸ¬ë¥¼ ì´ìš©í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤!\")\n",
    "print(f\"={'*'*70}\")\n",
    "\n",
    "# ê°„ë‹¨í•œ í†µê³„ ì¶œë ¥ (ê°€ëŠ¥í•œ ê²½ìš°)\n",
    "if 'crawler' in locals() and crawler and hasattr(crawler, 'stats'):\n",
    "    stats = crawler.stats\n",
    "    success_count = stats.get('success_count', 0)\n",
    "    total_processed = stats.get('total_processed', 0)\n",
    "    \n",
    "    if total_processed > 0:\n",
    "        print(f\"\\nğŸŠ ìµœì¢… ì„±ê³¼: {success_count}/{total_processed} ìƒí’ˆ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘!\")\n",
    "        success_rate = (success_count / total_processed) * 100\n",
    "        print(f\"ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%\")\n",
    "        \n",
    "        if success_rate >= 80:\n",
    "            print(f\"ğŸ‰ ìš°ìˆ˜í•œ ì„±ê³¼ì…ë‹ˆë‹¤!\")\n",
    "        elif success_rate >= 60:\n",
    "            print(f\"ğŸ‘ ì–‘í˜¸í•œ ê²°ê³¼ì…ë‹ˆë‹¤!\")\n",
    "        else:\n",
    "            print(f\"ğŸ’ª ë‹¤ìŒë²ˆì—” ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ìœ„í•´ ì„¤ì •ì„ ì¡°ì •í•´ë³´ì„¸ìš”!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mikael_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
