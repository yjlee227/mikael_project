"""
ğŸ“Š ë°ì´í„° í†µí•© ê´€ë¦¬ ì‹œìŠ¤í…œ
ranking_data, ranking_urls, url_collected, CSVë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì •ë¦¬
"""

import os
import json
import pandas as pd
import hashlib
from datetime import datetime
from collections import defaultdict
from .config import get_city_code, get_city_info

class DataConsolidator:
    """ë°ì´í„° í†µí•© ê´€ë¦¬ì"""
    
    def __init__(self):
        self.base_dirs = {
            'ranking_data': 'ranking_data',
            'ranking_urls': 'ranking_urls', 
            'url_collected': 'url_collected',
            'data': 'data'
        }
    
    def create_master_data_view(self, city_name):
        """ğŸ¯ í•µì‹¬: ëª¨ë“  ë°ì´í„°ë¥¼ ì—°ê²°í•œ ë§ˆìŠ¤í„° ë·° ìƒì„±"""
        try:
            city_code = get_city_code(city_name)
            
            print(f"ğŸ“Š '{city_name}' ë§ˆìŠ¤í„° ë°ì´í„° ë·° ìƒì„± ì¤‘...")
            
            # 1. ranking_dataì—ì„œ í•µì‹¬ ì •ë³´ ë¡œë“œ
            accumulated_file = f"{self.base_dirs['ranking_data']}/{city_code}_accumulated_rankings.json"
            if not os.path.exists(accumulated_file):
                print(f"âŒ ëˆ„ì  ë­í‚¹ ë°ì´í„° ì—†ìŒ: {accumulated_file}")
                return None
            
            with open(accumulated_file, 'r', encoding='utf-8') as f:
                ranking_data = json.load(f)
            
            # 2. CSV ë°ì´í„° ë¡œë“œ (ìˆë‹¤ë©´)
            csv_data = self._load_csv_data(city_name)
            
            # 3. url_collected ë¡œê·¸ ë¡œë“œ
            collected_log = self._load_collected_log(city_code)
            
            # 4. ë§ˆìŠ¤í„° ë·° ìƒì„±
            master_view = []
            
            for url_hash, url_info in ranking_data['url_rankings'].items():
                url = url_info['url']
                
                # ê¸°ë³¸ ì •ë³´
                row = {
                    'URL': url,
                    'URL_í•´ì‹œ': url_hash,
                    'first_found': url_info.get('first_found'),
                    'is_duplicate': url_info.get('is_duplicate', False),
                    'crawled': url_info.get('crawled', False),
                    'crawled_at': url_info.get('crawled_at'),
                }
                
                # íƒ­ë³„ ìˆœìœ„ ì •ë³´
                tab_rankings = url_info.get('tab_rankings', {})
                for tab_name, ranking_info in tab_rankings.items():
                    row[f'{tab_name}_ìˆœìœ„'] = ranking_info['ranking']
                    row[f'{tab_name}_ë°œê²¬ì‹œê°„'] = ranking_info['found_at']
                
                # CSV ë°ì´í„°ì™€ ë§¤ì¹­
                if csv_data is not None:
                    csv_match = csv_data[csv_data['URL'] == url]
                    if not csv_match.empty:
                        row['CSV_ë²ˆí˜¸'] = csv_match.iloc[0]['ë²ˆí˜¸']
                        row['ìƒí’ˆëª…'] = csv_match.iloc[0]['ìƒí’ˆëª…']
                        row['ê°€ê²©'] = csv_match.iloc[0]['ê°€ê²©_ì •ì œ']
                        row['CSV_íƒ­ë‚´ë­í‚¹'] = csv_match.iloc[0].get('íƒ­ë‚´_ë­í‚¹')
                        row['has_csv_data'] = True
                    else:
                        row['has_csv_data'] = False
                else:
                    row['has_csv_data'] = False
                
                # ìˆ˜ì§‘ ë¡œê·¸ì™€ ë§¤ì¹­
                if url in collected_log:
                    row['collection_timestamp'] = collected_log[url]
                    row['was_collected'] = True
                else:
                    row['was_collected'] = False
                
                master_view.append(row)
            
            # DataFrameìœ¼ë¡œ ë³€í™˜
            df = pd.DataFrame(master_view)
            
            # ì •ë ¬ (ì²« ë°œê²¬ ì‹œê°„ìˆœ)
            df = df.sort_values('first_found')
            
            print(f"âœ… ë§ˆìŠ¤í„° ë·° ìƒì„± ì™„ë£Œ: {len(df)}ê°œ URL")
            return df
            
        except Exception as e:
            print(f"âŒ ë§ˆìŠ¤í„° ë·° ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _load_csv_data(self, city_name):
        """CSV ë°ì´í„° ë¡œë“œ"""
        try:
            continent, country = get_city_info(city_name)
            
            # ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„
            csv_paths = []
            if city_name in ["ë§ˆì¹´ì˜¤", "í™ì½©", "ì‹±ê°€í¬ë¥´"]:
                csv_paths = [
                    f"data/{continent}/{city_name}_klook_products_all.csv",
                    f"data/{continent}/klook_{city_name}_products.csv"
                ]
            else:
                csv_paths = [
                    f"data/{continent}/{country}/{city_name}/{city_name}_klook_products_all.csv",
                    f"data/{continent}/{country}/{city_name}/klook_{city_name}_products.csv",
                    f"data/{continent}/{country}/{country}_klook_products_all.csv"
                ]
            
            for csv_path in csv_paths:
                if os.path.exists(csv_path):
                    return pd.read_csv(csv_path, encoding='utf-8-sig')
            
            print(f"âš ï¸ CSV íŒŒì¼ ì—†ìŒ: {city_name}")
            return None
            
        except Exception as e:
            print(f"âŒ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _load_collected_log(self, city_code):
        """ìˆ˜ì§‘ ë¡œê·¸ ë¡œë“œ"""
        try:
            log_file = f"{self.base_dirs['url_collected']}/{city_code}_url_log.txt"
            if not os.path.exists(log_file):
                return {}
            
            collected_log = {}
            with open(log_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if '|' in line:
                        parts = line.strip().split(' | ')
                        if len(parts) == 2:
                            timestamp, url = parts
                            collected_log[url] = timestamp
            
            return collected_log
            
        except Exception as e:
            print(f"âŒ ìˆ˜ì§‘ ë¡œê·¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def detect_data_inconsistencies(self, city_name):
        """ğŸ” ë°ì´í„° ë¶ˆì¼ì¹˜ ê°ì§€"""
        print(f"ğŸ” '{city_name}' ë°ì´í„° ì •í•©ì„± ê²€ì‚¬...")
        
        master_df = self.create_master_data_view(city_name)
        if master_df is None:
            return
        
        issues = []
        
        # 1. í¬ë¡¤ë§ ì™„ë£Œí–ˆì§€ë§Œ CSV ë°ì´í„° ì—†ìŒ
        crawled_no_csv = master_df[(master_df['crawled'] == True) & (master_df['has_csv_data'] == False)]
        if not crawled_no_csv.empty:
            issues.append({
                'type': 'crawled_but_no_csv',
                'count': len(crawled_no_csv),
                'description': 'í¬ë¡¤ë§ ì™„ë£Œí–ˆì§€ë§Œ CSVì— ì—†ëŠ” URLë“¤'
            })
        
        # 2. CSVì— ìˆì§€ë§Œ ranking_dataì— ì—†ìŒ
        if 'has_csv_data' in master_df.columns:
            csv_no_ranking = master_df[(master_df['has_csv_data'] == True) & (master_df['URL_í•´ì‹œ'].isna())]
            if not csv_no_ranking.empty:
                issues.append({
                    'type': 'csv_but_no_ranking',
                    'count': len(csv_no_ranking), 
                    'description': 'CSVì— ìˆì§€ë§Œ ë­í‚¹ ë°ì´í„°ì— ì—†ëŠ” URLë“¤'
                })
        
        # 3. ìˆ˜ì§‘ ë¡œê·¸ì™€ í¬ë¡¤ë§ ì™„ë£Œ ìƒíƒœ ë¶ˆì¼ì¹˜
        log_crawl_mismatch = master_df[
            (master_df['was_collected'] == True) & (master_df['crawled'] == False)
        ]
        if not log_crawl_mismatch.empty:
            issues.append({
                'type': 'log_crawl_mismatch', 
                'count': len(log_crawl_mismatch),
                'description': 'ìˆ˜ì§‘ ë¡œê·¸ì— ìˆì§€ë§Œ í¬ë¡¤ë§ ì™„ë£Œ í‘œì‹œë˜ì§€ ì•Šì€ URLë“¤'
            })
        
        # ê²°ê³¼ ì¶œë ¥
        if issues:
            print(f"âš ï¸ {len(issues)}ê°œ ë°ì´í„° ë¶ˆì¼ì¹˜ ë°œê²¬:")
            for issue in issues:
                print(f"   - {issue['description']}: {issue['count']}ê°œ")
        else:
            print("âœ… ë°ì´í„° ì •í•©ì„± ì–‘í˜¸")
        
        return issues, master_df
    
    def cleanup_redundant_files(self, city_name):
        """ğŸ§¹ ì¤‘ë³µ íŒŒì¼ ì •ë¦¬"""
        try:
            city_code = get_city_code(city_name)
            
            print(f"ğŸ§¹ '{city_name}' ì¤‘ë³µ íŒŒì¼ ì •ë¦¬ ì¤‘...")
            
            # ranking_urls í´ë”ì˜ ì¤‘ë³µ íŒŒì¼ë“¤ ì •ë¦¬
            ranking_urls_dir = self.base_dirs['ranking_urls']
            if os.path.exists(ranking_urls_dir):
                files = [f for f in os.listdir(ranking_urls_dir) if f.startswith(city_code)]
                
                # íƒ­ë³„ë¡œ ê·¸ë£¹í™”
                tab_files = defaultdict(list)
                for file in files:
                    if '_' in file:
                        parts = file.split('_')
                        if len(parts) >= 3:
                            tab_name = parts[1]
                            tab_files[tab_name].append((file, os.path.getctime(
                                os.path.join(ranking_urls_dir, file)
                            )))
                
                # ê° íƒ­ì—ì„œ ê°€ì¥ ìµœì‹  íŒŒì¼ë§Œ ìœ ì§€
                files_to_remove = []
                for tab_name, file_list in tab_files.items():
                    if len(file_list) > 1:
                        # ìƒì„± ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ìµœì‹ ì´ ë§ˆì§€ë§‰)
                        file_list.sort(key=lambda x: x[1])
                        # ìµœì‹  íŒŒì¼ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì‚­ì œ ëŒ€ìƒ
                        files_to_remove.extend([f[0] for f in file_list[:-1]])
                
                if files_to_remove:
                    print(f"   ğŸ“ ranking_urls: {len(files_to_remove)}ê°œ ì¤‘ë³µ íŒŒì¼ ë°œê²¬")
                    for file in files_to_remove:
                        file_path = os.path.join(ranking_urls_dir, file)
                        backup_path = file_path + '.backup'
                        os.rename(file_path, backup_path)
                        print(f"      ğŸ”„ ë°±ì—…: {file} â†’ {file}.backup")
                else:
                    print("   âœ… ranking_urls: ì¤‘ë³µ íŒŒì¼ ì—†ìŒ")
            
            # ranking_data í´ë”ì˜ ê°œë³„ íŒŒì¼ë“¤ (accumulatedê°€ ìˆìœ¼ë©´ ë¶ˆí•„ìš”)
            ranking_data_dir = self.base_dirs['ranking_data']
            accumulated_file = f"{city_code}_accumulated_rankings.json"
            
            if os.path.exists(os.path.join(ranking_data_dir, accumulated_file)):
                individual_files = [
                    f for f in os.listdir(ranking_data_dir) 
                    if f.startswith(city_code) and f != accumulated_file and f.endswith('.json')
                ]
                
                if individual_files:
                    print(f"   ğŸ“ ranking_data: {len(individual_files)}ê°œ ê°œë³„ íŒŒì¼ ë°œê²¬")
                    for file in individual_files:
                        file_path = os.path.join(ranking_data_dir, file)
                        backup_path = file_path + '.backup'
                        os.rename(file_path, backup_path)
                        print(f"      ğŸ”„ ë°±ì—…: {file} â†’ {file}.backup")
                else:
                    print("   âœ… ranking_data: ë¶ˆí•„ìš”í•œ ê°œë³„ íŒŒì¼ ì—†ìŒ")
            
            print("âœ… ì¤‘ë³µ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def create_unified_city_report(self, city_name):
        """ğŸ“‹ ë„ì‹œë³„ í†µí•© ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            print(f"ğŸ“‹ '{city_name}' í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
            
            # ë§ˆìŠ¤í„° ë·° ìƒì„±
            master_df = self.create_master_data_view(city_name)
            if master_df is None:
                return
            
            # ë°ì´í„° ë¶ˆì¼ì¹˜ ê²€ì‚¬
            issues, _ = self.detect_data_inconsistencies(city_name)
            
            # ë¦¬í¬íŠ¸ ìƒì„±
            report = {
                'city_name': city_name,
                'city_code': get_city_code(city_name),
                'generated_at': datetime.now().isoformat(),
                'summary': {
                    'total_urls': len(master_df),
                    'crawled_urls': len(master_df[master_df['crawled'] == True]),
                    'csv_data_urls': len(master_df[master_df['has_csv_data'] == True]),
                    'collected_log_urls': len(master_df[master_df['was_collected'] == True]),
                    'duplicate_urls': len(master_df[master_df['is_duplicate'] == True])
                },
                'tab_rankings': {},
                'data_issues': issues,
                'file_locations': {
                    'ranking_data': f"ranking_data/{get_city_code(city_name)}_accumulated_rankings.json",
                    'url_collected': f"url_collected/{get_city_code(city_name)}_url_log.txt",
                    'csv_data': "data/[continent]/[country]/[city]/"
                }
            }
            
            # íƒ­ë³„ ìˆœìœ„ í†µê³„
            tab_columns = [col for col in master_df.columns if col.endswith('_ìˆœìœ„')]
            for col in tab_columns:
                tab_name = col.replace('_ìˆœìœ„', '')
                valid_rankings = master_df[col].dropna()
                if not valid_rankings.empty:
                    report['tab_rankings'][tab_name] = {
                        'total_urls': len(valid_rankings),
                        'min_rank': int(valid_rankings.min()),
                        'max_rank': int(valid_rankings.max()),
                        'avg_rank': round(valid_rankings.mean(), 1)
                    }
            
            # ë¦¬í¬íŠ¸ ì €ì¥
            os.makedirs('reports', exist_ok=True)
            report_file = f"reports/{city_name}_data_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            # ë§ˆìŠ¤í„° ë·° CSV ì €ì¥
            master_csv = f"reports/{city_name}_master_view_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            master_df.to_csv(master_csv, index=False, encoding='utf-8-sig')
            
            print(f"âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ:")
            print(f"   ğŸ“Š JSON ë¦¬í¬íŠ¸: {report_file}")
            print(f"   ğŸ“Š ë§ˆìŠ¤í„° ë·°: {master_csv}")
            
            # ìš”ì•½ ì¶œë ¥
            print(f"\nğŸ“ˆ '{city_name}' ë°ì´í„° ìš”ì•½:")
            print(f"   ì „ì²´ URL: {report['summary']['total_urls']}ê°œ")
            print(f"   í¬ë¡¤ë§ ì™„ë£Œ: {report['summary']['crawled_urls']}ê°œ")
            print(f"   CSV ë°ì´í„°: {report['summary']['csv_data_urls']}ê°œ")
            print(f"   ìˆ˜ì§‘ ë¡œê·¸: {report['summary']['collected_log_urls']}ê°œ")
            print(f"   ì¤‘ë³µ URL: {report['summary']['duplicate_urls']}ê°œ")
            
            if report['tab_rankings']:
                print(f"   íƒ­ë³„ ìˆœìœ„:")
                for tab_name, stats in report['tab_rankings'].items():
                    print(f"      {tab_name}: {stats['total_urls']}ê°œ ({stats['min_rank']}-{stats['max_rank']}ìœ„)")
            
            return report_file, master_csv
            
        except Exception as e:
            print(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None, None

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
data_consolidator = DataConsolidator()

print("âœ… ë°ì´í„° í†µí•© ê´€ë¦¬ ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ!")
print("   ğŸ¯ ê¸°ëŠ¥:")
print("   - create_master_data_view(): ëª¨ë“  ë°ì´í„° ì—°ê²°í•œ ë§ˆìŠ¤í„° ë·°")
print("   - detect_data_inconsistencies(): ë°ì´í„° ë¶ˆì¼ì¹˜ ê°ì§€")
print("   - cleanup_redundant_files(): ì¤‘ë³µ íŒŒì¼ ì •ë¦¬")
print("   - create_unified_city_report(): í†µí•© ë¦¬í¬íŠ¸ ìƒì„±")