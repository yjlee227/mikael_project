크롤링 시스템 개발 로드맵 (2-Phase Plan)
이 문서는 안정적이고 확장 가능한 대용량 크롤링 시스템을 구축하기 위한 2단계 개발 계획을 정의합니다.
1단계: 기능 완성 (현재 목표)
목표: 현재의 주피터 노트북 환경 내에서, 중단과 재시작에 완벽히 대응할 수 있는 모든 핵심 기능을 갖춘 크롤링 시스템을 완성한다.
주요 개발 과제
1. 체계적인 폴더 구조 설정
   * 실행: config 폴더를 생성하여 모든 JSON 파일(city_codes.json, crawling_state.json 등)을 이곳에서 관리한다.
   * 위치: 그룹 1의 initialize_file_system 함수 내에서 스크립트 시작 시 자동으로 폴더를 생성하도록 구현한다.
2. 상태 관리 시스템 구축 (세션 연속성)
   * 실행: config/crawling_state.json 파일을 통해 크롤링의 모든 진행 상황을 실시간으로 기록한다.
   * 기록 정보:
      * total_collected_count: 모든 세션을 통틀어 수집한 총 상품 개수
      * last_worked_page: 마지막으로 작업했던 페이지 번호
      * completed_urls: 성공적으로 크롤링된 모든 URL의 목록
   * 동작: 스크립트 시작 시 이 파일을 읽어와 마지막 작업 지점부터 작업을 이어나간다.
3. 페이지 단위 크롤링 로직 구현
   * 실행: 다음 페이지 버튼이 없을 때까지 페이지를 순회하는 메인 루프를 구현한다.
   * URL 필터링: 각 페이지의 URL을 수집한 후, completed_urls 목록과 비교하여 아직 처리되지 않은 URL만을 작업 대상으로 삼아 중복 수집을 방지한다.
4. 페이지 내 랜덤 재시작 로직 구현
   * 실행: 각 페이지마다 7~20개의 상품을 처리한 뒤, 예측 불가능한 시점에 브라우저를 재시작한다.
   * 보장 조건: 페이지 내 상품 수와 관계없이 페이지당 최소 한 번의 재시작이 일어나도록 보장한다. (예: 상품이 5개뿐인 페이지는 5개 처리 후 재시작)
5. 실시간 데이터 연속성 보장
   * 상태 저장: 상품 하나를 성공적으로 크롤링할 때마다 crawling_state.json 파일을 즉시 업데이트하여 진행 상황을 기록한다.
   * 파일 저장:
      * CSV: 새로운 데이터는 기존 파일의 마지막 줄에 이어서 **추가(append)**한다.
      * 이미지: 파일명은 total_collected_count를 기준으로 다음 번호를 부여하여 생성한다 (예: HKT_058.jpg).
1단계 완료 시 기대 결과
* 수백, 수천 개의 상품을 크롤링하던 중 어떤 이유로든 중단되어도, 스크립트를 다시 실행하기만 하면 마지막 작업 지점부터 모든 작업을 정확하게 이어나가는 안정적인 자동화 시스템이 완성된다.
2단계: 코드 리팩토링 (미래 목표)
전제 조건: 1단계에서 개발된 시스템이 완벽하고 안정적으로 구동되는 것을 확인한 후 진행한다.
목표: 코드의 가독성, 재사용성, 유지보수성을 극대화하여 프로젝트를 전문적인 구조로 개선한다.
주요 개발 과제
1. 기능 라이브러리 파일 생성
   * 실행: crawling_utils.py 와 같은 별도의 파이썬 파일을 생성한다.
2. 코드 이전
   * 실행: 주피터 노트북 그룹 1에 정의된 모든 함수(def)와 클래스(class) 정의를 crawling_utils.py 파일로 옮긴다.
3. 노트북 간소화
   * 실행: 그룹 1의 길고 복잡했던 코드들을 모두 삭제하고, from crawling_utils import * 와 같은 한 줄의 코드로 대체하여 모든 기능을 불러온다.
2단계 완료 시 기대 결과
* 메인 노트북은 실행 흐름을 관리하는 지휘관의 역할만 하고, 실제 기능 구현은 라이브러리 파일이 담당하는 명확한 역할 분리가 이루어진다. 이로써 매우 전문적이고 관리하기 쉬운 프로젝트 구조가 완성된다.


















전체 작업 흐름도 (도식화)
┌───────────────────┐
│   스크립트 시작   │
└────────┬──────────┘
        │
        ▼
┌───────────────────┐
│[상태 확인]        │
│config/state.json  │
│파일 존재 여부?    │
└────────┬──────────┘
        ├───────────┐
        │           │
  (파일 있음)      (파일 없음)
        │           │
        ▼           ▼
┌────────┴───────────┐
│   상태 복원 / 초기화  │
└────────┬───────────┘
        │
        ▼
┌───────────────────┐
│[그룹 3: 정찰]     │
│전체 페이지 등 분석│
└────────┬──────────┘
        │
        ▼
┌──────────────────────────────────────────────┐
│ [그룹 4: 실행] 메인 루프 시작 (마지막 페이지부터) │
└────────────────────┬─────────────────────────┘
                    │
        ┌───────────┴────────────┐
        │      페이지 처리 루프     │
        └───────────┬────────────┘
                    │
         │

        ┌───────────┴────────────┐
        │ 1. URL 수집 및 미완료 URL 필터링 │
        │ 2. 상품 처리 루프 시작         │
        │   - 상품 1개 크롤링           │
        │   - CSV/이미지/상태 파일 저장 │
        │   - 재시작 조건 확인/실행     │
        │   - (루프 반복)              │
        └───────────┬────────────┘
                    │
        ┌───────────┴────────────┐
        │     다음 페이지로 이동      │
        └────────────────────────┘
                    │
        ┌───────────┴────────────┐
        │   (목표 달성까지 루프 반복)  │
        └───────────┬────────────┘
                    │
                    ▼
       ┌──────────────────┐
       │   모든 작업 완료   │
       └──────────────────┘