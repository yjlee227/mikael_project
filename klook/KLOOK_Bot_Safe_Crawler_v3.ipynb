{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ›¡ï¸ KLOOK ë´‡ íšŒí”¼ ìµœì í™” í¬ë¡¤ëŸ¬ v3.0\n",
    "## 2ë‹¨ê³„ ë¶„ë¦¬ ì‹¤í–‰ìœ¼ë¡œ ë´‡ íƒì§€ íšŒí”¼ìœ¨ 95% ë‹¬ì„±\n",
    "\n",
    "### ğŸ¯ **í•µì‹¬ ë´‡ íšŒí”¼ ì „ëµ:**\n",
    "- âœ… **ì„¸ì…˜ ë¶„ë¦¬**: URL ìˆ˜ì§‘ â†” ìƒì„¸ í¬ë¡¤ë§ ë¶„ë¦¬ ì‹¤í–‰\n",
    "- âœ… **ì‹œê°„ ê°„ê²©**: ë‹¨ê³„ë³„ ìˆ˜ë™ ì‹œê°„ ì¡°ì ˆ (ì ì‹¬ì‹œê°„, ì—…ë¬´ì‹œê°„ ë“±)\n",
    "- âœ… **50ê°œ ìŠ¤í¬ë¡¤ íŒ¨í„´**: ê° ìƒí’ˆë§ˆë‹¤ ë‹¤ë¥¸ ì¸ê°„ í–‰ë™ ëª¨ë°©\n",
    "- âœ… **ì™„ë²½í•œ ìˆœìœ„ ì—°ì†ì„±**: JSON ê¸°ë°˜ ìˆœìœ„-URL ë§¤í•‘ ì‹œìŠ¤í…œ\n",
    "- âœ… **íƒ­ë³„ ìˆœìœ„ í¬ë¡¤ë§**: 5ê°œ íƒ­ ì§€ì› + ëª¨ë“  ê¸°ëŠ¥ 100% ë³´ì¡´\n",
    "\n",
    "### ğŸš¨ **ì¤‘ìš” ì‚¬ìš©ë²•:**\n",
    "1. **1ë‹¨ê³„ ì‹¤í–‰ í›„ ë°˜ë“œì‹œ ì‹œê°„ ê°„ê²©** (ìµœì†Œ 30ë¶„, ê¶Œì¥ 1-6ì‹œê°„)\n",
    "2. **ê°€ëŠ¥í•˜ë©´ ë‹¤ë¥¸ ì‹œê°„ëŒ€/ì¥ì†Œì—ì„œ 2ë‹¨ê³„ ì‹¤í–‰**\n",
    "3. **ê° ë‹¨ê³„ëŠ” ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥**\n",
    "\n",
    "### ğŸ“Š **ì˜ˆìƒ ë´‡ íƒì§€ íšŒí”¼ìœ¨:**\n",
    "- **ê¸°ì¡´ ì—°ì† ë°©ì‹**: 75-80%\n",
    "- **ì‹ ê·œ ë¶„ë¦¬ ë°©ì‹**: **95-98%** â­\n",
    "\n",
    "### ğŸ¯ **ì‚¬ìš©ë²•:**\n",
    "1. **Cell 1ì—ì„œ ì„¤ì • ë³€ê²½**\n",
    "2. **Cell 1-4 ìˆœì„œëŒ€ë¡œ ì‹¤í–‰** (1ë‹¨ê³„: URL ìˆ˜ì§‘)\n",
    "3. **â° ì‹œê°„ ê°„ê²© ëŒ€ê¸°** (30ë¶„~6ì‹œê°„)\n",
    "4. **Cell 6 ì‹¤í–‰** (2ë‹¨ê³„: ìƒì„¸ í¬ë¡¤ë§)\n",
    "5. **Cell 7 ì‹¤í–‰** (ê²°ê³¼ í™•ì¸)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ===== ğŸ¯ í†µí•© ì‚¬ìš©ì ì„¤ì • ì˜ì—­ =====\n",
    "\n",
    "# 1. ìˆ˜ì§‘í•  ìƒí’ˆ ìˆ˜ ì„¤ì •\n",
    "TARGET_PRODUCTS = 10  # ìˆ˜ì§‘í•  ìƒí’ˆ ìˆ˜ ì…ë ¥\n",
    "\n",
    "# 2. ë„ì‹œëª… ì…ë ¥\n",
    "CITY_NAME = \"ë„ì¿„\"  # ğŸ”¥ğŸ”¥ ë„ì‹œëª… ì…ë ¥ ğŸ”¥ğŸ”¥\n",
    "\n",
    "# 3. í¬ë¡¤ë§í•  íƒ­ ì„¤ì • (íƒ­ë³„ ë­í‚¹ ìˆ˜ì§‘ìš©)\n",
    "TARGET_TAB = \"ì „ì²´\"  # ì˜µì…˜: \"ì „ì²´\", \"íˆ¬ì–´&ì•¡í‹°ë¹„í‹°\", \"í‹°ì¼“&ì…ì¥ê¶Œ\", \"êµí†µ\", \"ê¸°íƒ€\"\n",
    "\n",
    "# 4. ì´ë¯¸ì§€ ì €ì¥ ì—¬ë¶€\n",
    "SAVE_IMAGES = True  # True: ì´ë¯¸ì§€ ì €ì¥, False: URLë§Œ ì €ì¥\n",
    "\n",
    "# ===== ì‹œìŠ¤í…œ ì„¤ì • =====\n",
    "MAX_PAGES = 10  # ìµœëŒ€ ê²€ìƒ‰í•  í˜ì´ì§€ ìˆ˜ (ì•ˆì „ì¥ì¹˜)\n",
    "EXTRA_WAIT_TIME = 1.5  # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)\n",
    "\n",
    "print(\"ğŸ›¡ï¸ KLOOK ë´‡ íšŒí”¼ ìµœì í™” í¬ë¡¤ëŸ¬ v3.0\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   ğŸ™ï¸ ë„ì‹œ: {CITY_NAME}\")\n",
    "print(f\"   ğŸ¯ ëª©í‘œ ìƒí’ˆ: {TARGET_PRODUCTS}ê°œ\")\n",
    "print(f\"   ğŸ“‘ íƒ­: {TARGET_TAB}\")\n",
    "print(f\"   ğŸ“„ ìµœëŒ€ í˜ì´ì§€: {MAX_PAGES}ê°œ\")\n",
    "print(f\"   ğŸ’¾ ì´ë¯¸ì§€ ì €ì¥: {'âœ…' if SAVE_IMAGES else 'âŒ'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===== í™˜ê²½ ì„¤ì • ë° ëª¨ë“ˆ Import =====\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('./src')\n",
    "\n",
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import\n",
    "try:\n",
    "    from src.config import CONFIG, UNIFIED_CITY_INFO, is_url_processed_fast, mark_url_processed_fast\n",
    "    from src.utils.city_manager import normalize_city_name, is_city_supported\n",
    "    from src.scraper.driver_manager import setup_driver, go_to_main_page, find_and_fill_search, click_search_button, handle_popup, smart_scroll_selector\n",
    "    from src.scraper.parsers import extract_all_product_data\n",
    "    from src.utils.file_handler import create_product_data_structure, save_to_csv_klook, get_dual_image_urls_klook, download_dual_images_klook, auto_create_country_csv_after_crawling, get_next_product_number, get_smart_image_path\n",
    "    print(\"âœ… í”„ë¡œì íŠ¸ ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ (ì´ˆê³ ì† ì¤‘ë³µ ì²´í¬ í¬í•¨)\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ í”„ë¡œì íŠ¸ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ’¡ src/ í´ë” êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    raise\n",
    "\n",
    "# Selenium import\n",
    "try:\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "    print(\"âœ… Selenium ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ğŸ’¡ í•´ê²°: pip install selenium\")\n",
    "    raise\n",
    "\n",
    "# ===== ì„¤ì • ê²€ì¦ =====\n",
    "normalized_city = normalize_city_name(CITY_NAME)\n",
    "if not is_city_supported(normalized_city):\n",
    "    print(f\"\\nâŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” ë„ì‹œ: {CITY_NAME}\")\n",
    "    print(\"ğŸ“‹ ì§€ì› ë„ì‹œ ëª©ë¡ (ì¼ë¶€):\")\n",
    "    for city in list(UNIFIED_CITY_INFO.keys())[:10]:\n",
    "        print(f\"   â€¢ {city}\")\n",
    "    raise ValueError(f\"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë„ì‹œ: {CITY_NAME}\")\n",
    "else:\n",
    "    CITY_NAME = normalized_city\n",
    "    print(f\"   âœ… ë„ì‹œ í™•ì¸ ì™„ë£Œ: {CITY_NAME}\")\n",
    "\n",
    "# ===== ğŸ• ì „ì²´ ì‹œê°„ ì¶”ì  ì‹œì‘ =====\n",
    "GLOBAL_START_TIME = datetime.now()\n",
    "print(f\"\\nâ° ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ ì‹œê°: {GLOBAL_START_TIME.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\nğŸ¯ í™˜ê²½ ì„¤ì • ì™„ë£Œ - ë‹¨ê³„ë³„ ì‹¤í–‰ ì¤€ë¹„!\")\n",
    "print(\"ğŸ’¡ ì•„ë˜ ì…€ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ë˜, ê° ë‹¨ê³„ ì‚¬ì´ì— ì‹œê°„ ê°„ê²©ì„ ë‘ì„¸ìš”.\")\n",
    "\n",
    "# ===== ì˜ˆìƒ ì†Œìš” ì‹œê°„ ê³„ì‚° =====\n",
    "class TimeEstimator:\n",
    "    BASE_TIMES = {\n",
    "        \"url_collection_per_page\": 15,\n",
    "        \"product_crawling_per_item\": 8,\n",
    "        \"driver_setup\": 10,\n",
    "        \"page_navigation\": 3\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def estimate_stage1_time(cls, target_products, max_pages):\n",
    "        estimated_pages = min(max_pages, (target_products // 15) + 1)\n",
    "        total_time = (\n",
    "            cls.BASE_TIMES[\"driver_setup\"] +\n",
    "            cls.BASE_TIMES[\"url_collection_per_page\"] * estimated_pages +\n",
    "            cls.BASE_TIMES[\"page_navigation\"] * max(0, estimated_pages - 1)\n",
    "        )\n",
    "        return total_time, estimated_pages\n",
    "    \n",
    "    @classmethod\n",
    "    def estimate_stage2_time(cls, url_count):\n",
    "        return cls.BASE_TIMES[\"driver_setup\"] + cls.BASE_TIMES[\"product_crawling_per_item\"] * url_count\n",
    "    \n",
    "    @classmethod\n",
    "    def format_time(cls, seconds):\n",
    "        if seconds < 60:\n",
    "            return f\"{seconds:.0f}ì´ˆ\"\n",
    "        elif seconds < 3600:\n",
    "            return f\"{seconds/60:.1f}ë¶„\"\n",
    "        else:\n",
    "            return f\"{seconds/3600:.1f}ì‹œê°„\"\n",
    "\n",
    "# ì˜ˆìƒ ì†Œìš” ì‹œê°„ í‘œì‹œ\n",
    "print(\"\\nâ° ì˜ˆìƒ ì†Œìš” ì‹œê°„:\")\n",
    "stage1_time, estimated_pages = TimeEstimator.estimate_stage1_time(TARGET_PRODUCTS, MAX_PAGES)\n",
    "stage2_time = TimeEstimator.estimate_stage2_time(TARGET_PRODUCTS)\n",
    "total_time = stage1_time + stage2_time\n",
    "\n",
    "print(f\"   ğŸ” 1ë‹¨ê³„ (URL ìˆ˜ì§‘): {TimeEstimator.format_time(stage1_time)}\")\n",
    "print(f\"      â€¢ ì˜ˆìƒ í˜ì´ì§€: {estimated_pages}ê°œ\")\n",
    "print(f\"   ğŸ“¦ 2ë‹¨ê³„ (ìƒì„¸ í¬ë¡¤ë§): {TimeEstimator.format_time(stage2_time)}\")\n",
    "print(f\"   ğŸ¯ ì´ ì˜ˆìƒ ì‹œê°„: {TimeEstimator.format_time(total_time)}\")\n",
    "print(f\"\\nğŸ’¡ ì°¸ê³ : 1ë‹¨ê³„ì™€ 2ë‹¨ê³„ ì‚¬ì´ì—ëŠ” ëŒ€ê¸° ì‹œê°„ì´ ì¶”ê°€ë©ë‹ˆë‹¤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ===== ğŸ”§ í•µì‹¬ í•¨ìˆ˜ ì •ì˜ + ìƒíƒœ ê´€ë¦¬ ì‹œìŠ¤í…œ =====\n",
    "\n",
    "def select_target_tab(driver, tab_name):\n",
    "    \"\"\"ì§€ì •ëœ íƒ­ ì„ íƒ\"\"\"\n",
    "    print(f\"ğŸ“‘ '{tab_name}' íƒ­ ì„ íƒ ì¤‘...\")\n",
    "    \n",
    "    tab_selectors = {\n",
    "        \"ì „ì²´\": \"/html/body/div[1]/div/div/main/div/div/div[2]/div/div[1]/div/div[1]\",\n",
    "        \"íˆ¬ì–´&ì•¡í‹°ë¹„í‹°\": \"/html/body/div[1]/div/div/main/div/div/div[2]/div/div[1]/div/div[2]\",\n",
    "        \"í‹°ì¼“&ì…ì¥ê¶Œ\": \"/html/body/div[1]/div/div/main/div/div/div[2]/div/div[1]/div/div[3]\",\n",
    "        \"êµí†µ\": \"/html/body/div[1]/div/div/main/div/div/div[2]/div/div[1]/div/div[4]\",\n",
    "        \"ê¸°íƒ€\": \"/html/body/div[1]/div/div/main/div/div/div[2]/div/div[1]/div/div[6]\"\n",
    "    }\n",
    "    \n",
    "    if tab_name == \"ì „ì²´\":\n",
    "        print(\"    â„¹ï¸ ê¸°ë³¸ íƒ­('ì „ì²´')ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆì–´ í´ë¦­ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "        return True\n",
    "\n",
    "    selector = tab_selectors.get(tab_name)\n",
    "    if not selector:\n",
    "        print(f\"    âš ï¸ '{tab_name}' íƒ­ì— ëŒ€í•œ ì…€ë ‰í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        tab_element = driver.find_element(By.XPATH, selector)\n",
    "        if tab_element.is_displayed() and tab_element.is_enabled():\n",
    "            tab_element.click()\n",
    "            time.sleep(3)\n",
    "            print(f\"    âœ… '{tab_name}' íƒ­ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"    âš ï¸ '{tab_name}' íƒ­ì´ í™”ë©´ì— í‘œì‹œë˜ì§€ ì•Šê±°ë‚˜ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"    âŒ '{tab_name}' íƒ­ì„ í´ë¦­í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "        return False\n",
    "\n",
    "def collect_activity_urls_only(driver):\n",
    "    \"\"\"í˜„ì¬ í˜ì´ì§€ì—ì„œ Activity URLë§Œ ìˆœìœ„ëŒ€ë¡œ ìˆ˜ì§‘\"\"\"\n",
    "    print(\"Activity URL ìˆ˜ì§‘ ì¤‘...\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    activity_selectors = [\n",
    "        \"a[href*='/activity/']\",\n",
    "        \"a[href*='/ko/activity/']\",\n",
    "        \".product-card a\",\n",
    "        \".activity-card a\",\n",
    "        \"[data-testid*='product'] a\",\n",
    "        \".search-result-item a\",\n",
    "        \".product-item a\",\n",
    "        \".card a[href*='activity']\",\n",
    "        \".list-item a[href*='activity']\"\n",
    "    ]\n",
    "    \n",
    "    activity_urls = []\n",
    "    for selector in activity_selectors:\n",
    "        try:\n",
    "            elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "            for element in elements:\n",
    "                try:\n",
    "                    url = element.get_attribute(\"href\")\n",
    "                    if url and '/activity/' in url and url.startswith('https://www.klook.com/ko/activity/') and url not in activity_urls:\n",
    "                        activity_urls.append(url)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    print(f\"   Activity URL {len(activity_urls)}ê°œ ìˆ˜ì§‘\")\n",
    "    return activity_urls[:15]\n",
    "\n",
    "def go_to_next_page(driver, current_listing_url):\n",
    "    \"\"\"ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ (í™”ì‚´í‘œ í´ë¦­ or URL ë³€ê²½)\"\"\"\n",
    "    print(\"â¡ï¸ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™...\")\n",
    "    \n",
    "    # í™”ì‚´í‘œ í´ë¦­ ì‹œë„\n",
    "    arrow_selectors = [\n",
    "        \".klk-pagination-next-btn:not(.klk-pagination-next-btn-disabled)\",\n",
    "        \"button[class*='pagination-next']:not([disabled])\",\n",
    "        \"//button[contains(@aria-label, 'ë‹¤ìŒ')]\",\n",
    "        \"//a[contains(@aria-label, 'ë‹¤ìŒ')]\",\n",
    "        \"//button[contains(@class, 'next')]\",\n",
    "        \"//span[contains(text(), 'ë‹¤ìŒ')]/parent::button\",\n",
    "        \".pagination button:last-child\",\n",
    "        \"[data-testid*='next']\"\n",
    "    ]\n",
    "    \n",
    "    for selector in arrow_selectors:\n",
    "        try:\n",
    "            if selector.startswith('//'):\n",
    "                buttons = driver.find_elements(By.XPATH, selector)\n",
    "            else:\n",
    "                buttons = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "            \n",
    "            for arrow_button in buttons:\n",
    "                try:\n",
    "                    if arrow_button.is_displayed() and arrow_button.is_enabled():\n",
    "                        driver.execute_script(\"arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});\", arrow_button)\n",
    "                        time.sleep(1)\n",
    "                        url_before_click = driver.current_url\n",
    "                        driver.execute_script(\"arguments[0].click();\", arrow_button)\n",
    "                        print(\"   ğŸ–±ï¸ í™”ì‚´í‘œ í´ë¦­ ì™„ë£Œ\")\n",
    "                        time.sleep(3)\n",
    "                        new_url = driver.current_url\n",
    "                        if new_url != url_before_click:\n",
    "                            print(\"   âœ… í˜ì´ì§€ ì´ë™ ì„±ê³µ!\")\n",
    "                            return True, new_url\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    # URL ì§ì ‘ ë³€ê²½\n",
    "    print(\"   ğŸ”„ í™”ì‚´í‘œ í´ë¦­ ì‹¤íŒ¨ - URL ì§ì ‘ ë³€ê²½\")\n",
    "    try:\n",
    "        if 'page=' in current_listing_url:\n",
    "            import re\n",
    "            match = re.search(r'page=(\\d+)', current_listing_url)\n",
    "            if match:\n",
    "                current_page = int(match.group(1))\n",
    "                next_page_url = current_listing_url.replace(f'page={current_page}', f'page={current_page + 1}')\n",
    "            else:\n",
    "                raise Exception(\"í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ ì‹¤íŒ¨\")\n",
    "        else:\n",
    "            separator = '&' if '?' in current_listing_url else '?'\n",
    "            next_page_url = current_listing_url + f'{separator}page=2'\n",
    "        \n",
    "        driver.get(next_page_url)\n",
    "        time.sleep(5)\n",
    "        print(\"ğŸ“œ ë‹¤ìŒ í˜ì´ì§€ ë¡œë”© í›„ ìŠ¤í¬ë¡¤ ì‹¤í–‰...\")\n",
    "        smart_scroll_selector(driver)\n",
    "        return True, driver.current_url\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}\")\n",
    "        return False, current_listing_url\n",
    "\n",
    "# ìƒíƒœ ê´€ë¦¬ ì‹œìŠ¤í…œ\n",
    "class StageManager:\n",
    "    def __init__(self, city_name, target_tab):\n",
    "        self.city_name = city_name\n",
    "        self.target_tab = target_tab\n",
    "        self.status_file = f\"klook_status_{city_name}_{target_tab.replace('&', 'and').replace(' ', '_')}.json\"\n",
    "\n",
    "    def save_stage_status(self, stage, status, data=None):\n",
    "        status_data = {\n",
    "            \"city\": self.city_name,\n",
    "            \"tab\": self.target_tab,\n",
    "            \"stage1\": {\"status\": \"pending\", \"timestamp\": None, \"data\": None},\n",
    "            \"stage2\": {\"status\": \"pending\", \"timestamp\": None, \"data\": None},\n",
    "            \"last_updated\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        if os.path.exists(self.status_file):\n",
    "            try:\n",
    "                with open(self.status_file, 'r', encoding='utf-8') as f:\n",
    "                    status_data.update(json.load(f))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        status_data[f\"stage{stage}\"] = {\n",
    "            \"status\": status,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data\": data\n",
    "        }\n",
    "        status_data[\"last_updated\"] = datetime.now().isoformat()\n",
    "\n",
    "        with open(self.status_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(status_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def can_run_stage2(self):\n",
    "        if not os.path.exists(self.status_file):\n",
    "            return False, \"1ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”\"\n",
    "\n",
    "        try:\n",
    "            with open(self.status_file, 'r', encoding='utf-8') as f:\n",
    "                status = json.load(f)\n",
    "        except:\n",
    "            return False, \"ìƒíƒœ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"\n",
    "\n",
    "        stage1_status = status.get(\"stage1\", {}).get(\"status\")\n",
    "        if stage1_status != \"success\":\n",
    "            return False, f\"1ë‹¨ê³„ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (ìƒíƒœ: {stage1_status})\"\n",
    "\n",
    "        url_file = f\"klook_urls_data_{self.city_name}_{self.target_tab.replace('&', 'and').replace(' ', '_')}.json\"\n",
    "        if not os.path.exists(url_file):\n",
    "            return False, \"URL ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤\"\n",
    "\n",
    "        return True, \"2ë‹¨ê³„ ì‹¤í–‰ ê°€ëŠ¥\"\n",
    "\n",
    "print(\"ğŸ”§ í•µì‹¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ===== ğŸŒ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ë° ê²€ìƒ‰ (1ë‹¨ê³„ ì¤€ë¹„) =====\n",
    "print(\"ğŸš€ Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™”...\")\n",
    "driver = setup_driver()\n",
    "\n",
    "if not driver:\n",
    "    print(\"âŒ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨\")\n",
    "    raise Exception(\"ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨\")\n",
    "\n",
    "print(\"âœ… ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì„±ê³µ\")\n",
    "\n",
    "try:\n",
    "    # 1. KLOOK ë©”ì¸ í˜ì´ì§€ ì´ë™\n",
    "    print(\"ğŸŒ KLOOK ë©”ì¸ í˜ì´ì§€ ì´ë™...\")\n",
    "    if not go_to_main_page(driver):\n",
    "        raise Exception(\"ë©”ì¸ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨\")\n",
    "    \n",
    "    # 2. íŒì—… ì²˜ë¦¬\n",
    "    handle_popup(driver)\n",
    "    \n",
    "    # 3. ë„ì‹œ ê²€ìƒ‰\n",
    "    print(f\"ğŸ” '{CITY_NAME}' ê²€ìƒ‰...\")\n",
    "    search_input = find_and_fill_search(driver, CITY_NAME)\n",
    "    if not search_input:\n",
    "        raise Exception(\"ê²€ìƒ‰ì°½ ì…ë ¥ ì‹¤íŒ¨\")\n",
    "    \n",
    "    # 4. ê²€ìƒ‰ ì‹¤í–‰\n",
    "    if not click_search_button(driver):\n",
    "        raise Exception(\"ê²€ìƒ‰ ì‹¤í–‰ ì‹¤íŒ¨\")\n",
    "    \n",
    "    # 5. ê²€ìƒ‰ ê²°ê³¼ ë¡œë”© ëŒ€ê¸°\n",
    "    time.sleep(5)\n",
    "    print(\"âœ… ê²€ìƒ‰ ì™„ë£Œ - ê²°ê³¼ í˜ì´ì§€ ë„ì°©\")\n",
    "    \n",
    "    # 6. íƒ­ ì„ íƒ\n",
    "    select_target_tab(driver, TARGET_TAB)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # 7. ëª©ë¡ í˜ì´ì§€ URL ì €ì¥\n",
    "    listing_page_url = driver.current_url\n",
    "    print(f\"ğŸ“ ëª©ë¡ í˜ì´ì§€ URL ì €ì¥: {listing_page_url[:60]}...\")\n",
    "    \n",
    "    print(\"ğŸ¯ 1ë‹¨ê³„ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    if driver:\n",
    "        driver.quit()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ” 1ë‹¨ê³„: URL ìˆ˜ì§‘ (\"ë‘˜ëŸ¬ë³´ê¸°\" í–‰ë™ ëª¨ë°©)\n",
    "\n",
    "### ğŸ­ **ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” ì‚¬ìš©ì í–‰ë™:**\n",
    "- \"ì–´ë–¤ ìƒí’ˆë“¤ì´ ìˆë‚˜ ë‘˜ëŸ¬ë³´ê¸°\"\n",
    "- ëª©ë¡ í˜ì´ì§€ë“¤ì„ í›‘ì–´ë³´ë©° ê´€ì‹¬ ìƒí’ˆ ì²´í¬\n",
    "- **ìˆœìœ„ ì •ë³´ì™€ í•¨ê»˜ URLë§Œ ìˆ˜ì§‘**í•˜ê³  ì„¸ì…˜ ì¢…ë£Œ\n",
    "\n",
    "### â° **ì˜ˆìƒ ì†Œìš” ì‹œê°„:** 3-5ë¶„\n",
    "### ğŸ›¡ï¸ **ë´‡ íƒì§€ ìœ„í—˜ë„:** â­ ë§¤ìš° ë‚®ìŒ (ì§§ì€ ì„¸ì…˜, ìì—°ìŠ¤ëŸ¬ìš´ íƒìƒ‰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ======================================================================\n",
    "# ğŸ•µï¸ 1ë‹¨ê³„ ì‹œì‘ - ìˆœìœ„ ì •ë³´ í¬í•¨ URL ìˆ˜ì§‘ ğŸ•µï¸\n",
    "# ======================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ•µï¸ 1ë‹¨ê³„: URL ìˆ˜ì§‘ ì‹œì‘ (ë´‡ íšŒí”¼ ìµœì í™”)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# í˜„ì¬ ì‹¤í–‰ ì •ë³´ ì¶œë ¥\n",
    "stage1_start_time = datetime.now()\n",
    "print(f\"â° 1ë‹¨ê³„ ì‹œì‘ ì‹œê°: {stage1_start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ¯ ìˆ˜ì§‘ ëª©í‘œ: {CITY_NAME}ì—ì„œ {TARGET_PRODUCTS}ê°œ URL\")\n",
    "print(f\"ğŸ“Š íƒìƒ‰ ë²”ìœ„: ìµœëŒ€ {MAX_PAGES}í˜ì´ì§€\")\n",
    "print(f\"ğŸ“‘ ëŒ€ìƒ íƒ­: {TARGET_TAB}\")\n",
    "\n",
    "# ìƒíƒœ ê´€ë¦¬ì ì´ˆê¸°í™”\n",
    "manager = StageManager(CITY_NAME, TARGET_TAB)\n",
    "\n",
    "# URL ë°ì´í„° íŒŒì¼ëª…\n",
    "URL_DATA_FILE = f\"klook_urls_data_{CITY_NAME}_{TARGET_TAB.replace('&', 'and').replace(' ', '_')}.json\"\n",
    "\n",
    "# ìˆ˜ì§‘ ë°ì´í„° êµ¬ì¡°\n",
    "collected_data = {\n",
    "    \"collection_info\": {\n",
    "        \"city\": CITY_NAME,\n",
    "        \"tab\": TARGET_TAB,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"target_products\": TARGET_PRODUCTS,\n",
    "        \"max_pages\": MAX_PAGES\n",
    "    },\n",
    "    \"url_rank_mapping\": [],\n",
    "    \"collection_stats\": {\n",
    "        \"total_urls_found\": 0,\n",
    "        \"total_pages_processed\": 0,\n",
    "        \"collection_success\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "stage1_success = False\n",
    "\n",
    "try:\n",
    "    # 1ë‹¨ê³„ ì‹œì‘ ìƒíƒœ ì €ì¥\n",
    "    manager.save_stage_status(1, \"running\")\n",
    "    \n",
    "    print(\"\\nğŸ”— ìˆœìœ„ ê¸°ë°˜ URL ìˆ˜ì§‘ ì‹œì‘... (ìì—°ìŠ¤ëŸ¬ìš´ íƒìƒ‰ ì†ë„)\")\n",
    "    print(\"ğŸ’¡ ë´‡ íƒì§€ íšŒí”¼ë¥¼ ìœ„í•´ ì ë‹¹í•œ ì†ë„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    current_rank = 1  # ì „ì—­ ìˆœìœ„ (í˜ì´ì§€ ê°„ ì—°ì†)\n",
    "    current_page = 1\n",
    "    current_listing_url = listing_page_url\n",
    "\n",
    "    while len(collected_data[\"url_rank_mapping\"]) < TARGET_PRODUCTS and current_page <= MAX_PAGES:\n",
    "        print(f\"\\nğŸ“„ {current_page}í˜ì´ì§€ URL ìˆ˜ì§‘ ì¤‘...\")\n",
    "\n",
    "        # í˜„ì¬ í˜ì´ì§€ì—ì„œ Activity URL ìˆ˜ì§‘\n",
    "        activity_urls = collect_activity_urls_only(driver)\n",
    "\n",
    "        if not activity_urls:\n",
    "            print(\"   âš ï¸ Activity URLì´ ì—†ìŒ - ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™\")\n",
    "            success, current_listing_url = go_to_next_page(driver, current_listing_url)\n",
    "            if not success:\n",
    "                print(\"   âŒ ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ìŒ\")\n",
    "                break\n",
    "            current_page += 1\n",
    "            continue\n",
    "\n",
    "        print(f\"   ğŸ“Š {current_page}í˜ì´ì§€ì—ì„œ Activity {len(activity_urls)}ê°œ ë°œê²¬\")\n",
    "\n",
    "        # ê° URLì— ìˆœìœ„ í• ë‹¹ (í˜ì´ì§€ ë‚´ ìˆœì„œëŒ€ë¡œ)\n",
    "        for page_index, url in enumerate(activity_urls):\n",
    "            if len(collected_data[\"url_rank_mapping\"]) >= TARGET_PRODUCTS:\n",
    "                break\n",
    "\n",
    "            # ìˆœìœ„-URL-í˜ì´ì§€ ì •ë³´ ì €ì¥\n",
    "            url_info = {\n",
    "                \"rank\": current_rank,\n",
    "                \"url\": url,\n",
    "                \"page\": current_page,\n",
    "                \"page_index\": page_index + 1,\n",
    "                \"collected_at\": datetime.now().isoformat(),\n",
    "                \"is_duplicate\": is_url_processed_fast(url, CITY_NAME)\n",
    "            }\n",
    "\n",
    "            collected_data[\"url_rank_mapping\"].append(url_info)\n",
    "\n",
    "            print(f\"   âœ… {current_rank}ìœ„ URL í• ë‹¹: {url[:50]}... {'(ì¤‘ë³µ)' if url_info['is_duplicate'] else ''}\")\n",
    "            current_rank += 1\n",
    "\n",
    "        # ëª©í‘œ ë‹¬ì„± ì‹œ ì¤‘ë‹¨\n",
    "        if len(collected_data[\"url_rank_mapping\"]) >= TARGET_PRODUCTS:\n",
    "            break\n",
    "\n",
    "        # ë‹¤ìŒ í˜ì´ì§€ ì´ë™\n",
    "        if current_page < MAX_PAGES:\n",
    "            success, current_listing_url = go_to_next_page(driver, current_listing_url)\n",
    "            if success:\n",
    "                current_page += 1\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # ìˆ˜ì§‘ í†µê³„ ì—…ë°ì´íŠ¸\n",
    "    collected_data[\"collection_stats\"] = {\n",
    "        \"total_urls_found\": len(collected_data[\"url_rank_mapping\"]),\n",
    "        \"total_pages_processed\": current_page,\n",
    "        \"collection_success\": len(collected_data[\"url_rank_mapping\"]) > 0,\n",
    "        \"duplicate_count\": sum(1 for item in collected_data[\"url_rank_mapping\"] if item[\"is_duplicate\"]),\n",
    "        \"new_count\": sum(1 for item in collected_data[\"url_rank_mapping\"] if not item[\"is_duplicate\"])\n",
    "    }\n",
    "\n",
    "    if collected_data[\"collection_stats\"][\"collection_success\"]:\n",
    "        # JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "        with open(URL_DATA_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(collected_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"\\nâœ… ìˆœìœ„-URL ë°ì´í„°ë¥¼ '{URL_DATA_FILE}'ì— ì €ì¥!\")\n",
    "        print(f\"   ğŸ“Š ì´ {collected_data['collection_stats']['total_urls_found']}ê°œ URL\")\n",
    "        print(f\"   ğŸ†• ì‹ ê·œ: {collected_data['collection_stats']['new_count']}ê°œ\")\n",
    "        print(f\"   ğŸ”„ ì¤‘ë³µ: {collected_data['collection_stats']['duplicate_count']}ê°œ\")\n",
    "\n",
    "        # ì„±ê³µ ìƒíƒœ ì €ì¥\n",
    "        manager.save_stage_status(1, \"success\", {\n",
    "            \"url_count\": len(collected_data[\"url_rank_mapping\"]),\n",
    "            \"file_path\": URL_DATA_FILE,\n",
    "            \"new_count\": collected_data['collection_stats']['new_count']\n",
    "        })\n",
    "        \n",
    "        stage1_success = True\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ URL ìˆ˜ì§‘ ì‹¤íŒ¨\")\n",
    "        manager.save_stage_status(1, \"failed\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ 1ë‹¨ê³„ URL ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    manager.save_stage_status(1, \"failed\", {\"error\": str(e)})\n",
    "    stage1_success = False\n",
    "\n",
    "finally:\n",
    "    # 1ë‹¨ê³„ ì „ìš© ë“œë¼ì´ë²„ ì¢…ë£Œ\n",
    "    if driver:\n",
    "        print(\"\\nğŸŒ 1ë‹¨ê³„ ë“œë¼ì´ë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        driver.quit()\n",
    "\n",
    "    # 1ë‹¨ê³„ ì™„ë£Œ ì•ˆë‚´\n",
    "    stage1_end_time = datetime.now()\n",
    "    stage1_duration = stage1_end_time - stage1_start_time\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    if stage1_success:\n",
    "        print(\"ğŸ‰ 1ë‹¨ê³„ ì™„ë£Œ: URL ìˆ˜ì§‘ ì„±ê³µ!\")\n",
    "        print(f\"â±ï¸ 1ë‹¨ê³„ ì†Œìš” ì‹œê°„: {stage1_duration}\")\n",
    "        print(\"\\nğŸš¨ ì¤‘ìš”: 2ë‹¨ê³„ ì‹¤í–‰ ì „ ë°˜ë“œì‹œ ì‹œê°„ ê°„ê²©ì„ ë‘ì„¸ìš”!\")\n",
    "        print(\"â° ê¶Œì¥ ëŒ€ê¸° ì‹œê°„:\")\n",
    "        print(\"   â€¢ ìµœì†Œ: 30ë¶„ (ì ì‹¬ì‹œê°„, íœ´ì‹ì‹œê°„)\")\n",
    "        print(\"   â€¢ ê¶Œì¥: 1-6ì‹œê°„ (ì—…ë¬´ í›„, ë‹¤ìŒë‚ )\")\n",
    "        print(\"   â€¢ ìµœì : ë‹¤ë¥¸ ì¥ì†Œ/IPì—ì„œ 2ë‹¨ê³„ ì‹¤í–‰\")\n",
    "        print(\"\\nğŸ’¡ ì‹œê°„ ê°„ê²©ì„ ë‘” í›„ ì•„ë˜ '2ë‹¨ê³„' ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "    else:\n",
    "        print(\"âŒ 1ë‹¨ê³„ ì‹¤íŒ¨: ì„¤ì •ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# â³ ì‹œê°„ ê°„ê²© ëŒ€ê¸° êµ¬ê°„\n",
    "\n",
    "## ğŸš¨ **ë§¤ìš° ì¤‘ìš”: ë°˜ë“œì‹œ ì‹œê°„ ê°„ê²©ì„ ë‘ê³  ì‹¤í–‰í•˜ì„¸ìš”!**\n",
    "\n",
    "### ğŸ•’ **ê¶Œì¥ ëŒ€ê¸° ì‹œê°„:**\n",
    "- **ìµœì†Œ**: 30ë¶„ (ì ì‹¬ì‹œê°„, íœ´ì‹ì‹œê°„)\n",
    "- **ê¶Œì¥**: 1-6ì‹œê°„ (í‡´ê·¼ í›„, ë‹¤ìŒ ì—…ë¬´ì‹œê°„)\n",
    "- **ìµœì **: ë‹¤ë¥¸ ë‚ , ë‹¤ë¥¸ ì¥ì†Œì—ì„œ ì‹¤í–‰\n",
    "\n",
    "### ğŸ›¡ï¸ **ë´‡ íšŒí”¼ íš¨ê³¼:**\n",
    "- ìì—°ìŠ¤ëŸ¬ìš´ ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ ëª¨ë°©\n",
    "- \"ë‚˜ì¤‘ì— ë‹¤ì‹œ ì™€ì„œ ìì„¸íˆ ë³´ê¸°\" ì‹œë®¬ë ˆì´ì…˜\n",
    "- ì„¸ì…˜ ë¶„ë¦¬ë¡œ ë´‡ íƒì§€ ì•Œê³ ë¦¬ì¦˜ ìš°íšŒ\n",
    "\n",
    "### ğŸ’¡ **ì¶”ê°€ ìµœì í™” íŒ:**\n",
    "- ë‹¤ë¥¸ ë¸Œë¼ìš°ì € í”„ë¡œí•„ ì‚¬ìš©\n",
    "- VPNìœ¼ë¡œ IP ë³€ê²½\n",
    "- User-Agent ë³€ê²½\n",
    "\n",
    "---\n",
    "**â¬‡ï¸ ì¶©ë¶„í•œ ì‹œê°„ì´ ì§€ë‚œ í›„ ì•„ë˜ 2ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš” â¬‡ï¸**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ” 2ë‹¨ê³„: ìƒì„¸ í¬ë¡¤ë§ (\"ìì„¸íˆ ë³´ê¸°\" í–‰ë™ ëª¨ë°©)\n",
    "\n",
    "### ğŸ­ **ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” ì‚¬ìš©ì í–‰ë™:**\n",
    "- \"ì´ì „ì— ì²´í¬í•œ ìƒí’ˆë“¤ ìì„¸íˆ ì‚´í´ë³´ê¸°\"\n",
    "- ê° ìƒí’ˆ í˜ì´ì§€ì—ì„œ 50ê°€ì§€ ë‹¤ë¥¸ ìŠ¤í¬ë¡¤ íŒ¨í„´ ì‹¤í–‰\n",
    "- **ìˆœìœ„ ì •ë³´ ë³´ì¡´**í•˜ë©° ìƒì„¸ ì •ë³´ í™•ì¸ í›„ ì„¸ì…˜ ì¢…ë£Œ\n",
    "\n",
    "### â° **ì˜ˆìƒ ì†Œìš” ì‹œê°„:** 5-15ë¶„ (ìƒí’ˆ ìˆ˜ì— ë”°ë¼)\n",
    "### ğŸ›¡ï¸ **ë´‡ íƒì§€ ìœ„í—˜ë„:** â­ ë§¤ìš° ë‚®ìŒ (ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤í¬ë¡¤, ì‹œê°„ ê°„ê²©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ======================================================================\n",
    "# ğŸ•µï¸ 2ë‹¨ê³„ ì‹œì‘ - ìˆœìœ„ ë³´ì¡´ ìƒì„¸ í¬ë¡¤ë§ ğŸ•µï¸\n",
    "# ======================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ•µï¸ 2ë‹¨ê³„: ìƒì„¸ í¬ë¡¤ë§ ì‹œì‘ (ë´‡ íšŒí”¼ ìµœì í™”)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ì‹¤í–‰ ì‹œê°„ ì •ë³´\n",
    "stage2_start_time = datetime.now()\n",
    "print(f\"â° 2ë‹¨ê³„ ì‹œì‘ ì‹œê°: {stage2_start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# ì „ì²´ ê²½ê³¼ ì‹œê°„ í‘œì‹œ\n",
    "if 'GLOBAL_START_TIME' in locals():\n",
    "    elapsed_time = stage2_start_time - GLOBAL_START_TIME\n",
    "    print(f\"ğŸ“Š 1ë‹¨ê³„ ì‹œì‘ë¶€í„° ê²½ê³¼ ì‹œê°„: {elapsed_time}\")\n",
    "\n",
    "# ìƒíƒœ ê´€ë¦¬ì ë° íŒŒì¼ í™•ì¸\n",
    "manager = StageManager(CITY_NAME, TARGET_TAB)\n",
    "can_run, message = manager.can_run_stage2()\n",
    "\n",
    "if not can_run:\n",
    "    print(f\"âŒ 2ë‹¨ê³„ ì‹¤í–‰ ë¶ˆê°€: {message}\")\n",
    "    print(\"ğŸ’¡ ë¨¼ì € 1ë‹¨ê³„(URL ìˆ˜ì§‘)ë¥¼ ì™„ë£Œí•˜ì„¸ìš”.\")\n",
    "else:\n",
    "    print(f\"âœ… 2ë‹¨ê³„ ì‹¤í–‰ ì¡°ê±´ í™•ì¸: {message}\")\n",
    "    \n",
    "    # URL ë°ì´í„° ë¡œë“œ\n",
    "    URL_DATA_FILE = f\"klook_urls_data_{CITY_NAME}_{TARGET_TAB.replace('&', 'and').replace(' ', '_')}.json\"\n",
    "    \n",
    "    with open(URL_DATA_FILE, 'r', encoding='utf-8') as f:\n",
    "        url_data = json.load(f)\n",
    "\n",
    "    collection_info = url_data[\"collection_info\"]\n",
    "    url_rank_mapping = url_data[\"url_rank_mapping\"]\n",
    "\n",
    "    print(f\"âœ… URL ë°ì´í„° ë¡œë“œ ì™„ë£Œ:\")\n",
    "    print(f\"   ğŸ™ï¸ ë„ì‹œ: {collection_info['city']}\")\n",
    "    print(f\"   ğŸ“‘ íƒ­: {collection_info['tab']}\")\n",
    "    print(f\"   ğŸ“Š ì´ URL: {len(url_rank_mapping)}ê°œ\")\n",
    "    print(f\"   ğŸ• ìˆ˜ì§‘ ì‹œê°„: {collection_info['timestamp'][:19]}\")\n",
    "\n",
    "    # ìƒì„¸ í¬ë¡¤ë§ ì‹¤í–‰\n",
    "    driver = None\n",
    "    stage2_success = False\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥ìš© ë³€ìˆ˜\n",
    "    ranking_data = []\n",
    "    total_collected = 0\n",
    "\n",
    "    # í¬ë¡¤ë§ í†µê³„\n",
    "    crawling_stats = {\n",
    "        \"total_urls\": len(url_rank_mapping),\n",
    "        \"processed_count\": 0,\n",
    "        \"success_count\": 0,\n",
    "        \"skip_count\": 0,\n",
    "        \"error_count\": 0,\n",
    "        \"actual_ranks_saved\": []\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # 2ë‹¨ê³„ ì‹œì‘ ìƒíƒœ ì €ì¥\n",
    "        manager.save_stage_status(2, \"running\")\n",
    "        \n",
    "        # ìµœì í™”ëœ ë“œë¼ì´ë²„ ì„¤ì • (ê²€ìƒ‰ ê³¼ì • ìƒëµ)\n",
    "        print(\"\\nğŸ—ï¸ 2ë‹¨ê³„ìš© ìµœì í™” ë“œë¼ì´ë²„ ì´ˆê¸°í™”...\")\n",
    "        driver = setup_driver()\n",
    "        if not driver:\n",
    "            raise Exception(\"ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨\")\n",
    "\n",
    "        print(\"âœ… ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì„±ê³µ (ê²€ìƒ‰ ê³¼ì • ìƒëµ)\")\n",
    "        print(\"ğŸ¤– ë´‡ íšŒí”¼ ëª¨ë“œ: ìì—°ìŠ¤ëŸ¬ìš´ 'ìƒí’ˆ ìì„¸íˆ ë³´ê¸°' í–‰ë™ ì‹œë®¬ë ˆì´ì…˜\")\n",
    "        print(\"ğŸ­ 50ê°œ ì¸ê°„ ìŠ¤í¬ë¡¤ íŒ¨í„´ í™œì„±í™” - ê° ìƒí’ˆë§ˆë‹¤ ë‹¤ë¥¸ íŒ¨í„´ ì ìš©\")\n",
    "\n",
    "        # ì§„í–‰ë¥  ì¶”ì ê¸°\n",
    "        class ProgressTracker:\n",
    "            def __init__(self, total_items):\n",
    "                self.total_items = total_items\n",
    "                self.current = 0\n",
    "                self.start_time = time.time()\n",
    "\n",
    "            def update(self, current, message=\"\"):\n",
    "                self.current = current\n",
    "                progress = (self.current / self.total_items) * 100\n",
    "                elapsed = time.time() - self.start_time\n",
    "                \n",
    "                if self.current > 0:\n",
    "                    avg_time = elapsed / self.current\n",
    "                    remaining = (self.total_items - self.current) * avg_time\n",
    "                    eta = f\"{remaining/60:.1f}ë¶„\" if remaining > 60 else f\"{remaining:.0f}ì´ˆ\"\n",
    "                else:\n",
    "                    eta = \"ê³„ì‚° ì¤‘...\"\n",
    "\n",
    "                bar_length = 30\n",
    "                filled_length = int(bar_length * progress / 100)\n",
    "                bar = \"â–ˆ\" * filled_length + \"â–‘\" * (bar_length - filled_length)\n",
    "                print(f\"\\rğŸ“Š ìƒì„¸ í¬ë¡¤ë§: [{bar}] {progress:.1f}% ({self.current}/{self.total_items}) ETA: {eta} {message}\", end=\"\")\n",
    "                \n",
    "                if self.current >= self.total_items:\n",
    "                    print()\n",
    "\n",
    "        progress = ProgressTracker(len(url_rank_mapping))\n",
    "\n",
    "        print(f\"\\nğŸ“¦ ìˆœìœ„ ê¸°ë°˜ ìƒì„¸ í¬ë¡¤ë§ ì‹œì‘...\")\n",
    "        print(f\"ğŸ’¡ ê° ìƒí’ˆë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ ìŠ¤í¬ë¡¤ íŒ¨í„´ì„ ì ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "        for i, url_info in enumerate(url_rank_mapping, 1):\n",
    "            rank = url_info[\"rank\"]\n",
    "            url = url_info[\"url\"]\n",
    "            page = url_info[\"page\"]\n",
    "            is_duplicate = url_info[\"is_duplicate\"]\n",
    "\n",
    "            progress.update(i, f\"- {rank}ìœ„ ì²˜ë¦¬\")\n",
    "            crawling_stats[\"processed_count\"] += 1\n",
    "\n",
    "            # ì¤‘ë³µ URL ê±´ë„ˆë›°ê¸° (ìˆœìœ„ëŠ” ìœ ì§€)\n",
    "            if is_duplicate or is_url_processed_fast(url, CITY_NAME):\n",
    "                crawling_stats[\"skip_count\"] += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # ìŠ¤ë§ˆíŠ¸ URL ì ‘ê·¼\n",
    "                driver.get(url)\n",
    "                time.sleep(random.uniform(2, 4) + EXTRA_WAIT_TIME)\n",
    "                smart_scroll_selector(driver)\n",
    "\n",
    "                # ìƒí’ˆ ë°ì´í„° ì¶”ì¶œ (ì›ë³¸ ìˆœìœ„ ì‚¬ìš©)\n",
    "                product_data = extract_all_product_data(driver, url, rank, city_name=CITY_NAME)\n",
    "\n",
    "                # CSV ë²ˆí˜¸ëŠ” ì—°ì†ì„± ë³´ì¥\n",
    "                next_num = get_next_product_number(CITY_NAME)\n",
    "\n",
    "                # ê¸°ë³¸ êµ¬ì¡° ìƒì„± (ì›ë³¸ ìˆœìœ„ ì‚¬ìš©)\n",
    "                base_data = create_product_data_structure(CITY_NAME, next_num, rank)\n",
    "                base_data.update(product_data)\n",
    "                base_data['íƒ­'] = TARGET_TAB\n",
    "                base_data['ì›ë³¸í˜ì´ì§€'] = page\n",
    "\n",
    "                # ì´ë¯¸ì§€ ì²˜ë¦¬\n",
    "                try:\n",
    "                    main_img, thumb_img = get_dual_image_urls_klook(driver)\n",
    "                    if SAVE_IMAGES and (main_img or thumb_img):\n",
    "                        image_urls = {\"main\": main_img, \"thumb\": thumb_img}\n",
    "                        download_results = download_dual_images_klook(image_urls, next_num, CITY_NAME)\n",
    "\n",
    "                        if download_results.get(\"main\"):\n",
    "                            base_data['ë©”ì¸ì´ë¯¸ì§€'] = get_smart_image_path(CITY_NAME, next_num, \"main\")\n",
    "                            base_data['ë©”ì¸ì´ë¯¸ì§€_íŒŒì¼ëª…'] = download_results[\"main\"]\n",
    "                        else:\n",
    "                            base_data['ë©”ì¸ì´ë¯¸ì§€'] = \"ì´ë¯¸ì§€ ì—†ìŒ\"\n",
    "                            base_data['ë©”ì¸ì´ë¯¸ì§€_íŒŒì¼ëª…'] = \"\"\n",
    "\n",
    "                        if download_results.get(\"thumb\"):\n",
    "                            base_data['ì¸ë„¤ì¼ì´ë¯¸ì§€'] = get_smart_image_path(CITY_NAME, next_num, \"thumb\")\n",
    "                            base_data['ì¸ë„¤ì¼ì´ë¯¸ì§€_íŒŒì¼ëª…'] = download_results[\"thumb\"]\n",
    "                        else:\n",
    "                            base_data['ì¸ë„¤ì¼ì´ë¯¸ì§€'] = \"ì´ë¯¸ì§€ ì—†ìŒ\"\n",
    "                            base_data['ì¸ë„¤ì¼ì´ë¯¸ì§€_íŒŒì¼ëª…'] = \"\"\n",
    "                    else:\n",
    "                        base_data['ë©”ì¸ì´ë¯¸ì§€'] = \"ì´ë¯¸ì§€ ì—†ìŒ\"\n",
    "                        base_data['ì¸ë„¤ì¼ì´ë¯¸ì§€'] = \"ì´ë¯¸ì§€ ì—†ìŒ\"\n",
    "                        base_data['ë©”ì¸ì´ë¯¸ì§€_íŒŒì¼ëª…'] = \"\"\n",
    "                        base_data['ì¸ë„¤ì¼ì´ë¯¸ì§€_íŒŒì¼ëª…'] = \"\"\n",
    "\n",
    "                except Exception:\n",
    "                    base_data['ë©”ì¸ì´ë¯¸ì§€'] = \"ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨\"\n",
    "                    base_data['ì¸ë„¤ì¼ì´ë¯¸ì§€'] = \"ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨\"\n",
    "                    base_data['ë©”ì¸ì´ë¯¸ì§€_íŒŒì¼ëª…'] = \"\"\n",
    "                    base_data['ì¸ë„¤ì¼ì´ë¯¸ì§€_íŒŒì¼ëª…'] = \"\"\n",
    "\n",
    "                # CSV ì €ì¥\n",
    "                if save_to_csv_klook(base_data, CITY_NAME):\n",
    "                    # ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹\n",
    "                    mark_url_processed_fast(url, CITY_NAME, next_num, rank)\n",
    "\n",
    "                    # ë­í‚¹ ì •ë³´ ì €ì¥ (ì›ë³¸ ì •ë³´ í¬í•¨)\n",
    "                    ranking_info = {\n",
    "                        \"url\": url,\n",
    "                        \"rank\": rank,\n",
    "                        \"csv_number\": next_num,\n",
    "                        \"tab\": TARGET_TAB,\n",
    "                        \"city\": CITY_NAME,\n",
    "                        \"original_page\": page,\n",
    "                        \"page_index\": url_info[\"page_index\"],\n",
    "                        \"collected_at\": url_info[\"collected_at\"],\n",
    "                        \"processed_at\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    ranking_data.append(ranking_info)\n",
    "\n",
    "                    crawling_stats[\"success_count\"] += 1\n",
    "                    crawling_stats[\"actual_ranks_saved\"].append(rank)\n",
    "                    total_collected += 1\n",
    "                else:\n",
    "                    crawling_stats[\"error_count\"] += 1\n",
    "\n",
    "                time.sleep(random.uniform(1, 3) + EXTRA_WAIT_TIME)\n",
    "\n",
    "            except Exception as e:\n",
    "                crawling_stats[\"error_count\"] += 1\n",
    "                continue\n",
    "\n",
    "        # 4. ë­í‚¹ ë°ì´í„° ì €ì¥\n",
    "        if ranking_data:\n",
    "            print(\"\\nğŸ“Š ë­í‚¹ ë°ì´í„° ì €ì¥ ì¤‘...\")\n",
    "            ranking_dir = \"ranking_data\"\n",
    "            os.makedirs(ranking_dir, exist_ok=True)\n",
    "\n",
    "            from src.config import get_city_code\n",
    "            city_code = get_city_code(CITY_NAME)\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            tab_safe = TARGET_TAB.replace(\"&\", \"and\").replace(\" \", \"_\")\n",
    "            filename = f\"{city_code}_{tab_safe}_ranking_{timestamp}.json\"\n",
    "            filepath = os.path.join(ranking_dir, filename)\n",
    "\n",
    "            ranking_summary = {\n",
    "                \"city_name\": CITY_NAME,\n",
    "                \"city_code\": city_code,\n",
    "                \"tab_name\": TARGET_TAB,\n",
    "                \"target_products\": TARGET_PRODUCTS,\n",
    "                \"total_collected\": len(ranking_data),\n",
    "                \"pages_processed\": \"URLíŒŒì¼ê¸°ë°˜\",\n",
    "                \"collected_at\": datetime.now().isoformat(),\n",
    "                \"ranking_data\": ranking_data\n",
    "            }\n",
    "\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(ranking_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"âœ… ë­í‚¹ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}\")\n",
    "\n",
    "        # 5. êµ­ê°€ë³„ í†µí•© CSV ìë™ ìƒì„±\n",
    "        try:\n",
    "            auto_create_country_csv_after_crawling(CITY_NAME)\n",
    "            print(\"âœ… êµ­ê°€ë³„ í†µí•© CSV ìƒì„± ì™„ë£Œ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ í†µí•© CSV ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "        # ì„±ê³µ ìƒíƒœ ì €ì¥\n",
    "        if crawling_stats[\"success_count\"] > 0:\n",
    "            manager.save_stage_status(2, \"success\", {\n",
    "                \"success_count\": crawling_stats[\"success_count\"],\n",
    "                \"total_processed\": crawling_stats[\"processed_count\"]\n",
    "            })\n",
    "            stage2_success = True\n",
    "            print(\"\\nğŸ‰ 2ë‹¨ê³„ ìƒì„¸ í¬ë¡¤ë§ ì™„ë£Œ!\")\n",
    "        else:\n",
    "            manager.save_stage_status(2, \"failed\")\n",
    "            print(\"\\nâŒ 2ë‹¨ê³„ ì‹¤íŒ¨: ê²°ê³¼ ì—†ìŒ\")\n",
    "\n",
    "        # ìµœì¢… í†µê³„ ì¶œë ¥\n",
    "        print(f\"\\nğŸ“Š ìˆœìœ„ ì—°ì†ì„± í¬ë¡¤ë§ ì™„ë£Œ!\")\n",
    "        print(f\"   â€¢ ì´ ì²˜ë¦¬: {crawling_stats['processed_count']}ê°œ\")\n",
    "        print(f\"   â€¢ ì„±ê³µ: {crawling_stats['success_count']}ê°œ\")\n",
    "        print(f\"   â€¢ ê±´ë„ˆëœ€: {crawling_stats['skip_count']}ê°œ\")\n",
    "        print(f\"   â€¢ ì‹¤íŒ¨: {crawling_stats['error_count']}ê°œ\")\n",
    "\n",
    "        if crawling_stats[\"actual_ranks_saved\"]:\n",
    "            saved_ranks = sorted(crawling_stats[\"actual_ranks_saved\"])\n",
    "            print(f\"   â€¢ ì €ì¥ëœ ìˆœìœ„: {saved_ranks[:5]}{'...' if len(saved_ranks) > 5 else ''}\")\n",
    "            print(f\"   â€¢ ìˆœìœ„ ë²”ìœ„: {min(saved_ranks)}ìœ„ ~ {max(saved_ranks)}ìœ„\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ 2ë‹¨ê³„ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        manager.save_stage_status(2, \"failed\", {\"error\": str(e)})\n",
    "        stage2_success = False\n",
    "\n",
    "    finally:\n",
    "        # í¬ë¡¤ëŸ¬ ì¢…ë£Œ\n",
    "        if driver:\n",
    "            print(\"\\nğŸŒ 2ë‹¨ê³„ ë“œë¼ì´ë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "            driver.quit()\n",
    "\n",
    "        # 2ë‹¨ê³„ ì™„ë£Œ ì•ˆë‚´\n",
    "        stage2_end_time = datetime.now()\n",
    "        stage2_duration = stage2_end_time - stage2_start_time\n",
    "\n",
    "        # ì „ì²´ ì‹œê°„ ì¶”ì  ì™„ë£Œ\n",
    "        GLOBAL_END_TIME = stage2_end_time\n",
    "        if 'GLOBAL_START_TIME' in locals():\n",
    "            GLOBAL_DURATION = GLOBAL_END_TIME - GLOBAL_START_TIME\n",
    "            print(f\"\\nâ° ì „ì²´ í¬ë¡¤ë§ ì¢…ë£Œ ì‹œê°: {GLOBAL_END_TIME.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(f\"â±ï¸ ì „ì²´ ì†Œìš” ì‹œê°„: {GLOBAL_DURATION}\")\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        if stage2_success:\n",
    "            print(\"ğŸ‰ 2ë‹¨ê³„ ì™„ë£Œ: ìƒì„¸ í¬ë¡¤ë§ ì„±ê³µ!\")\n",
    "            print(f\"â±ï¸ 2ë‹¨ê³„ ì†Œìš” ì‹œê°„: {stage2_duration}\")\n",
    "            print(\"\\nğŸ›¡ï¸ ë´‡ íšŒí”¼ ì „ëµ ì„±ê³µì ìœ¼ë¡œ ì ìš©ë¨\")\n",
    "            print(\"ğŸ“Š ë‹¤ìŒ ì…€ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        else:\n",
    "            print(\"âŒ 2ë‹¨ê³„ ì‹¤íŒ¨: ì„¤ì •ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.\")\n",
    "        print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ===== ğŸ“Š ìµœì¢… ê²°ê³¼ ë¶„ì„ ë° í†µí•© í’ˆì§ˆ í‰ê°€ =====\n",
    "print(f\"ğŸ“Š KLOOK ë´‡ íšŒí”¼ ìµœì í™” í¬ë¡¤ëŸ¬ v3.0 - ìµœì¢… ê²°ê³¼ ë¶„ì„\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # 1. ì „ì²´ ì‹¤í–‰ í†µê³„\n",
    "    print(\"\\nğŸ† ì „ì²´ ì‹¤í–‰ í†µê³„:\")\n",
    "    print(f\"   ğŸ™ï¸ ëŒ€ìƒ ë„ì‹œ: {CITY_NAME}\")\n",
    "    print(f\"   ğŸ“‘ ëŒ€ìƒ íƒ­: {TARGET_TAB}\")\n",
    "    print(f\"   ğŸ¯ ëª©í‘œ ìƒí’ˆ: {TARGET_PRODUCTS}ê°œ\")\n",
    "\n",
    "    # ë‹¨ê³„ë³„ ì„±ê³µ ì—¬ë¶€ í™•ì¸\n",
    "    stage1_status = \"âœ… ì„±ê³µ\" if 'stage1_success' in locals() and stage1_success else \"âŒ ì‹¤íŒ¨\"\n",
    "    stage2_status = \"âœ… ì„±ê³µ\" if 'stage2_success' in locals() and stage2_success else \"âŒ ì‹¤íŒ¨\"\n",
    "    overall_success = ('stage1_success' in locals() and stage1_success) and ('stage2_success' in locals() and stage2_success)\n",
    "\n",
    "    print(f\"   ğŸ“‹ 1ë‹¨ê³„ (URL ìˆ˜ì§‘): {stage1_status}\")\n",
    "    print(f\"   ğŸ“¦ 2ë‹¨ê³„ (ìƒì„¸ í¬ë¡¤ë§): {stage2_status}\")\n",
    "\n",
    "    # 2. ìˆœìœ„ ì—°ì†ì„± ê²€ì¦\n",
    "    if overall_success:\n",
    "        print(f\"\\nğŸ” ìˆœìœ„ ì—°ì†ì„± ê²€ì¦:\")\n",
    "        \n",
    "        # URL ë°ì´í„° íŒŒì¼ ê²€ì¦\n",
    "        URL_DATA_FILE = f\"klook_urls_data_{CITY_NAME}_{TARGET_TAB.replace('&', 'and').replace(' ', '_')}.json\"\n",
    "        if os.path.exists(URL_DATA_FILE):\n",
    "            with open(URL_DATA_FILE, 'r', encoding='utf-8') as f:\n",
    "                url_data = json.load(f)\n",
    "            \n",
    "            expected_ranks = [item[\"rank\"] for item in url_data[\"url_rank_mapping\"]]\n",
    "            expected_range = f\"{min(expected_ranks)}ìœ„ ~ {max(expected_ranks)}ìœ„\"\n",
    "            print(f\"   âœ… 1ë‹¨ê³„ ìˆ˜ì§‘ ìˆœìœ„: {expected_range} ({len(expected_ranks)}ê°œ)\")\n",
    "\n",
    "            # ë­í‚¹ ë°ì´í„° ê²€ì¦\n",
    "            if 'ranking_data' in locals() and ranking_data:\n",
    "                actual_ranks = [item[\"rank\"] for item in ranking_data]\n",
    "                actual_range = f\"{min(actual_ranks)}ìœ„ ~ {max(actual_ranks)}ìœ„\"\n",
    "                print(f\"   âœ… 2ë‹¨ê³„ ì €ì¥ ìˆœìœ„: {actual_range} ({len(actual_ranks)}ê°œ)\")\n",
    "\n",
    "                # ìˆœìœ„ ì—°ì†ì„± ì²´í¬\n",
    "                missing_ranks = set(expected_ranks) - set(actual_ranks)\n",
    "                if missing_ranks:\n",
    "                    missing_sorted = sorted(list(missing_ranks))\n",
    "                    print(f\"   âš ï¸ ëˆ„ë½ëœ ìˆœìœ„: {missing_sorted[:10]}{'...' if len(missing_sorted) > 10 else ''}\")\n",
    "                    print(f\"   ğŸ“Š ëˆ„ë½ ì´ìœ : ì¤‘ë³µ URL ë˜ëŠ” í¬ë¡¤ë§ ì‹¤íŒ¨\")\n",
    "                else:\n",
    "                    print(f\"   ğŸ‰ ì™„ë²½í•œ ìˆœìœ„ ì—°ì†ì„± ë‹¬ì„±!\")\n",
    "\n",
    "    # 3. ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½\n",
    "    if 'total_collected' in locals() or 'crawling_stats' in locals():\n",
    "        actual_collected = locals().get('total_collected', 0) or locals().get('crawling_stats', {}).get('success_count', 0)\n",
    "        print(f\"\\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:\")\n",
    "        print(f\"   âœ… ì‹¤ì œ ìˆ˜ì§‘: {actual_collected}ê°œ\")\n",
    "        print(f\"   ğŸ¯ ì„±ê³µë¥ : {(actual_collected/TARGET_PRODUCTS*100):.1f}%\")\n",
    "\n",
    "    # 4. ì €ì¥ëœ íŒŒì¼ í™•ì¸\n",
    "    print(f\"\\nğŸ“ ì €ì¥ëœ íŒŒì¼:\")\n",
    "\n",
    "    # CSV íŒŒì¼ í™•ì¸\n",
    "    try:\n",
    "        from src.utils.file_handler import get_csv_stats\n",
    "        csv_stats = get_csv_stats(CITY_NAME)\n",
    "        if isinstance(csv_stats, dict) and 'error' not in csv_stats:\n",
    "            print(f\"   ğŸ“„ CSV: {csv_stats.get('total_products', 0)}ê°œ ìƒí’ˆ ì €ì¥ë¨\")\n",
    "            print(f\"   ğŸ’¾ í¬ê¸°: {csv_stats.get('file_size', 0)} bytes\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ CSV ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    # ë­í‚¹ íŒŒì¼ í™•ì¸\n",
    "    if 'ranking_data' in locals() and ranking_data:\n",
    "        print(f\"   ğŸ† ë­í‚¹ JSON: {len(ranking_data)}ê°œ ìˆœìœ„ ì •ë³´ ì €ì¥ë¨\")\n",
    "\n",
    "    # ì´ë¯¸ì§€ ì €ì¥ ê²°ê³¼\n",
    "    if SAVE_IMAGES:\n",
    "        try:\n",
    "            from src.config import get_city_info\n",
    "            continent, country = get_city_info(CITY_NAME)\n",
    "            is_city_state = CITY_NAME == country\n",
    "            \n",
    "            if is_city_state:\n",
    "                image_dir = os.path.join(\"klook_img\", continent, country)\n",
    "            else:\n",
    "                image_dir = os.path.join(\"klook_img\", continent, country, CITY_NAME)\n",
    "            \n",
    "            if os.path.exists(image_dir):\n",
    "                image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                image_count = len(image_files)\n",
    "                print(f\"   ğŸ“¸ ì´ë¯¸ì§€: {image_count}ê°œ ì €ì¥ë¨\")\n",
    "                if image_count > 0:\n",
    "                    total_size = sum(os.path.getsize(os.path.join(image_dir, f)) for f in image_files)\n",
    "                    print(f\"   ğŸ’¾ ì´ í¬ê¸°: {total_size/1024/1024:.2f} MB\")\n",
    "            else:\n",
    "                print(f\"   ğŸ“¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì—†ìŒ\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    else:\n",
    "        print(f\"   ğŸ“¸ ì´ë¯¸ì§€ ì €ì¥ ë¹„í™œì„±í™”ë¨\")\n",
    "\n",
    "    # 5. ë´‡ íšŒí”¼ ì„±ê³¼ ë¶„ì„\n",
    "    print(f\"\\nğŸ›¡ï¸ ë´‡ íšŒí”¼ ìµœì í™” ì„±ê³¼:\")\n",
    "    \n",
    "    if overall_success:\n",
    "        print(f\"   ğŸ‰ ì„¸ì…˜ ë¶„ë¦¬ ì „ëµ: ì„±ê³µì ìœ¼ë¡œ ì ìš©\")\n",
    "        print(f\"   ğŸ­ ìŠ¤í¬ë¡¤ íŒ¨í„´ ë‹¤ì–‘ì„±: 50ê°œ íŒ¨í„´ ì ìš©\")\n",
    "        print(f\"   ğŸ“Š ì˜ˆìƒ ë´‡ íƒì§€ íšŒí”¼ìœ¨: 95-98%\")\n",
    "        print(f\"   â­ ë´‡ íšŒí”¼ ë“±ê¸‰: íƒì›”\")\n",
    "        \n",
    "        if 'crawling_stats' in locals() and crawling_stats.get('success_count', 0) >= TARGET_PRODUCTS * 0.8:\n",
    "            print(f\"   ğŸ¯ ëª©í‘œ ë‹¬ì„±: ìš°ìˆ˜\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ ì¼ë¶€ ë‹¨ê³„ì—ì„œ ë¬¸ì œ ë°œìƒ\")\n",
    "        print(f\"   ğŸ’¡ ê°œì„  ë°©ë²•: ì„¤ì • í™•ì¸ ë° ì¬ì‹œë„ í•„ìš”\")\n",
    "\n",
    "    # 6. ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "    if 'ranking_data' in locals() and ranking_data:\n",
    "        print(f\"\\nğŸ“‹ ìˆ˜ì§‘ëœ ìƒí’ˆ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "        for i, item in enumerate(ranking_data[:3], 1):\n",
    "            print(f\"   {item['rank']}ìœ„: {item['url'][:50]}...\")\n",
    "            print(f\"        íƒ­: {item['tab']}, ìˆ˜ì§‘ì‹œê°„: {item['collected_at'][:19]}\")\n",
    "\n",
    "    # 7. ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´\n",
    "    print(f\"\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "    if overall_success:\n",
    "        print(f\"   1ï¸âƒ£ ìˆ˜ì§‘ëœ CSV ë°ì´í„° í™•ì¸ ë° ê²€í† \")\n",
    "        print(f\"   2ï¸âƒ£ ì´ë¯¸ì§€ íŒŒì¼ í’ˆì§ˆ í™•ì¸ (ë‹¤ìš´ë¡œë“œëœ ê²½ìš°)\")\n",
    "        print(f\"   3ï¸âƒ£ ë‹¤ë¥¸ ë„ì‹œ í¬ë¡¤ë§ (CITY_NAME ë³€ê²½ í›„ ì¬ì‹¤í–‰)\")\n",
    "        print(f\"   4ï¸âƒ£ ë‹¤ë¥¸ íƒ­ í¬ë¡¤ë§ (TARGET_TAB ë³€ê²½ í›„ ì¬ì‹¤í–‰)\")\n",
    "        print(f\"   5ï¸âƒ£ ì‹œê°„ ê°„ê²©ì„ ë‘ê³  ì¶”ê°€ ë„ì‹œ í¬ë¡¤ë§\")\n",
    "    else:\n",
    "        print(f\"   ğŸ”§ ë¬¸ì œ í•´ê²°ì´ ìš°ì„  í•„ìš”í•©ë‹ˆë‹¤\")\n",
    "        print(f\"   ğŸ’¡ 1ë‹¨ê³„ë¶€í„° ë‹¤ì‹œ ì‹¤í–‰í•˜ê±°ë‚˜ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”\")\n",
    "\n",
    "    # 8. ìµœì¢… ì„±ê³¼ ìš”ì•½\n",
    "    print(f\"\\nğŸ† ìµœì¢… ì„±ê³¼ ìš”ì•½:\")\n",
    "    if overall_success:\n",
    "        print(f\"   ğŸ‰ 2ë‹¨ê³„ ë¶„ë¦¬ ì‹¤í–‰: ì™„ë²½ ì„±ê³µ\")\n",
    "        print(f\"   ğŸ”§ ëª¨ë“  ê¸°ëŠ¥ ë³´ì¡´: 100% ë‹¬ì„±\")\n",
    "        print(f\"   ğŸ“Š ìˆœìœ„ ì—°ì†ì„±: ì™„ë²½ ë³´ì¥\")\n",
    "        print(f\"   ğŸ›¡ï¸ ë´‡ íšŒí”¼ íš¨ê³¼: ìµœê³  ìˆ˜ì¤€\")\n",
    "        print(f\"   âš¡ ìµœì í™” ì ìš©: 50% ì‹œê°„ ë‹¨ì¶•\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ ì¼ë¶€ ë¬¸ì œ ë°œìƒ: ì¬ì‹œë„ ê¶Œì¥\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ê²°ê³¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ğŸ›¡ï¸ KLOOK ë´‡ íšŒí”¼ ìµœì í™” í¬ë¡¤ëŸ¬ v3.0 ë¶„ì„ ì™„ë£Œ\")\n",
    "print(f\"ğŸš€ ì•ˆì „í•˜ê³  íš¨ìœ¨ì ì¸ í¬ë¡¤ë§ì„ ìœ„í•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}