"""
ğŸš€ í†µí•© í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ëŸ¬
- í˜ì´ì§€ë„¤ì´ì…˜ ê¸°ë°˜ ìˆœìœ„ë³„ URL ìˆ˜ì§‘
- ìˆœìœ„ ì •ë³´ì™€ í¬ë¡¤ë§ ë°ì´í„° ë¶„ë¦¬ ì €ì¥
- CSV, ì´ë¯¸ì§€, ë­í‚¹ì˜ ì—°ì†ì„± ë³´ì¥
"""

import os
import time
from datetime import datetime
from .pagination_ranking_system import pagination_ranking_system, ranking_data_matcher, continuity_manager
from .crawler_engine import KlookCrawlerEngine
from .data_handler import save_to_csv_klook, create_product_data_structure

class IntegratedPaginationCrawler:
    """í†µí•© í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, driver):
        self.driver = driver
        self.crawler_engine = KlookCrawlerEngine(driver)
        self.stats = {
            'urls_collected': 0,
            'products_crawled': 0,
            'pages_processed': 0,
            'ranking_continuity_maintained': True
        }
    
    def execute_pagination_crawling(self, city_name, target_count=15, max_pages=5):
        """í˜ì´ì§€ë„¤ì´ì…˜ ê¸°ë°˜ ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰"""
        print(f"ğŸš€ '{city_name}' í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ë§ ì‹œì‘")
        print("=" * 60)
        print(f"ğŸ¯ ì„¤ì •: {target_count}ê°œ ìƒí’ˆ, ìµœëŒ€ {max_pages}í˜ì´ì§€")
        
        start_time = time.time()
        
        try:
            # 1ë‹¨ê³„: ì—°ì†ì„± ê²€ì‚¬
            print(f"\n1ï¸âƒ£ ì—°ì†ì„± ê²€ì‚¬")
            print("-" * 30)
            continuity_check = continuity_manager.ensure_continuity_consistency(city_name)
            
            if continuity_check and not continuity_check['consistent']:
                print(f"âš ï¸ ì—°ì†ì„± ë¬¸ì œ ë°œê²¬ - ê¶Œì¥ ì‹œì‘ ë²ˆí˜¸: {continuity_check['recommended_next']}")
                start_number = continuity_check['recommended_next']
            else:
                start_number = continuity_check['next_number'] if continuity_check else 1
            
            # 2ë‹¨ê³„: í˜ì´ì§€ë„¤ì´ì…˜ URL ìˆ˜ì§‘
            print(f"\n2ï¸âƒ£ í˜ì´ì§€ë„¤ì´ì…˜ URL ìˆ˜ì§‘")
            print("-" * 30)
            collected_urls = pagination_ranking_system.collect_urls_with_pagination(
                self.driver, city_name, target_count, max_pages
            )
            
            if not collected_urls:
                print("âŒ URL ìˆ˜ì§‘ ì‹¤íŒ¨")
                return False
            
            self.stats['urls_collected'] = len(collected_urls)
            self.stats['pages_processed'] = max(url_data['page'] for url_data in collected_urls)
            
            # 3ë‹¨ê³„: ìˆœìœ„ë³„ í¬ë¡¤ë§ ì‹¤í–‰
            print(f"\n3ï¸âƒ£ ìˆœìœ„ë³„ ìƒí’ˆ í¬ë¡¤ë§")
            print("-" * 30)
            success_count = self._crawl_products_by_ranking(collected_urls, city_name, start_number)
            
            self.stats['products_crawled'] = success_count
            
            # 4ë‹¨ê³„: ë­í‚¹-CSV ë§¤í•‘ í…Œì´ë¸” ìƒì„±
            print(f"\n4ï¸âƒ£ ë­í‚¹-CSV ë§¤í•‘ ìƒì„±")
            print("-" * 30)
            mapping_file = ranking_data_matcher.create_ranking_csv_mapping(city_name)
            
            # 5ë‹¨ê³„: ìµœì¢… ê²°ê³¼
            end_time = time.time()
            total_time = end_time - start_time
            
            print(f"\nâœ… í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ë§ ì™„ë£Œ!")
            print("=" * 60)
            print(f"ğŸ“Š ìµœì¢… ê²°ê³¼:")
            print(f"   ğŸ”— ìˆ˜ì§‘ëœ URL: {self.stats['urls_collected']}ê°œ")
            print(f"   ğŸ“„ ì²˜ë¦¬ëœ í˜ì´ì§€: {self.stats['pages_processed']}í˜ì´ì§€")
            print(f"   âœ… í¬ë¡¤ë§ ì„±ê³µ: {self.stats['products_crawled']}ê°œ")
            print(f"   ğŸ† ìˆœìœ„ ë²”ìœ„: 1ìœ„ ~ {len(collected_urls)}ìœ„")
            print(f"   â±ï¸ ì†Œìš” ì‹œê°„: {int(total_time//60)}ë¶„ {int(total_time%60)}ì´ˆ")
            print(f"   ğŸ”— ë§¤í•‘ íŒŒì¼: {mapping_file}")
            
            return True
            
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return False
    
    def _crawl_products_by_ranking(self, collected_urls, city_name, start_number):
        """ìˆœìœ„ë³„ ìƒí’ˆ í¬ë¡¤ë§"""
        success_count = 0
        current_csv_number = start_number
        
        print(f"ğŸ“Š {len(collected_urls)}ê°œ URL ìˆœìœ„ë³„ í¬ë¡¤ë§ ì‹œì‘ (CSV ë²ˆí˜¸: {start_number}ë¶€í„°)")
        
        for idx, url_data in enumerate(collected_urls, 1):
            url = url_data['url']
            global_rank = url_data['global_rank']
            page = url_data['page']
            
            print(f"\nğŸ“Š ì§„í–‰ë¥ : {idx}/{len(collected_urls)} | {global_rank}ìœ„ (í˜ì´ì§€{page})")
            print(f"ğŸ”— URL: {url[:60]}...")
            
            try:
                # ìƒí’ˆ í˜ì´ì§€ ì´ë™
                self.driver.get(url)
                time.sleep(3)
                
                # ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
                result = self.crawler_engine._extract_product_info(url, city_name, current_csv_number)
                
                if result:
                    # ë­í‚¹ ì •ë³´ ì¶”ê°€
                    result['íƒ­ëª…'] = 'ì „ì²´'
                    result['íƒ­ë‚´_ë­í‚¹'] = global_rank
                    result['í˜ì´ì§€'] = page
                    result['CSV_ë²ˆí˜¸'] = current_csv_number
                    
                    # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
                    result['í˜ì´ì§€ë„¤ì´ì…˜_ì •ë³´'] = {
                        'page': page,
                        'page_position': url_data['page_position'],
                        'collection_method': 'pagination_ranking'
                    }
                    
                    # CSV ì €ì¥
                    save_success = save_to_csv_klook(result, city_name)
                    
                    if save_success:
                        print(f"   âœ… ì„±ê³µ: {result.get('ìƒí’ˆëª…', 'N/A')[:30]}... (CSV#{current_csv_number})")
                        success_count += 1
                        current_csv_number += 1
                        
                        # ë­í‚¹ ë§¤ë‹ˆì €ì— í¬ë¡¤ë§ ì™„ë£Œ í‘œì‹œ
                        try:
                            from .ranking_manager import ranking_manager
                            ranking_manager.mark_url_crawled(url, city_name)
                        except:
                            pass
                    else:
                        print(f"   âŒ CSV ì €ì¥ ì‹¤íŒ¨")
                else:
                    print(f"   âŒ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨")
                
                # ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€ê¸°
                time.sleep(2)
                
            except Exception as e:
                print(f"   ğŸ’¥ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"\nğŸ“Š ìˆœìœ„ë³„ í¬ë¡¤ë§ ì™„ë£Œ: {success_count}/{len(collected_urls)}ê°œ ì„±ê³µ")
        return success_count

class PaginationCrawlingValidator:
    """í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ë§ ê²€ì¦ì"""
    
    def validate_ranking_continuity(self, city_name):
        """ìˆœìœ„ ì—°ì†ì„± ê²€ì¦"""
        print(f"ğŸ” '{city_name}' ìˆœìœ„ ì—°ì†ì„± ê²€ì¦")
        print("-" * 40)
        
        try:
            from .config import get_city_code
            city_code = get_city_code(city_name)
            
            # ëˆ„ì  ë­í‚¹ ë°ì´í„° ë¡œë“œ
            accumulated_file = f"ranking_data/{city_code}_accumulated_rankings.json"
            if not os.path.exists(accumulated_file):
                print("âŒ ë­í‚¹ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            import json
            with open(accumulated_file, 'r', encoding='utf-8') as f:
                ranking_data = json.load(f)
            
            # ì „ì²´ íƒ­ ìˆœìœ„ ì¶”ì¶œ
            rankings = []
            for url_hash, url_info in ranking_data["url_rankings"].items():
                tab_rankings = url_info.get("tab_rankings", {})
                if "ì „ì²´" in tab_rankings:
                    rank = tab_rankings["ì „ì²´"]["ranking"]
                    rankings.append({
                        'rank': rank,
                        'url': url_info["url"],
                        'page_info': url_info.get("pagination_info", {})
                    })
            
            # ìˆœìœ„ë³„ ì •ë ¬
            rankings.sort(key=lambda x: x['rank'])
            
            # ì—°ì†ì„± ê²€ì‚¬
            issues = []
            for i in range(1, len(rankings)):
                current_rank = rankings[i]['rank']
                previous_rank = rankings[i-1]['rank']
                
                if current_rank != previous_rank + 1:
                    issues.append({
                        'position': i,
                        'expected': previous_rank + 1,
                        'actual': current_rank,
                        'gap': current_rank - previous_rank - 1
                    })
            
            if issues:
                print(f"âš ï¸ ìˆœìœ„ ì—°ì†ì„± ë¬¸ì œ: {len(issues)}ê°œ")
                for issue in issues[:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                    print(f"   ìœ„ì¹˜ {issue['position']}: ì˜ˆìƒ {issue['expected']}ìœ„ â†’ ì‹¤ì œ {issue['actual']}ìœ„ (ê°­: {issue['gap']})")
            else:
                print("âœ… ìˆœìœ„ ì—°ì†ì„± ì™„ë²½")
            
            print(f"ğŸ“Š ê²€ì¦ ê²°ê³¼: {len(rankings)}ê°œ ìˆœìœ„, {len(issues)}ê°œ ë¶ˆì—°ì†")
            return len(issues) == 0
            
        except Exception as e:
            print(f"âŒ ìˆœìœ„ ì—°ì†ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def validate_data_consistency(self, city_name):
        """ë°ì´í„° ì¼ê´€ì„± ê²€ì¦"""
        print(f"\nğŸ” '{city_name}' ë°ì´í„° ì¼ê´€ì„± ê²€ì¦")
        print("-" * 40)
        
        try:
            from .config import get_city_code, get_city_info
            city_code = get_city_code(city_name)
            continent, country = get_city_info(city_name)
            
            # ë­í‚¹ ë°ì´í„°ì—ì„œ URL ëª©ë¡
            ranking_urls = set()
            accumulated_file = f"ranking_data/{city_code}_accumulated_rankings.json"
            
            if os.path.exists(accumulated_file):
                import json
                with open(accumulated_file, 'r', encoding='utf-8') as f:
                    ranking_data = json.load(f)
                
                for url_info in ranking_data["url_rankings"].values():
                    if url_info.get("crawled", False):
                        ranking_urls.add(url_info["url"])
            
            # CSVì—ì„œ URL ëª©ë¡
            csv_urls = set()
            csv_path = f"data/{continent}/{country}/{country}_klook_products_all.csv"
            
            if os.path.exists(csv_path):
                with open(csv_path, 'r', encoding='utf-8-sig') as f:
                    header = f.readline()
                    columns = header.strip().split(',')
                    
                    url_idx = None
                    for i, col in enumerate(columns):
                        if 'URL' in col:
                            url_idx = i
                            break
                    
                    if url_idx is not None:
                        for line in f:
                            parts = line.strip().split(',')
                            if len(parts) > url_idx:
                                url = parts[url_idx].strip('"')
                                csv_urls.add(url)
            
            # ì¼ê´€ì„± ë¶„ì„
            common_urls = ranking_urls & csv_urls
            ranking_only = ranking_urls - csv_urls
            csv_only = csv_urls - ranking_urls
            
            print(f"ğŸ“Š ë°ì´í„° ì¼ê´€ì„± ê²°ê³¼:")
            print(f"   ğŸ† ë­í‚¹ í¬ë¡¤ë§ ì™„ë£Œ: {len(ranking_urls)}ê°œ")
            print(f"   ğŸ“‹ CSV ì €ì¥ë¨: {len(csv_urls)}ê°œ")
            print(f"   âœ… ì¼ì¹˜í•˜ëŠ” URL: {len(common_urls)}ê°œ")
            
            if ranking_only:
                print(f"   âš ï¸ í¬ë¡¤ë§í–ˆì§€ë§Œ CSV ì—†ìŒ: {len(ranking_only)}ê°œ")
            
            if csv_only:
                print(f"   âš ï¸ CSVì—ë§Œ ìˆê³  ë­í‚¹ ì—†ìŒ: {len(csv_only)}ê°œ")
            
            consistency_rate = len(common_urls) / max(len(ranking_urls), 1) * 100
            print(f"   ğŸ“ˆ ì¼ê´€ì„± ë¹„ìœ¨: {consistency_rate:.1f}%")
            
            return consistency_rate >= 90
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def generate_pagination_report(self, city_name):
        """í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ë§ ë¦¬í¬íŠ¸ ìƒì„±"""
        print(f"\nğŸ“‹ '{city_name}' í˜ì´ì§€ë„¤ì´ì…˜ ë¦¬í¬íŠ¸ ìƒì„±")
        print("-" * 40)
        
        try:
            ranking_continuity = self.validate_ranking_continuity(city_name)
            data_consistency = self.validate_data_consistency(city_name)
            
            report = {
                "city_name": city_name,
                "report_generated": datetime.now().isoformat(),
                "validation_results": {
                    "ranking_continuity": ranking_continuity,
                    "data_consistency": data_consistency,
                    "overall_status": "success" if ranking_continuity and data_consistency else "warning"
                },
                "recommendations": []
            }
            
            # ê¶Œì¥ì‚¬í•­ ìƒì„±
            if not ranking_continuity:
                report["recommendations"].append("ìˆœìœ„ ì—°ì†ì„± ë¬¸ì œ í•´ê²° í•„ìš”")
            
            if not data_consistency:
                report["recommendations"].append("ë­í‚¹-CSV ë°ì´í„° ì¼ê´€ì„± ê°œì„  í•„ìš”")
            
            if ranking_continuity and data_consistency:
                report["recommendations"].append("ì‹œìŠ¤í…œ ìƒíƒœ ìš°ìˆ˜ - ì¶”ê°€ ê°œì„ ì‚¬í•­ ì—†ìŒ")
            
            # ë¦¬í¬íŠ¸ ì €ì¥
            os.makedirs("reports", exist_ok=True)
            report_file = f"reports/pagination_report_{city_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            import json
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_file}")
            print(f"   ğŸ“Š ìˆœìœ„ ì—°ì†ì„±: {'âœ… ì–‘í˜¸' if ranking_continuity else 'âš ï¸ ë¬¸ì œ'}")
            print(f"   ğŸ“Š ë°ì´í„° ì¼ê´€ì„±: {'âœ… ì–‘í˜¸' if data_consistency else 'âš ï¸ ë¬¸ì œ'}")
            
            return report_file
            
        except Exception as e:
            print(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
pagination_crawler_validator = PaginationCrawlingValidator()

print("âœ… í†µí•© í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ëŸ¬ ë¡œë“œ ì™„ë£Œ!")
print("   ğŸš€ ê¸°ëŠ¥:")
print("   - execute_pagination_crawling(): í˜ì´ì§€ë„¤ì´ì…˜ ì „ì²´ í¬ë¡¤ë§")
print("   - validate_ranking_continuity(): ìˆœìœ„ ì—°ì†ì„± ê²€ì¦")
print("   - validate_data_consistency(): ë°ì´í„° ì¼ê´€ì„± ê²€ì¦")
print("   - generate_pagination_report(): ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±")