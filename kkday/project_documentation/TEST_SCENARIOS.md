# KKday í¬ë¡¤ë§ ì‹œìŠ¤í…œ - í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ë° ê²€ì¦ ê°€ì´ë“œ

## ðŸ§ª í…ŒìŠ¤íŠ¸ ê°œìš”

ì´ ë¬¸ì„œëŠ” Klookì—ì„œ KKdayë¡œ ì‹œìŠ¤í…œ ì „í™˜ ê³¼ì •ì—ì„œ ìˆ˜í–‰í•´ì•¼ í•  ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ê° í…ŒìŠ¤íŠ¸ëŠ” êµ¬ì²´ì ì¸ ì‹¤í–‰ ë°©ë²•ê³¼ ì„±ê³µ ê¸°ì¤€ì„ í¬í•¨í•©ë‹ˆë‹¤.

## ðŸŽ¯ í…ŒìŠ¤íŠ¸ ëª©í‘œ

### ì£¼ìš” ê²€ì¦ í•­ëª©
- **ê¸°ëŠ¥ ì™„ì •ì„±**: ëª¨ë“  í¬ë¡¤ë§ ê¸°ëŠ¥ì´ KKdayì—ì„œ ì •ìƒ ìž‘ë™
- **ë°ì´í„° ì •í™•ì„±**: ì¶”ì¶œë˜ëŠ” ë°ì´í„°ì˜ í’ˆì§ˆ ë° ì¼ê´€ì„± 
- **ì„±ëŠ¥ ìœ ì§€**: ê¸°ì¡´ Klook ì‹œìŠ¤í…œ ëŒ€ë¹„ ì„±ëŠ¥ ì €í•˜ ì—†ìŒ
- **ì•ˆì •ì„± í™•ë³´**: ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì•ˆì •ì ì¸ ë™ìž‘

## ðŸ“‹ í…ŒìŠ¤íŠ¸ ë¶„ë¥˜ ì²´ê³„

### Level 1: ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (Unit Tests)
ê° ëª¨ë“ˆë³„ ê°œë³„ ê¸°ëŠ¥ ê²€ì¦

### Level 2: í†µí•© í…ŒìŠ¤íŠ¸ (Integration Tests)  
ëª¨ë“ˆ ê°„ ì—°ë™ ë° ì „ì²´ ì›Œí¬í”Œë¡œìš° ê²€ì¦

### Level 3: ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ (End-to-End Tests)
ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ ì¢…í•© ê²€ì¦

### Level 4: ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (Performance Tests)
ëŒ€ìš©ëŸ‰ ë°ì´í„° ë° ìž¥ì‹œê°„ ì‹¤í–‰ ì•ˆì •ì„± ê²€ì¦

## ðŸ”¬ Level 1: ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

### 1.1 config.py ëª¨ë“ˆ í…ŒìŠ¤íŠ¸

#### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: ë„ì‹œ ì •ë³´ ì¡°íšŒ
```python
def test_config_city_lookup():
    """ë„ì‹œ ì½”ë“œ ë° ì •ë³´ ì¡°íšŒ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    # Given: í…ŒìŠ¤íŠ¸ ë„ì‹œëª…ë“¤
    test_cities = ["ì„œìš¸", "ë„ì¿„", "ë°©ì½•", "íŒŒë¦¬"]
    
    # When & Then: ê° ë„ì‹œë³„ ê²€ì¦
    for city in test_cities:
        city_info = get_city_info(city)
        
        assert city_info is not None, f"{city} ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"
        assert 'city_code' in city_info, f"{city} ë„ì‹œ ì½”ë“œ ëˆ„ë½"
        assert 'country' in city_info, f"{city} êµ­ê°€ ì •ë³´ ëˆ„ë½"
        assert len(city_info['city_code']) == 3, f"{city} ë„ì‹œ ì½”ë“œ í˜•ì‹ ì˜¤ë¥˜"
        
        print(f"âœ… {city}: {city_info['city_code']} ({city_info['country']})")

# ì˜ˆìƒ ê²°ê³¼:
# âœ… ì„œìš¸: SEL (ëŒ€í•œë¯¼êµ­)
# âœ… ë„ì¿„: TYO (ì¼ë³¸)
# âœ… ë°©ì½•: BKK (íƒœêµ­)
# âœ… íŒŒë¦¬: PAR (í”„ëž‘ìŠ¤)
```

#### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: KKday ì„¤ì • ê²€ì¦
```python
def test_kkday_configuration():
    """KKday ì „ìš© ì„¤ì •ê°’ ê²€ì¦"""
    
    from src.config import CONFIG
    
    # í”Œëž«í¼ ì •ë³´ ê²€ì¦
    assert CONFIG['platform'] == 'KKday'
    assert CONFIG['base_url'] == 'https://www.kkday.com'
    assert CONFIG['data_source'] == 'KKday'
    
    # Klook ê´€ë ¨ ì„¤ì • ì œê±° í™•ì¸
    config_str = str(CONFIG)
    assert 'klook' not in config_str.lower(), "Klook ê´€ë ¨ ì„¤ì • ìž”ì¡´"
    assert 'KLOOK' not in config_str, "KLOOK ëŒ€ë¬¸ìž ìž”ì¡´"
    
    print("âœ… KKday ì„¤ì • ê²€ì¦ ì™„ë£Œ")
```

### 1.2 utils/ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸

#### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: city_manager ë³„ì¹­ ì²˜ë¦¬
```python
def test_city_alias_mapping():
    """ë„ì‹œëª… ë³„ì¹­ ë§¤í•‘ í…ŒìŠ¤íŠ¸"""
    
    from src.utils.city_manager import normalize_city_name
    
    # ë³„ì¹­ ë§¤í•‘ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        ("í† ì¿„", "ë„ì¿„"),
        ("ë¶ê²½", "ë² ì´ì§•"),  
        ("ì²­ë„", "ì¹­ë”°ì˜¤"),
        ("KL", "ì¿ ì•Œë¼ë£¸í‘¸ë¥´"),
        ("ì„œìš¸", "ì„œìš¸")  # ìžê¸° ìžì‹ 
    ]
    
    for input_city, expected in test_cases:
        result = normalize_city_name(input_city)
        assert result == expected, f"{input_city} â†’ {result} (ì˜ˆìƒ: {expected})"
        print(f"âœ… {input_city} â†’ {result}")
```

#### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: file_handler KKday ì „ìš© í•¨ìˆ˜
```python
def test_file_handler_kkday_functions():
    """file_handlerì˜ KKday ì „ìš© í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
    
    from src.utils.file_handler import save_to_csv_kkday, get_csv_path_kkday
    
    # CSV ê²½ë¡œ ìƒì„± í…ŒìŠ¤íŠ¸
    test_city = "ì„œìš¸"
    csv_path = get_csv_path_kkday(test_city)
    
    assert 'kkday' in csv_path.lower(), "íŒŒì¼ëª…ì— kkday ëˆ„ë½"
    assert 'klook' not in csv_path.lower(), "íŒŒì¼ëª…ì— klook ìž”ì¡´"
    assert test_city in csv_path, "ë„ì‹œëª… ëˆ„ë½"
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ CSV ì €ìž¥
    test_data = {
        "ë²ˆí˜¸": "1",
        "ìƒí’ˆëª…": "í…ŒìŠ¤íŠ¸ ìƒí’ˆ",
        "ê°€ê²©": "â‚©50,000",
        "URL": "https://www.kkday.com/ko/product/12345",
        "ë°ì´í„°ì†ŒìŠ¤": "KKday"
    }
    
    result = save_to_csv_kkday(test_data, test_city)
    assert result == True, "CSV ì €ìž¥ ì‹¤íŒ¨"
    
    print(f"âœ… CSV ì €ìž¥: {csv_path}")
```

### 1.3 scraper/ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸

#### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: KKdayCrawler ì´ˆê¸°í™”
```python
def test_kkday_crawler_initialization():
    """KKdayCrawler í´ëž˜ìŠ¤ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
    
    from src.scraper.crawler import KKdayCrawler
    
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    crawler = KKdayCrawler("ì„œìš¸")
    
    assert crawler.city_name == "ì„œìš¸"
    assert crawler.platform == "KKday"
    assert hasattr(crawler, 'collect_urls_kkday')
    assert hasattr(crawler, 'crawl_product_kkday')
    
    # Klook ê´€ë ¨ ì†ì„±/ë©”ì„œë“œ ì œê±° í™•ì¸
    class_methods = dir(crawler)
    klook_methods = [m for m in class_methods if 'klook' in m.lower()]
    assert len(klook_methods) == 0, f"Klook ë©”ì„œë“œ ìž”ì¡´: {klook_methods}"
    
    print("âœ… KKdayCrawler ì´ˆê¸°í™” ì™„ë£Œ")
```

## ðŸ”— Level 2: í†µí•© í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

### 2.1 ëª¨ë“ˆ ê°„ ì—°ë™ í…ŒìŠ¤íŠ¸

#### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: config â†” crawler ì—°ë™
```python
def test_config_crawler_integration():
    """config ëª¨ë“ˆê³¼ crawler ëª¨ë“ˆ ì—°ë™ í…ŒìŠ¤íŠ¸"""
    
    from src.config import get_city_info
    from src.scraper.crawler import KKdayCrawler
    
    test_city = "ì„œìš¸"
    
    # Step 1: configì—ì„œ ë„ì‹œ ì •ë³´ ì¡°íšŒ
    city_info = get_city_info(test_city)
    assert city_info is not None
    
    # Step 2: crawlerì— ë„ì‹œ ì •ë³´ ì „ë‹¬
    crawler = KKdayCrawler(test_city)
    assert crawler.city_code == city_info['city_code']
    
    # Step 3: í†µí•© ë™ìž‘ í™•ì¸
    initialization_result = crawler.initialize()
    assert initialization_result == True
    
    print(f"âœ… config â†” crawler ì—°ë™: {test_city} ({city_info['city_code']})")
```

#### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: crawler â†” file_handler ì—°ë™
```python
def test_crawler_filehandler_integration():
    """crawlerì™€ file_handler ì—°ë™ í…ŒìŠ¤íŠ¸"""
    
    from src.scraper.crawler import KKdayCrawler
    from src.utils.file_handler import get_csv_path_kkday
    import os
    
    test_city = "ì„œìš¸"
    crawler = KKdayCrawler(test_city)
    
    # Step 1: í¬ë¡¤ëŸ¬ì—ì„œ ë°ì´í„° ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
    mock_product_data = {
        "ë²ˆí˜¸": "TEST001",
        "ìƒí’ˆëª…": "ì„œìš¸ ì‹œí‹°íˆ¬ì–´",
        "ê°€ê²©": "â‚©75,000",
        "URL": "https://www.kkday.com/ko/product/test001",
        "ë°ì´í„°ì†ŒìŠ¤": "KKday"
    }
    
    # Step 2: file_handlerë¡œ ì €ìž¥
    save_result = crawler.save_product_data(mock_product_data)
    assert save_result == True
    
    # Step 3: íŒŒì¼ ìƒì„± í™•ì¸
    csv_path = get_csv_path_kkday(test_city)
    assert os.path.exists(csv_path), f"CSV íŒŒì¼ ë¯¸ìƒì„±: {csv_path}"
    
    print(f"âœ… crawler â†” file_handler ì—°ë™ ì™„ë£Œ")
```

### 2.2 ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸

#### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§ ì „ì²´ ê³¼ì •
```python
def test_single_product_crawling_workflow():
    """ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§ ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
    
    from src.scraper.crawler import KKdayCrawler
    import time
    
    # Given: í…ŒìŠ¤íŠ¸ ì„¤ì •
    test_city = "ì„œìš¸"
    max_products = 1
    
    # When: ì „ì²´ í¬ë¡¤ë§ ê³¼ì • ì‹¤í–‰
    start_time = time.time()
    
    try:
        # Step 1: í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = KKdayCrawler(test_city)
        init_result = crawler.initialize()
        assert init_result == True, "í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨"
        
        # Step 2: URL ìˆ˜ì§‘
        urls = crawler.collect_urls_kkday(max_pages=1)
        assert len(urls) > 0, "URL ìˆ˜ì§‘ ì‹¤íŒ¨"
        print(f"ðŸ“Š ìˆ˜ì§‘ëœ URL: {len(urls)}ê°œ")
        
        # Step 3: ì²« ë²ˆì§¸ ìƒí’ˆ í¬ë¡¤ë§
        first_url = urls[0]
        crawl_result = crawler.crawl_product_kkday(first_url, rank=1)
        assert crawl_result == True, "ìƒí’ˆ í¬ë¡¤ë§ ì‹¤íŒ¨"
        
        # Step 4: ë°ì´í„° ê²€ì¦
        assert crawler.last_product_data is not None
        assert 'ìƒí’ˆëª…' in crawler.last_product_data
        assert 'ê°€ê²©' in crawler.last_product_data
        assert 'URL' in crawler.last_product_data
        
        end_time = time.time()
        elapsed = round(end_time - start_time, 2)
        
        print(f"âœ… ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§ ì™„ë£Œ ({elapsed}ì´ˆ)")
        print(f"ðŸ“‹ ìƒí’ˆëª…: {crawler.last_product_data.get('ìƒí’ˆëª…', 'N/A')}")
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        assert False, f"ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨: {e}"
```

## ðŸš€ Level 3: ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ (E2E)

### 3.1 ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸

#### ì‹œë‚˜ë¦¬ì˜¤: ì„œìš¸ ì—¬í–‰ìƒí’ˆ 10ê°œ ìˆ˜ì§‘
```python
def test_seoul_10_products_scenario():
    """ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ì„œìš¸ ìƒí’ˆ 10ê°œ ìˆ˜ì§‘"""
    
    from src.scraper.crawler import KKdayCrawler
    from src.utils.file_handler import get_csv_path_kkday
    import pandas as pd
    import os
    
    # í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°
    TARGET_CITY = "ì„œìš¸"
    TARGET_COUNT = 10
    MAX_PAGES = 3
    
    print(f"ðŸš€ ì‹œë‚˜ë¦¬ì˜¤ ì‹œìž‘: {TARGET_CITY} ìƒí’ˆ {TARGET_COUNT}ê°œ ìˆ˜ì§‘")
    
    try:
        # Step 1: í™˜ê²½ ì¤€ë¹„
        crawler = KKdayCrawler(TARGET_CITY)
        assert crawler.initialize() == True
        
        # Step 2: URL ëŒ€ëŸ‰ ìˆ˜ì§‘
        start_time = time.time()
        all_urls = crawler.collect_urls_kkday(max_pages=MAX_PAGES)
        collect_time = time.time() - start_time
        
        assert len(all_urls) >= TARGET_COUNT, f"URL ìˆ˜ì§‘ ë¶€ì¡±: {len(all_urls)}/{TARGET_COUNT}"
        print(f"ðŸ“Š URL ìˆ˜ì§‘ ì™„ë£Œ: {len(all_urls)}ê°œ ({collect_time:.1f}ì´ˆ)")
        
        # Step 3: ìƒí’ˆë³„ í¬ë¡¤ë§
        successful_products = 0
        failed_products = 0
        
        for i, url in enumerate(all_urls[:TARGET_COUNT]):
            print(f"ðŸ”„ ì§„í–‰ì¤‘ ({i+1}/{TARGET_COUNT}): {url[:50]}...")
            
            crawl_start = time.time()
            result = crawler.crawl_product_kkday(url, rank=i+1)
            crawl_time = time.time() - crawl_start
            
            if result:
                successful_products += 1
                print(f"âœ… ì„±ê³µ ({crawl_time:.1f}ì´ˆ)")
            else:
                failed_products += 1
                print(f"âŒ ì‹¤íŒ¨ ({crawl_time:.1f}ì´ˆ)")
            
            # ìžì—°ìŠ¤ëŸ¬ìš´ ëŒ€ê¸° (2-4ì´ˆ)
            time.sleep(random.uniform(2, 4))
        
        # Step 4: ê²°ê³¼ ê²€ì¦
        total_time = time.time() - start_time
        success_rate = (successful_products / TARGET_COUNT) * 100
        
        assert success_rate >= 80, f"ì„±ê³µë¥  ë¶€ì¡±: {success_rate}% (ìµœì†Œ 80% í•„ìš”)"
        
        # Step 5: CSV íŒŒì¼ ê²€ì¦
        csv_path = get_csv_path_kkday(TARGET_CITY)
        assert os.path.exists(csv_path), "CSV íŒŒì¼ ë¯¸ìƒì„±"
        
        df = pd.read_csv(csv_path)
        assert len(df) >= successful_products, "CSV ë°ì´í„° ëˆ„ë½"
        
        print(f"ðŸŽ‰ ì‹œë‚˜ë¦¬ì˜¤ ì™„ë£Œ!")
        print(f"ðŸ“Š ì„±ê³µë¥ : {success_rate}% ({successful_products}/{TARGET_COUNT})")
        print(f"â±ï¸ ì´ ì‹œê°„: {total_time/60:.1f}ë¶„")
        print(f"ðŸ“ ì €ìž¥ ìœ„ì¹˜: {csv_path}")
        
    except AssertionError as e:
        print(f"âŒ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤íŒ¨: {e}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
        raise
```

#### ì‹œë‚˜ë¦¬ì˜¤: ë‹¤ì¤‘ ë„ì‹œ í¬ë¡¤ë§
```python
def test_multi_city_scenario():
    """ë‹¤ì¤‘ ë„ì‹œ í¬ë¡¤ë§ ì‹œë‚˜ë¦¬ì˜¤"""
    
    # í…ŒìŠ¤íŠ¸ ë„ì‹œ ëª©ë¡ (ë‚œì´ë„ ìˆœ)
    TEST_CITIES = [
        ("ì„œìš¸", 3),      # í•œêµ­ì–´, ì‰¬ì›€
        ("ë„ì¿„", 3),      # ì¼ë³¸, ì¤‘ê°„  
        ("ë°©ì½•", 5),      # íƒœêµ­, ì¤‘ê°„
        ("íŒŒë¦¬", 3)       # ìœ ëŸ½, ì–´ë ¤ì›€
    ]
    
    results = {}
    
    for city, target_count in TEST_CITIES:
        print(f"\nðŸŒ {city} í¬ë¡¤ë§ ì‹œìž‘ (ëª©í‘œ: {target_count}ê°œ)")
        
        try:
            crawler = KKdayCrawler(city)
            
            # ê° ë„ì‹œë³„ í¬ë¡¤ë§
            success_count = crawler.run_crawling(
                max_products=target_count,
                max_pages=2
            )
            
            results[city] = {
                'target': target_count,
                'success': success_count,
                'rate': round((success_count/target_count)*100, 1)
            }
            
            print(f"âœ… {city} ì™„ë£Œ: {success_count}/{target_count} ({results[city]['rate']}%)")
            
        except Exception as e:
            results[city] = {
                'target': target_count, 
                'success': 0,
                'rate': 0,
                'error': str(e)
            }
            print(f"âŒ {city} ì‹¤íŒ¨: {e}")
    
    # ì „ì²´ ê²°ê³¼ ê²€ì¦
    total_success = sum(r['success'] for r in results.values())
    total_target = sum(r['target'] for r in results.values())
    overall_rate = (total_success / total_target) * 100
    
    print(f"\nðŸ“Š ì „ì²´ ê²°ê³¼:")
    for city, result in results.items():
        status = "âœ…" if result['rate'] >= 70 else "âŒ" 
        print(f"{status} {city}: {result['rate']}%")
    
    print(f"ðŸŽ¯ ì „ì²´ ì„±ê³µë¥ : {overall_rate:.1f}%")
    
    assert overall_rate >= 75, f"ì „ì²´ ì„±ê³µë¥  ë¶€ì¡±: {overall_rate}% (ìµœì†Œ 75% í•„ìš”)"
```

### 3.2 ì˜ˆì™¸ ìƒí™© ì‹œë‚˜ë¦¬ì˜¤

#### ì‹œë‚˜ë¦¬ì˜¤: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ëŒ€ì‘
```python
def test_network_error_scenario():
    """ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ìƒí™© ëŒ€ì‘ í…ŒìŠ¤íŠ¸"""
    
    from src.scraper.crawler import KKdayCrawler
    from unittest.mock import patch
    import requests
    
    crawler = KKdayCrawler("ì„œìš¸")
    
    # ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œë®¬ë ˆì´ì…˜
    with patch('requests.get') as mock_get:
        mock_get.side_effect = requests.ConnectionError("ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì‹¤íŒ¨")
        
        # ìžë™ ìž¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸
        result = crawler.crawl_with_retry("https://www.kkday.com/test")
        
        # ìµœëŒ€ ìž¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨í•˜ë©´ False ë°˜í™˜
        assert result == False
        print("âœ… ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ëŒ€ì‘ í™•ì¸")
```

#### ì‹œë‚˜ë¦¬ì˜¤: ìž˜ëª»ëœ URL ì²˜ë¦¬
```python
def test_invalid_url_scenario():
    """ìž˜ëª»ëœ URL ìž…ë ¥ ì‹œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    
    from src.scraper.crawler import KKdayCrawler
    
    crawler = KKdayCrawler("ì„œìš¸")
    
    invalid_urls = [
        "https://www.kkday.com/nonexistent",
        "https://invalid-domain.com/product/123", 
        "",
        None,
        "not-a-url"
    ]
    
    for url in invalid_urls:
        result = crawler.crawl_product_kkday(url, rank=1)
        assert result == False, f"ìž˜ëª»ëœ URLì—ì„œ ì„±ê³µ ë°˜í™˜: {url}"
        print(f"âœ… ìž˜ëª»ëœ URL ì²˜ë¦¬: {url}")
```

## âš¡ Level 4: ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

### 4.1 ë¶€í•˜ í…ŒìŠ¤íŠ¸

#### ì‹œë‚˜ë¦¬ì˜¤: ëŒ€ìš©ëŸ‰ URL ì²˜ë¦¬ (100ê°œ)
```python
def test_large_scale_crawling():
    """ëŒ€ìš©ëŸ‰ í¬ë¡¤ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (100ê°œ ìƒí’ˆ)"""
    
    import psutil
    import time
    from src.scraper.crawler import KKdayCrawler
    
    # ì„±ëŠ¥ ì¸¡ì • ì‹œìž‘
    process = psutil.Process()
    start_memory = process.memory_info().rss / 1024 / 1024  # MB
    start_time = time.time()
    
    TARGET_COUNT = 100
    crawler = KKdayCrawler("ì„œìš¸")
    
    print(f"ðŸš€ ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ì‹œìž‘: {TARGET_COUNT}ê°œ ìƒí’ˆ")
    print(f"ðŸ“Š ì‹œìž‘ ë©”ëª¨ë¦¬: {start_memory:.1f}MB")
    
    try:
        # URL ëŒ€ëŸ‰ ìˆ˜ì§‘
        urls = crawler.collect_urls_kkday(max_pages=10)
        assert len(urls) >= TARGET_COUNT, f"URL ìˆ˜ì§‘ ë¶€ì¡±: {len(urls)}"
        
        # ë°°ì¹˜ í¬ë¡¤ë§ (10ê°œì”© ì²˜ë¦¬)
        batch_size = 10
        total_processed = 0
        
        for i in range(0, min(TARGET_COUNT, len(urls)), batch_size):
            batch_urls = urls[i:i+batch_size]
            batch_start = time.time()
            
            # ë°°ì¹˜ ì²˜ë¦¬
            for j, url in enumerate(batch_urls):
                result = crawler.crawl_product_kkday(url, rank=i+j+1)
                if result:
                    total_processed += 1
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
                if (i+j+1) % 20 == 0:
                    current_memory = process.memory_info().rss / 1024 / 1024
                    print(f"ðŸ“Š ì§„í–‰ë¥ : {i+j+1}/{TARGET_COUNT}, ë©”ëª¨ë¦¬: {current_memory:.1f}MB")
            
            batch_time = time.time() - batch_start
            print(f"âœ… ë°°ì¹˜ {i//batch_size + 1} ì™„ë£Œ ({batch_time:.1f}ì´ˆ, {len(batch_urls)}ê°œ)")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if i % 50 == 0:
                import gc
                gc.collect()
        
        # ì„±ëŠ¥ ì¸¡ì • ì™„ë£Œ
        end_time = time.time()
        end_memory = process.memory_info().rss / 1024 / 1024
        
        total_time = end_time - start_time
        memory_increase = end_memory - start_memory
        success_rate = (total_processed / TARGET_COUNT) * 100
        
        # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
        assert success_rate >= 85, f"ì„±ê³µë¥  ê¸°ì¤€ ë¯¸ë‹¬: {success_rate}%"
        assert memory_increase <= 2000, f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼: {memory_increase}MB"  # 2GB ì œí•œ
        assert total_time <= 3600, f"ì‹œê°„ ì´ˆê³¼: {total_time}ì´ˆ (1ì‹œê°„ ì œí•œ)"
        
        print(f"ðŸŽ‰ ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"ðŸ“Š ì„±ê³µë¥ : {success_rate}%")
        print(f"â±ï¸ ì†Œìš”ì‹œê°„: {total_time/60:.1f}ë¶„") 
        print(f"ðŸ’¾ ë©”ëª¨ë¦¬ ì¦ê°€: {memory_increase:.1f}MB")
        print(f"ðŸš€ í‰ê·  ì†ë„: {total_processed/(total_time/60):.1f}ê°œ/ë¶„")
        
    except Exception as e:
        print(f"âŒ ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise
```

### 4.2 ìž¥ì‹œê°„ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸

#### ì‹œë‚˜ë¦¬ì˜¤: 6ì‹œê°„ ì—°ì† ì‹¤í–‰
```python
def test_long_running_stability():
    """ìž¥ì‹œê°„ ì—°ì† ì‹¤í–‰ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"""
    
    from src.scraper.crawler import KKdayCrawler
    import time
    import datetime
    
    # 6ì‹œê°„ = 21600ì´ˆ
    TEST_DURATION = 6 * 60 * 60  
    CHECK_INTERVAL = 30 * 60     # 30ë¶„ë§ˆë‹¤ ì²´í¬
    
    start_time = time.time()
    crawler = KKdayCrawler("ì„œìš¸")
    
    print(f"ðŸ• ìž¥ì‹œê°„ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì‹œìž‘ (6ì‹œê°„)")
    print(f"â° ì‹œìž‘ ì‹œê°„: {datetime.datetime.now()}")
    
    error_count = 0
    success_count = 0
    check_points = []
    
    while time.time() - start_time < TEST_DURATION:
        try:
            # ì£¼ê¸°ì ìœ¼ë¡œ í¬ë¡¤ë§ ì‹¤í–‰ (ë§¤ 5ë¶„)
            urls = crawler.collect_urls_kkday(max_pages=1)
            
            if urls:
                result = crawler.crawl_product_kkday(urls[0], rank=1)
                if result:
                    success_count += 1
                else:
                    error_count += 1
            
            # 30ë¶„ë§ˆë‹¤ ìƒíƒœ ì²´í¬
            current_time = time.time()
            if len(check_points) == 0 or current_time - check_points[-1] >= CHECK_INTERVAL:
                check_points.append(current_time)
                elapsed_hours = (current_time - start_time) / 3600
                
                print(f"ðŸ“Š {elapsed_hours:.1f}ì‹œê°„ ê²½ê³¼ - ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {error_count}")
                
                # ì˜¤ë¥˜ìœ¨ ì²´í¬
                if success_count + error_count > 0:
                    error_rate = (error_count / (success_count + error_count)) * 100
                    assert error_rate <= 10, f"ì˜¤ë¥˜ìœ¨ ì´ˆê³¼: {error_rate}%"
            
            # 5ë¶„ ëŒ€ê¸°
            time.sleep(5 * 60)
            
        except KeyboardInterrupt:
            print("ðŸ›‘ ì‚¬ìš©ìž ì¤‘ë‹¨")
            break
        except Exception as e:
            error_count += 1
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            # ì—°ì† ì˜¤ë¥˜ ì²´í¬
            assert error_count <= 20, f"ì—°ì† ì˜¤ë¥˜ í•œê³„ ì´ˆê³¼: {error_count}"
    
    total_time = time.time() - start_time
    final_error_rate = (error_count / max(success_count + error_count, 1)) * 100
    
    print(f"ðŸ ìž¥ì‹œê°„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {total_time/3600:.1f}ì‹œê°„")
    print(f"ðŸ“Š ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {error_count}")
    print(f"ðŸ“ˆ ìµœì¢… ì„±ê³µë¥ : {100-final_error_rate:.1f}%")
    
    assert final_error_rate <= 5, f"ìµœì¢… ì˜¤ë¥˜ìœ¨ ì´ˆê³¼: {final_error_rate}%"
```

## ðŸ“Š í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ê²°ê³¼ ê´€ë¦¬

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìŠ¤ì¼€ì¤„

#### ê°œë°œ ì¤‘ í…ŒìŠ¤íŠ¸ (ë§¤ì¼)
```bash
# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (5ë¶„ ì´ë‚´)
python -m pytest tests/unit/ -v
python -m pytest tests/integration/ -k "not slow" -v
```

#### ì£¼ê°„ í…ŒìŠ¤íŠ¸ (ì£¼ë§)
```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ (2-3ì‹œê°„)
python -m pytest tests/ -v --html=report.html
python test_scenarios.py --scenario=multi_city
```

#### ë°°í¬ ì „ í…ŒìŠ¤íŠ¸ (ìˆ˜ë™)
```bash
# ì„±ëŠ¥ ë° ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ (6-8ì‹œê°„) 
python test_scenarios.py --scenario=large_scale
python test_scenarios.py --scenario=long_running
```

### í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŒ…

#### ìžë™ ë¦¬í¬íŠ¸ ìƒì„±
```python
def generate_test_report(test_results):
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸ ìžë™ ìƒì„±"""
    
    report = f"""
# KKday í¬ë¡¤ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸

**ì‹¤í–‰ ì‹œê°„**: {datetime.datetime.now()}
**ì´ í…ŒìŠ¤íŠ¸**: {test_results['total']}
**ì„±ê³µ**: {test_results['passed']} 
**ì‹¤íŒ¨**: {test_results['failed']}
**ì„±ê³µë¥ **: {(test_results['passed']/test_results['total'])*100:.1f}%

## ìƒì„¸ ê²°ê³¼

### âœ… í†µê³¼í•œ í…ŒìŠ¤íŠ¸
{chr(10).join(test_results['passed_tests'])}

### âŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸  
{chr(10).join(test_results['failed_tests'])}

### ðŸ“Š ì„±ëŠ¥ ì§€í‘œ
- í‰ê·  ì‘ë‹µì‹œê°„: {test_results['avg_response_time']:.2f}ì´ˆ
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {test_results['memory_usage']:.1f}MB
- ì²˜ë¦¬ìœ¨: {test_results['throughput']:.1f}ê°œ/ë¶„

## ê¶Œìž¥ì‚¬í•­
{chr(10).join(test_results['recommendations'])}
"""
    
    # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ìž¥
    with open('test_report.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    return report
```

## âš ï¸ í…ŒìŠ¤íŠ¸ ì‹œ ì£¼ì˜ì‚¬í•­

### ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­
- **ìš”ì²­ ë¹ˆë„**: 5ì´ˆ ì´ìƒ ê°„ê²© ìœ ì§€
- **ë™ì‹œ ì—°ê²°**: ìµœëŒ€ 1ê°œ ì„¸ì…˜ë§Œ ì‚¬ìš©
- **í…ŒìŠ¤íŠ¸ ë°ì´í„°**: ì‹¤ì œ ìƒìš© ì„œë¹„ìŠ¤ì— ì˜í–¥ ì—†ë„ë¡ ì œí•œ

### ê¸°ìˆ ì  ì œì•½ì‚¬í•­  
- **ë¸Œë¼ìš°ì € ë²„ì „**: Chrome 120+ í•„ìˆ˜
- **ë„¤íŠ¸ì›Œí¬**: ì•ˆì •ì ì¸ ì¸í„°ë„· ì—°ê²° í•„ìš”
- **ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤**: ìµœì†Œ 4GB RAM, ê¶Œìž¥ 8GB

### í…ŒìŠ¤íŠ¸ ë°ì´í„° ê´€ë¦¬
- **ìž„ì‹œ íŒŒì¼**: í…ŒìŠ¤íŠ¸ í›„ ìžë™ ì •ë¦¬
- **CSV ë°±ì—…**: ê¸°ì¡´ ë°ì´í„° ë®ì–´ì“°ê¸° ë°©ì§€  
- **ì´ë¯¸ì§€ í´ë”**: í…ŒìŠ¤íŠ¸ìš© ë³„ë„ ë””ë ‰í„°ë¦¬ ì‚¬ìš©

---

**ë¬¸ì„œ ë²„ì „**: v1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2024-12-07  
**ë‹´ë‹¹ìž**: QAíŒ€  
**ë‹¤ìŒ ë¦¬ë·°**: 2024-12-14