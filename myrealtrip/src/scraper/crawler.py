"""
MyRealTrip ë©”ì¸ í¬ë¡¤ë§ ì—”ì§„
- ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ í†µí•© ê´€ë¦¬
- ê°ì²´ ì§€í–¥ ì„¤ê³„ë¥¼ í†µí•´ ì¬ì‚¬ìš©ì„± ë° ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
"""

import time
import random
from datetime import datetime

# ë¦¬íŒ©í† ë§ëœ ëª¨ë“ˆ import
from ..config import CONFIG
from ..utils.file_handler import create_product_data_structure, save_batch_data, get_last_product_number
from .driver_manager import setup_driver, go_to_main_page, find_and_fill_search
from .url_manager import collect_product_urls_from_page, hybrid_is_processed, mark_url_processed_fast
from .parsers import get_product_name, get_price, get_rating, get_review_count, clean_price, clean_rating

class MyRealTripCrawler:
    """MyRealTrip í¬ë¡¤ë§ì„ ìœ„í•œ ëª¨ë“  ë¡œì§ì„ ìº¡ìŠí™”í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, city_name):
        self.city_name = city_name
        self.driver = None
        self.stats = {
            "start_time": None,
            "end_time": None,
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "urls_collected": 0,
        }
        self.stop_flag = False

    def _initialize_driver(self):
        """ë“œë¼ì´ë²„ë¥¼ ì„¤ì •í•˜ê³  ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤."""
        try:
            self.driver = setup_driver()
            go_to_main_page(self.driver)
            return find_and_fill_search(self.driver, self.city_name)
        except Exception as e:
            print(f"âŒ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def _collect_urls(self, use_infinite_scroll=True):
        """URLì„ ìˆ˜ì§‘í•˜ê³  ì¤‘ë³µì„ í•„í„°ë§í•©ë‹ˆë‹¤."""
        print("ğŸ”— URL ìˆ˜ì§‘ ì‹œì‘...")
        all_urls = collect_product_urls_from_page(self.driver, use_infinite_scroll)
        
        new_urls = []
        for url in all_urls:
            if not hybrid_is_processed(url, self.city_name):
                new_urls.append(url)
        
        self.stats["urls_collected"] = len(new_urls)
        print(f"âœ… {len(new_urls)}ê°œì˜ ìƒˆë¡œìš´ URL ìˆ˜ì§‘ ì™„ë£Œ.")
        return new_urls

    def _crawl_single_product(self, url, product_number):
        """ë‹¨ì¼ ìƒí’ˆ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        if self.stop_flag: return None

        print(f"\nğŸ” ìƒí’ˆ ì²˜ë¦¬ ì‹œì‘ (ë²ˆí˜¸: {product_number}): {url[:70]}...")
        main_window = self.driver.current_window_handle
        self.driver.switch_to.new_window('tab')
        self.driver.get(url)
        time.sleep(random.uniform(3, 5))

        try:
            product_data = create_product_data_structure(self.city_name, product_number)
            product_data["URL"] = url
            product_data["ìƒí’ˆëª…"] = get_product_name(self.driver)
            product_data["ê°€ê²©"] = clean_price(get_price(self.driver))
            product_data["í‰ì "] = clean_rating(get_rating(self.driver))
            product_data["ë¦¬ë·°ìˆ˜"] = get_review_count(self.driver)
            
            # TODO: kkday ê·œê²©ì— ë§ê²Œ ì¶”ê°€ ë°ì´í„° ì¶”ì¶œ ë¡œì§ êµ¬í˜„

            self.stats["success_count"] += 1
            print(f"  âœ… ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ì„±ê³µ: {product_data['ìƒí’ˆëª…'][:30]}...")
            return product_data
        except Exception as e:
            print(f"  âŒ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            self.stats["error_count"] += 1
            return None
        finally:
            self.driver.close()
            self.driver.switch_to.window(main_window)

    def run_crawling(self, max_products=10, use_infinite_scroll=True):
        """ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        self.stats["start_time"] = datetime.now()
        print(f"ğŸš€ {self.city_name} í¬ë¡¤ë§ ì‹œì‘ (ëª©í‘œ: {max_products}ê°œ)")

        if not self._initialize_driver():
            return

        urls_to_crawl = self._collect_urls(use_infinite_scroll)
        if not urls_to_crawl:
            print("âš ï¸ í¬ë¡¤ë§í•  ìƒˆë¡œìš´ URLì´ ì—†ìŠµë‹ˆë‹¤.")
            self.driver.quit()
            return

        product_number = get_last_product_number(self.city_name) + 1
        results = []

        for i, url in enumerate(urls_to_crawl):
            if i >= max_products:
                print(f"ğŸ¯ ëª©í‘œ ìˆ˜ëŸ‰({max_products}ê°œ) ë‹¬ì„±. í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
            if self.stop_flag:
                print("ğŸ›‘ ì •ì§€ ì‹ í˜¸ ê°ì§€. í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break

            data = self._crawl_single_product(url, product_number + i)
            if data:
                results.append(data)
                mark_url_processed_fast(url, self.city_name, product_number + i)
            
            self.stats["total_processed"] += 1

        if results:
            save_batch_data(results, self.city_name)

        self.stats["end_time"] = datetime.now()
        print("ğŸ‰ í¬ë¡¤ë§ ì„¸ì…˜ ì™„ë£Œ.")
        self._print_stats()
        self.driver.quit()

    def _print_stats(self):
        duration = self.stats["end_time"] - self.stats["start_time"]
        print("\n--- ìµœì¢… í†µê³„ ---")
        print(f"ì†Œìš” ì‹œê°„: {duration}")
        print(f"ì´ ì²˜ë¦¬ ì‹œë„: {self.stats['total_processed']}")
        print(f"ì„±ê³µ: {self.stats['success_count']}")
        print(f"ì‹¤íŒ¨: {self.stats['error_count']}")
        print("---------------")

print("âœ… crawler.py ìƒì„± ì™„ë£Œ: MyRealTripCrawler í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ!")
