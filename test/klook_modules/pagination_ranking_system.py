"""
ğŸ“„ í˜ì´ì§€ë„¤ì´ì…˜ ê¸°ë°˜ ìˆœìœ„ë³„ í¬ë¡¤ë§ ì‹œìŠ¤í…œ
- ì „ì²´ íƒ­ì—ì„œ í˜ì´ì§€ë¥¼ ë„˜ì–´ê°€ë©´ì„œ ìˆœìœ„ ì—°ì†ì„± ìœ ì§€
- ë­í‚¹ ì •ë³´ì™€ í¬ë¡¤ë§ ë°ì´í„° ë¶„ë¦¬ ì €ì¥
- CSV, ì´ë¯¸ì§€, ë­í‚¹ ì •ë³´ì˜ ì—°ì†ì„± ë³´ì¥
"""

import os
import json
import time
from datetime import datetime
from .config import get_city_code, get_city_info

class PaginationRankingSystem:
    """í˜ì´ì§€ë„¤ì´ì…˜ ê¸°ë°˜ ìˆœìœ„ë³„ í¬ë¡¤ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.ranking_continuity = {}  # í˜ì´ì§€ ê°„ ìˆœìœ„ ì—°ì†ì„± ì¶”ì 
        self.collected_urls = []      # ìˆ˜ì§‘ëœ URL ëª©ë¡
        self.current_global_rank = 1  # ì „ì²´ì ì¸ ìˆœìœ„ ì¶”ì 
        
    def collect_urls_with_pagination(self, driver, city_name, target_count=15, max_pages=5):
        """í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•œ ìˆœìœ„ë³„ URL ìˆ˜ì§‘"""
        print(f"ğŸ“„ í˜ì´ì§€ë„¤ì´ì…˜ ê¸°ë°˜ URL ìˆ˜ì§‘ ì‹œì‘")
        print(f"ğŸ¯ ëª©í‘œ: {target_count}ê°œ URL, ìµœëŒ€ {max_pages}í˜ì´ì§€")
        
        city_code = get_city_code(city_name)
        all_collected_urls = []
        current_page = 1
        global_rank = 1
        
        try:
            while len(all_collected_urls) < target_count and current_page <= max_pages:
                print(f"\nğŸ“– í˜ì´ì§€ {current_page} ì²˜ë¦¬ ì¤‘...")
                
                # í˜„ì¬ í˜ì´ì§€ì—ì„œ URL ìˆ˜ì§‘ (ìˆœìœ„ ìˆœì„œëŒ€ë¡œ)
                page_urls = self._collect_urls_from_current_page(driver, global_rank)
                
                if not page_urls:
                    print(f"   âŒ í˜ì´ì§€ {current_page}ì—ì„œ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    break
                
                print(f"   âœ… í˜ì´ì§€ {current_page}: {len(page_urls)}ê°œ URL ìˆ˜ì§‘")
                
                # ëª©í‘œ ê°œìˆ˜ë§Œí¼ë§Œ ì¶”ê°€
                remaining_needed = target_count - len(all_collected_urls)
                urls_to_add = page_urls[:remaining_needed]
                
                for url_data in urls_to_add:
                    all_collected_urls.append({
                        'url': url_data['url'],
                        'global_rank': global_rank,
                        'page': current_page,
                        'page_position': url_data['page_position'],
                        'collected_at': datetime.now().isoformat()
                    })
                    global_rank += 1
                
                print(f"   ğŸ“Š ëˆ„ì  ìˆ˜ì§‘: {len(all_collected_urls)}ê°œ/{target_count}ê°œ")
                
                # ëª©í‘œ ë‹¬ì„± ì‹œ ì¤‘ë‹¨
                if len(all_collected_urls) >= target_count:
                    print(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±: {len(all_collected_urls)}ê°œ URL ìˆ˜ì§‘ ì™„ë£Œ")
                    break
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                if current_page < max_pages:
                    next_success = self._navigate_to_next_page(driver)
                    if not next_success:
                        print(f"   âš ï¸ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ ì‹¤íŒ¨ - ìˆ˜ì§‘ ì¢…ë£Œ")
                        break
                    
                    current_page += 1
                    time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                else:
                    print(f"   ğŸ“„ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ë„ë‹¬ ({max_pages}í˜ì´ì§€)")
                    break
            
            # ìˆ˜ì§‘ ê²°ê³¼ ì €ì¥
            if all_collected_urls:
                self._save_pagination_ranking_data(all_collected_urls, city_name)
            
            print(f"\nâœ… í˜ì´ì§€ë„¤ì´ì…˜ ìˆ˜ì§‘ ì™„ë£Œ:")
            print(f"   ğŸ“Š ì´ ìˆ˜ì§‘: {len(all_collected_urls)}ê°œ URL")
            print(f"   ğŸ“„ ì²˜ë¦¬ í˜ì´ì§€: {current_page}í˜ì´ì§€")
            print(f"   ğŸ† ìˆœìœ„ ë²”ìœ„: 1ìœ„ ~ {len(all_collected_urls)}ìœ„")
            
            return all_collected_urls
            
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ë„¤ì´ì…˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return all_collected_urls
    
    def _collect_urls_from_current_page(self, driver, start_rank):
        """í˜„ì¬ í˜ì´ì§€ì—ì„œ ìˆœìœ„ ìˆœì„œëŒ€ë¡œ URL ìˆ˜ì§‘"""
        try:
            from selenium.webdriver.common.by import By
            import time
            
            # í˜ì´ì§€ ì™„ì „ ë¡œë”© ëŒ€ê¸°
            time.sleep(3)
            
            # ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¡œ URL ìˆ˜ì§‘
            selectors = [
                "a[href*='/activity/']",
                ".result-card a[href*='/activity/']",
                ".search-result a[href*='/activity/']",
                "[data-testid*='product'] a[href*='/activity/']",
                ".product-card a[href*='/activity/']",
                ".card a[href*='/activity/']"
            ]
            
            all_elements = []
            
            for selector in selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        try:
                            href = element.get_attribute('href')
                            if href and '/activity/' in href and href not in [e['url'] for e in all_elements]:
                                # í™”ë©´ìƒ ìœ„ì¹˜ í™•ì¸ (ìˆœìœ„ ê²°ì •ìš©)
                                location = element.location
                                y_coord = location.get('y', 0)
                                x_coord = location.get('x', 0)
                                
                                if y_coord > 0:  # í™”ë©´ì— ë³´ì´ëŠ” ìš”ì†Œë§Œ
                                    all_elements.append({
                                        'url': href,
                                        'y': y_coord,
                                        'x': x_coord,
                                        'element': element
                                    })
                        except:
                            continue
                except:
                    continue
            
            # Yì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìœ„ì—ì„œ ì•„ë˜ë¡œ = ìˆœìœ„ ìˆœì„œ)
            all_elements.sort(key=lambda x: (x['y'], x['x']))
            
            # í˜ì´ì§€ë³„ ìœ„ì¹˜ì™€ í•¨ê»˜ ë°˜í™˜
            page_urls = []
            for idx, element_data in enumerate(all_elements, 1):
                page_urls.append({
                    'url': element_data['url'],
                    'page_position': idx,
                    'y_coord': element_data['y'],
                    'x_coord': element_data['x']
                })
            
            return page_urls
            
        except Exception as e:
            print(f"   âŒ í˜„ì¬ í˜ì´ì§€ URL ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def _navigate_to_next_page(self, driver):
        """ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™"""
        try:
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            import time
            
            # ë‹¤ì–‘í•œ ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ ì…€ë ‰í„°
            next_selectors = [
                "button[aria-label*='ë‹¤ìŒ']",
                "button[aria-label*='Next']", 
                ".pagination .next",
                ".pagination button:contains('>'))",
                "[data-testid*='next']",
                "button:contains('>'))",
                ".pagination a[rel='next']"
            ]
            
            for selector in next_selectors:
                try:
                    # CSS ì„ íƒìì™€ XPath êµ¬ë¶„
                    if 'contains' in selector:
                        # XPath ë°©ì‹
                        next_button = WebDriverWait(driver, 5).until(
                            EC.element_to_be_clickable((By.XPATH, f"//button[contains(text(), '>')]"))
                        )
                    else:
                        # CSS ì„ íƒì ë°©ì‹
                        next_button = WebDriverWait(driver, 5).until(
                            EC.element_to_be_clickable((By.CSS_SELECTOR, selector))
                        )
                    
                    # ë²„íŠ¼ì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    if next_button.is_enabled() and not next_button.get_attribute('disabled'):
                        print(f"   ğŸ”„ ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ í´ë¦­: {selector}")
                        
                        # JavaScriptë¡œ í´ë¦­ (ë” ì•ˆì •ì )
                        driver.execute_script("arguments[0].click();", next_button)
                        
                        # í˜ì´ì§€ ë³€í™” ëŒ€ê¸°
                        time.sleep(3)
                        
                        print(f"   âœ… ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ ì„±ê³µ")
                        return True
                        
                except Exception as e:
                    continue
            
            print(f"   âŒ ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return False
            
        except Exception as e:
            print(f"   âŒ ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
            return False
    
    def _save_pagination_ranking_data(self, collected_urls, city_name):
        """í˜ì´ì§€ë„¤ì´ì…˜ ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ë­í‚¹ ë°ì´í„°ë¡œ ì €ì¥"""
        try:
            city_code = get_city_code(city_name)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # 1. ìˆœìœ„ë³„ URL ì €ì¥ (ranking_urls/)
            ranking_urls_data = {
                "city_name": city_name,
                "city_code": city_code,
                "collection_method": "pagination_ranking",
                "collected_at": datetime.now().isoformat(),
                "total_urls": len(collected_urls),
                "max_page": max(url_data['page'] for url_data in collected_urls),
                "ranking_continuity": True,  # í˜ì´ì§€ ê°„ ìˆœìœ„ ì—°ì†ì„± ë³´ì¥
                "urls_with_ranking": []
            }
            
            for url_data in collected_urls:
                ranking_urls_data["urls_with_ranking"].append({
                    "global_rank": url_data['global_rank'],
                    "url": url_data['url'],
                    "page": url_data['page'],
                    "page_position": url_data['page_position'],
                    "collected_at": url_data['collected_at']
                })
            
            # ranking_urls ì €ì¥
            os.makedirs("ranking_urls", exist_ok=True)
            ranking_urls_file = f"ranking_urls/{city_code}_ì „ì²´_pagination_{timestamp}.json"
            
            with open(ranking_urls_file, 'w', encoding='utf-8') as f:
                json.dump(ranking_urls_data, f, ensure_ascii=False, indent=2)
            
            print(f"   ğŸ’¾ ìˆœìœ„ URL ì €ì¥: {ranking_urls_file}")
            
            # 2. ëˆ„ì  ë­í‚¹ ë°ì´í„° ì—…ë°ì´íŠ¸ (ranking_data/)
            self._update_accumulated_rankings(collected_urls, city_name)
            
            # 3. í¬ë¡¤ë§ìš© ë‹¨ìˆœ URL ëª©ë¡ ì €ì¥ (url_collected/)
            self._save_crawling_url_list(collected_urls, city_name)
            
        except Exception as e:
            print(f"   âŒ ë­í‚¹ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _update_accumulated_rankings(self, collected_urls, city_name):
        """ëˆ„ì  ë­í‚¹ ë°ì´í„° ì—…ë°ì´íŠ¸"""
        try:
            from .ranking_manager import ranking_manager
            
            # URLê³¼ ìˆœìœ„ ì •ë³´ë¡œ ë³€í™˜
            urls_with_ranking = []
            for url_data in collected_urls:
                urls_with_ranking.append(url_data['url'])
            
            # ê¸°ì¡´ ë­í‚¹ ë§¤ë‹ˆì € ì‚¬ìš©í•´ì„œ ì €ì¥
            success = ranking_manager.save_tab_ranking(
                urls_with_ranking, city_name, "ì „ì²´", "pagination"
            )
            
            if success:
                print(f"   ğŸ’¾ ëˆ„ì  ë­í‚¹ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
            # ì¶”ê°€ë¡œ ìˆœìœ„ ì •ë³´ ì§ì ‘ ì €ì¥
            city_code = get_city_code(city_name)
            accumulated_file = f"ranking_data/{city_code}_accumulated_rankings.json"
            
            if os.path.exists(accumulated_file):
                with open(accumulated_file, 'r', encoding='utf-8') as f:
                    accumulated = json.load(f)
            else:
                accumulated = {
                    "city_name": city_name,
                    "city_code": city_code,
                    "last_updated": datetime.now().isoformat(),
                    "url_rankings": {},
                    "stats": {
                        "total_urls": 0,
                        "tabs_processed": [],
                        "duplicate_urls": 0
                    }
                }
            
            # ìˆœìœ„ ì •ë³´ ì—…ë°ì´íŠ¸
            import hashlib
            for url_data in collected_urls:
                url = url_data['url']
                url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()
                
                if url_hash not in accumulated["url_rankings"]:
                    accumulated["url_rankings"][url_hash] = {
                        "url": url,
                        "url_hash": url_hash,
                        "first_found": url_data['collected_at'],
                        "tab_rankings": {},
                        "is_duplicate": False,
                        "pagination_info": {
                            "page": url_data['page'],
                            "page_position": url_data['page_position']
                        }
                    }
                
                # ì „ì²´ íƒ­ ìˆœìœ„ ì •ë³´ ì¶”ê°€
                accumulated["url_rankings"][url_hash]["tab_rankings"]["ì „ì²´"] = {
                    "ranking": url_data['global_rank'],
                    "found_at": url_data['collected_at'],
                    "collection_method": "pagination"
                }
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            accumulated["last_updated"] = datetime.now().isoformat()
            accumulated["stats"]["total_urls"] = len(accumulated["url_rankings"])
            if "ì „ì²´" not in accumulated["stats"]["tabs_processed"]:
                accumulated["stats"]["tabs_processed"].append("ì „ì²´")
            
            # íŒŒì¼ ì €ì¥
            with open(accumulated_file, 'w', encoding='utf-8') as f:
                json.dump(accumulated, f, ensure_ascii=False, indent=2)
            
            print(f"   ğŸ’¾ ëˆ„ì  ë­í‚¹ ì§ì ‘ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            print(f"   âŒ ëˆ„ì  ë­í‚¹ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _save_crawling_url_list(self, collected_urls, city_name):
        """í¬ë¡¤ë§ìš© ë‹¨ìˆœ URL ëª©ë¡ ì €ì¥"""
        try:
            city_code = get_city_code(city_name)
            os.makedirs("url_collected", exist_ok=True)
            
            # ê¸°ì¡´ ë¡œê·¸ì— ì¶”ê°€
            log_file = f"url_collected/{city_code}_pagination_log.txt"
            
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write(f"# í˜ì´ì§€ë„¤ì´ì…˜ ê¸°ë°˜ ìˆœìœ„ë³„ URL ìˆ˜ì§‘\n")
                f.write(f"# ìˆ˜ì§‘ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# ì´ {len(collected_urls)}ê°œ URL (ìˆœìœ„ë³„)\n\n")
                
                for url_data in collected_urls:
                    f.write(f"{url_data['global_rank']:2d}ìœ„ | í˜ì´ì§€{url_data['page']} | {url_data['url']}\n")
            
            print(f"   ğŸ“ í¬ë¡¤ë§ìš© URL ëª©ë¡ ì €ì¥: {log_file}")
            
        except Exception as e:
            print(f"   âŒ í¬ë¡¤ë§ URL ëª©ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")

class RankingDataMatcher:
    """ë­í‚¹ ì •ë³´ì™€ í¬ë¡¤ë§ ë°ì´í„° ë§¤ì¹­ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.ranking_cache = {}
    
    def get_ranking_info_for_url(self, url, city_name):
        """íŠ¹ì • URLì˜ ë­í‚¹ ì •ë³´ ì¡°íšŒ"""
        try:
            city_code = get_city_code(city_name)
            accumulated_file = f"ranking_data/{city_code}_accumulated_rankings.json"
            
            if not os.path.exists(accumulated_file):
                return None
            
            # ìºì‹œ í™•ì¸
            cache_key = f"{city_code}_{url}"
            if cache_key in self.ranking_cache:
                return self.ranking_cache[cache_key]
            
            # íŒŒì¼ì—ì„œ ì¡°íšŒ
            with open(accumulated_file, 'r', encoding='utf-8') as f:
                accumulated = json.load(f)
            
            import hashlib
            url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()
            
            if url_hash in accumulated["url_rankings"]:
                ranking_info = accumulated["url_rankings"][url_hash]
                
                # ìºì‹œì— ì €ì¥
                self.ranking_cache[cache_key] = ranking_info
                
                return ranking_info
            
            return None
            
        except Exception as e:
            print(f"âŒ ë­í‚¹ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def create_ranking_csv_mapping(self, city_name):
        """ë­í‚¹ ì •ë³´ì™€ CSV ë§¤í•‘ í…Œì´ë¸” ìƒì„±"""
        try:
            city_code = get_city_code(city_name)
            continent, country = get_city_info(city_name)
            
            # CSV íŒŒì¼ ê²½ë¡œ
            csv_path = f"data/{continent}/{country}/{country}_klook_products_all.csv"
            if not os.path.exists(csv_path):
                print(f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {csv_path}")
                return None
            
            # ë­í‚¹ ì •ë³´ ë¡œë“œ
            accumulated_file = f"ranking_data/{city_code}_accumulated_rankings.json"
            if not os.path.exists(accumulated_file):
                print(f"âŒ ë­í‚¹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {accumulated_file}")
                return None
            
            with open(accumulated_file, 'r', encoding='utf-8') as f:
                ranking_data = json.load(f)
            
            # ë§¤í•‘ í…Œì´ë¸” ìƒì„±
            mapping_table = []
            
            for url_hash, url_info in ranking_data["url_rankings"].items():
                url = url_info["url"]
                tab_rankings = url_info.get("tab_rankings", {})
                pagination_info = url_info.get("pagination_info", {})
                
                mapping_entry = {
                    "url": url,
                    "url_hash": url_hash,
                    "global_rank": tab_rankings.get("ì „ì²´", {}).get("ranking", 0),
                    "page": pagination_info.get("page", 0),
                    "page_position": pagination_info.get("page_position", 0),
                    "csv_present": False,  # CSVì—ì„œ í™•ì¸ í›„ ì—…ë°ì´íŠ¸
                    "csv_row_number": None,
                    "crawled": url_info.get("crawled", False),
                    "first_found": url_info.get("first_found"),
                    "last_updated": ranking_data.get("last_updated")
                }
                
                mapping_table.append(mapping_entry)
            
            # ìˆœìœ„ìˆœìœ¼ë¡œ ì •ë ¬
            mapping_table.sort(key=lambda x: x['global_rank'])
            
            # ë§¤í•‘ í…Œì´ë¸” ì €ì¥
            mapping_file = f"ranking_data/{city_code}_ranking_csv_mapping.json"
            mapping_data = {
                "city_name": city_name,
                "city_code": city_code,
                "created_at": datetime.now().isoformat(),
                "total_rankings": len(mapping_table),
                "mapping_table": mapping_table
            }
            
            with open(mapping_file, 'w', encoding='utf-8') as f:
                json.dump(mapping_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ë­í‚¹-CSV ë§¤í•‘ í…Œì´ë¸” ìƒì„±: {mapping_file}")
            print(f"   ğŸ“Š ì´ {len(mapping_table)}ê°œ URL ë§¤í•‘")
            
            return mapping_file
            
        except Exception as e:
            print(f"âŒ ë­í‚¹-CSV ë§¤í•‘ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
            return None

class ContinuityManager:
    """ì—°ì†ì„± ê´€ë¦¬ì - CSV, ì´ë¯¸ì§€, ë­í‚¹ì˜ ì—°ì†ì„± ë³´ì¥"""
    
    def __init__(self):
        self.sequences = {
            'csv_sequence': 0,
            'image_sequence': 0, 
            'ranking_sequence': 0
        }
    
    def get_next_csv_number(self, city_name):
        """ë‹¤ìŒ CSV ë²ˆí˜¸ íšë“"""
        try:
            continent, country = get_city_info(city_name)
            csv_path = f"data/{continent}/{country}/{country}_klook_products_all.csv"
            
            if os.path.exists(csv_path):
                # ê¸°ì¡´ íŒŒì¼ì—ì„œ ë§ˆì§€ë§‰ ë²ˆí˜¸ í™•ì¸
                with open(csv_path, 'r', encoding='utf-8-sig') as f:
                    lines = f.readlines()
                    if len(lines) > 1:  # í—¤ë” ì œì™¸
                        last_line = lines[-1].strip()
                        if last_line:
                            parts = last_line.split(',')
                            try:
                                last_number = int(parts[0])
                                return last_number + 1
                            except:
                                pass
            
            return 1
            
        except Exception as e:
            print(f"âŒ CSV ë²ˆí˜¸ íšë“ ì‹¤íŒ¨: {e}")
            return 1
    
    def get_next_image_number(self, city_name):
        """ë‹¤ìŒ ì´ë¯¸ì§€ ë²ˆí˜¸ íšë“"""
        try:
            city_code = get_city_code(city_name)
            continent, country = get_city_info(city_name)
            
            img_dir = f"klook_thumb_img/{continent}/{country}/{city_name}"
            if os.path.exists(img_dir):
                image_files = [f for f in os.listdir(img_dir) if f.startswith(f"{city_code}_")]
                if image_files:
                    # íŒŒì¼ëª…ì—ì„œ ë²ˆí˜¸ ì¶”ì¶œ
                    numbers = []
                    for filename in image_files:
                        try:
                            # FCO_0001.jpg ì—ì„œ 0001 ì¶”ì¶œ
                            number_str = filename.split('_')[1].split('.')[0].split('_')[0]
                            numbers.append(int(number_str))
                        except:
                            continue
                    
                    if numbers:
                        return max(numbers) + 1
            
            return 1
            
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ë²ˆí˜¸ íšë“ ì‹¤íŒ¨: {e}")
            return 1
    
    def ensure_continuity_consistency(self, city_name):
        """ì—°ì†ì„± ì¼ê´€ì„± ê²€ì‚¬ ë° ë³´ì •"""
        try:
            print(f"ğŸ”„ '{city_name}' ì—°ì†ì„± ì¼ê´€ì„± ê²€ì‚¬...")
            
            csv_next = self.get_next_csv_number(city_name)
            img_next = self.get_next_image_number(city_name)
            
            print(f"   ğŸ“Š ë‹¤ìŒ CSV ë²ˆí˜¸: {csv_next}")
            print(f"   ğŸ“¸ ë‹¤ìŒ ì´ë¯¸ì§€ ë²ˆí˜¸: {img_next}")
            
            # ë¶ˆì¼ì¹˜ ê°ì§€
            if abs(csv_next - img_next) > 1:
                print(f"   âš ï¸ ì—°ì†ì„± ë¶ˆì¼ì¹˜ ê°ì§€: CSV({csv_next}) vs ì´ë¯¸ì§€({img_next})")
                
                # ë³´ì • ë°©ì•ˆ ì œì‹œ
                max_number = max(csv_next, img_next)
                print(f"   ğŸ’¡ ê¶Œì¥ ë‹¤ìŒ ë²ˆí˜¸: {max_number}")
                
                return {
                    'consistent': False,
                    'csv_next': csv_next,
                    'image_next': img_next,
                    'recommended_next': max_number
                }
            else:
                print(f"   âœ… ì—°ì†ì„± ì¼ê´€ì„± ì–‘í˜¸")
                return {
                    'consistent': True,
                    'next_number': max(csv_next, img_next)
                }
                
        except Exception as e:
            print(f"âŒ ì—°ì†ì„± ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            return None

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
pagination_ranking_system = PaginationRankingSystem()
ranking_data_matcher = RankingDataMatcher()
continuity_manager = ContinuityManager()

print("âœ… í˜ì´ì§€ë„¤ì´ì…˜ ìˆœìœ„ ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ!")
print("   ğŸ“„ ê¸°ëŠ¥:")
print("   - collect_urls_with_pagination(): í˜ì´ì§€ë³„ ìˆœìœ„ ì—°ì†ì„± URL ìˆ˜ì§‘")
print("   - get_ranking_info_for_url(): URLë³„ ë­í‚¹ ì •ë³´ ì¡°íšŒ")
print("   - create_ranking_csv_mapping(): ë­í‚¹-CSV ë§¤í•‘ í…Œì´ë¸” ìƒì„±")
print("   - ensure_continuity_consistency(): ì—°ì†ì„± ì¼ê´€ì„± ë³´ì¥")