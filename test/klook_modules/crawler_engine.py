"""
üöÄ Í∑∏Î£π 9-A,9-B: KLOOK ÌÅ¨Î°§Îü¨ ÏóîÏßÑ ÏãúÏä§ÌÖú (Î∂ÑÎ¶¨ Î≤ÑÏ†Ñ)
- Í∑∏Î£π 9-A: ÌïµÏã¨ ÌÅ¨Î°§ÎßÅ ÏóîÏßÑ Î∞è ÏÉÅÌíà Ï†ïÎ≥¥ ÏàòÏßë
- Í∑∏Î£π 9-B: Í≥†Í∏â ÌÅ¨Î°§ÎßÅ Ï†úÏñ¥ Î∞è ÏóêÎü¨ Î≥µÍµ¨ ÏãúÏä§ÌÖú
- ÌÜµÌï©Îêú ÌÅ¨Î°§ÎßÅ ÏõåÌÅ¨ÌîåÎ°úÏö∞ Î∞è ÏÉÅÌÉú Í¥ÄÎ¶¨
"""

import os
import time
import random
import json
from datetime import datetime
import traceback

# config Î™®ÎìàÏóêÏÑú ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÉÅÌÉú import
from .config import CONFIG, get_city_code, get_city_info

# Ï°∞Í±¥Î∂Ä import
try:
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
    SELENIUM_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è SeleniumÏù¥ ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. ÌÅ¨Î°§ÎßÅ ÏóîÏßÑ Í∏∞Îä•Ïù¥ Ï†úÌïúÎê©ÎãàÎã§.")
    SELENIUM_AVAILABLE = False

# Îã§Î•∏ Î™®ÎìàÎì§ import
from .url_manager import is_url_already_processed, mark_url_as_processed, get_unprocessed_urls
from .data_handler import get_image_src_klook, download_and_save_image_klook, save_to_csv_klook, create_product_data_structure
from .system_utils import get_product_name, get_price, get_rating, clean_price, clean_rating

# =============================================================================
# üöÄ Í∑∏Î£π 9-A: ÌïµÏã¨ ÌÅ¨Î°§ÎßÅ ÏóîÏßÑ
# =============================================================================

class KlookCrawlerEngine:
    """KLOOK ÌÅ¨Î°§ÎßÅ ÏóîÏßÑ ÌïµÏã¨ ÌÅ¥ÎûòÏä§"""
    
    def __init__(self, driver):
        self.driver = driver
        self.stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "skip_count": 0,
            "start_time": None,
            "current_city": None
        }
        self.error_log = []
        
    def reset_stats(self, city_name):
        """ÌÜµÍ≥Ñ Ï¥àÍ∏∞Ìôî"""
        self.stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "skip_count": 0,
            "start_time": datetime.now(),
            "current_city": city_name
        }
        self.error_log = []
        
    def final_backup(self, city_name):
        """ÏµúÏ¢Ö Î∞±ÏóÖ Ïã§Ìñâ"""
        try:
            if self.stats["success_count"] > 0:
                print(f"üíæ ÏµúÏ¢Ö Î∞±ÏóÖ Ïã§Ìñâ Ï§ë... (Ï¥ù {self.stats['success_count']}Í∞ú ÏôÑÎ£å)")
                
                from .data_handler import backup_csv_data
                backup_suffix = f"final_{self.stats['success_count']}"
                backup_success = backup_csv_data(city_name, backup_suffix)
                
                # Íµ≠Í∞ÄÎ≥Ñ CSVÎäî save_to_csv_klookÏóêÏÑú ÏûêÎèôÏúºÎ°ú Ï≤òÎ¶¨Îê® (ÏõêÎ≥∏ ÎÖ∏Ìä∏Î∂ÅÍ≥º ÎèôÏùº)
                print(f"üåè '{city_name}' ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å - Íµ≠Í∞ÄÎ≥Ñ CSVÎäî Í∞Å ÏÉÅÌíà Ï†ÄÏû• Ïãú ÏûêÎèô ÏÉùÏÑ±Îê®")
                
                if backup_success:
                    print(f"‚úÖ ÏµúÏ¢Ö Î∞±ÏóÖ ÏôÑÎ£å (Ï¥ù {self.stats['success_count']}Í∞ú)")
                    return True
                else:
                    print(f"‚ö†Ô∏è ÏµúÏ¢Ö Î∞±ÏóÖ Ïã§Ìå®")
                    return False
            else:
                print(f"‚ÑπÔ∏è Î∞±ÏóÖÌï† Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå (0Í∞ú ÏôÑÎ£å)")
                return True
                
        except Exception as e:
            print(f"‚ùå ÏµúÏ¢Ö Î∞±ÏóÖ Ïã§Ìå®: {e}")
            return False
        
    def process_single_url(self, url, city_name, product_number):
        """Îã®Ïùº URL Ï≤òÎ¶¨ (ÌïµÏã¨ ÌÅ¨Î°§ÎßÅ Î°úÏßÅ)"""
        if not SELENIUM_AVAILABLE:
            return {"success": False, "error": "Selenium not available"}
        
        print(f"üîÑ ÏÉÅÌíà {product_number}: URL Ï≤òÎ¶¨ Ï§ë...")
        print(f"   üîó {url}")
        
        try:
            # 1. URL Ï§ëÎ≥µ Ï≤¥ÌÅ¨ (Í∏∞Ï°¥ ÏãúÏä§ÌÖú + Îû≠ÌÇπ Îß§ÎãàÏ†Ä)
            if is_url_already_processed(url, city_name):
                print(f"   ‚è≠Ô∏è Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú URL - Ïä§ÌÇµ")
                self.stats["skip_count"] += 1
                return {"success": True, "skipped": True, "reason": "already_processed"}
            
            # Îû≠ÌÇπ Îß§ÎãàÏ†ÄÏóêÏÑú Ï§ëÎ≥µ URL ÌÅ¨Î°§ÎßÅ Ïó¨Î∂Ä ÌôïÏù∏
            try:
                from .ranking_manager import ranking_manager
                if not ranking_manager.should_crawl_url(url, city_name):
                    print(f"   ‚è≠Ô∏è Îû≠ÌÇπ Îß§ÎãàÏ†Ä: Ï§ëÎ≥µ URL Ïä§ÌÇµ (Îã§Î•∏ ÌÉ≠ÏóêÏÑú Ïù¥ÎØ∏ ÌÅ¨Î°§ÎßÅ)")
                    self.stats["skip_count"] += 1
                    return {"success": True, "skipped": True, "reason": "duplicate_in_ranking"}
            except Exception as e:
                print(f"   ‚ö†Ô∏è Îû≠ÌÇπ Îß§ÎãàÏ†Ä ÌôïÏù∏ Ïã§Ìå®: {e}")
            
            # 2. ÌéòÏù¥ÏßÄ Ïù¥Îèô
            self.driver.get(url)
            
            # 3. Ïä§ÎßàÌä∏ ÌéòÏù¥ÏßÄ Î°úÎî© ÎåÄÍ∏∞
            self._smart_page_wait()
            
            # 4. ÌéòÏù¥ÏßÄ Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨
            if not self._validate_page():
                print(f"   ‚ùå ÌéòÏù¥ÏßÄ Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨ Ïã§Ìå®")
                self.stats["error_count"] += 1
                return {"success": False, "error": "invalid_page"}
            
            # 4.5. ÏûêÎèô Ïä§ÌÅ¨Î°§ Ïã§Ìñâ (Í≥†Í∏â Ìå®ÌÑ¥ Ï†ÅÏö©)
            self._apply_advanced_scroll()
            
            # 5. ÏÉÅÌíà Ï†ïÎ≥¥ ÏàòÏßë
            product_data = self._extract_product_info(url, city_name, product_number)
            
            if not product_data:
                print(f"   ‚ùå ÏÉÅÌíà Ï†ïÎ≥¥ ÏàòÏßë Ïã§Ìå®")
                self.stats["error_count"] += 1
                return {"success": False, "error": "extraction_failed"}
            
            # 6. Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
            save_success = save_to_csv_klook(product_data, city_name)
            
            # 6.5. ÏûêÎèô Î∞±ÏóÖ (ÏùºÏ†ï Ï£ºÍ∏∞ÎßàÎã§)
            self._check_auto_backup(city_name)
            
            # 7. URL Ï≤òÎ¶¨ ÏôÑÎ£å ÌëúÏãú (ÏàúÏúÑ Ï†ïÎ≥¥ Ìè¨Ìï®)
            mark_url_as_processed(url, city_name, product_number, product_number)
            
            # Îû≠ÌÇπ Îß§ÎãàÏ†ÄÏóê ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å ÌëúÏãú
            try:
                from .ranking_manager import ranking_manager
                ranking_manager.mark_url_crawled(url, city_name)
            except Exception as e:
                print(f"   ‚ö†Ô∏è Îû≠ÌÇπ Îß§ÎãàÏ†Ä ÏôÑÎ£å ÌëúÏãú Ïã§Ìå®: {e}")
            
            if save_success:
                print(f"   ‚úÖ ÏÉÅÌíà {product_number} Ï≤òÎ¶¨ ÏôÑÎ£å")
                self.stats["success_count"] += 1
                return {
                    "success": True,
                    "product_data": product_data,
                    "product_number": product_number
                }
            else:
                print(f"   ‚ö†Ô∏è Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Ïã§Ìå®")
                self.stats["error_count"] += 1
                return {"success": False, "error": "save_failed"}
                
        except Exception as e:
            error_info = {
                "url": url,
                "error": str(e),
                "traceback": traceback.format_exc(),
                "timestamp": datetime.now().isoformat()
            }
            self.error_log.append(error_info)
            
            print(f"   ‚ùå Ï≤òÎ¶¨ Ïã§Ìå®: {type(e).__name__}: {e}")
            self.stats["error_count"] += 1
            return {"success": False, "error": str(e)}
    
    def _validate_page(self):
        """ÌéòÏù¥ÏßÄ Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨"""
        try:
            # ÌéòÏù¥ÏßÄ Ï†úÎ™© ÌôïÏù∏
            page_title = self.driver.title
            if not page_title or "error" in page_title.lower() or "404" in page_title:
                return False
            
            # KLOOK ÌôúÎèô ÌéòÏù¥ÏßÄÏù∏ÏßÄ ÌôïÏù∏
            activity_indicators = [
                "#activity_title",
                ".activity-title",
                "[data-testid='activity-title']",
                ".product-title"
            ]
            
            for indicator in activity_indicators:
                try:
                    element = self.driver.find_element(By.CSS_SELECTOR, indicator)
                    if element and element.is_displayed():
                        return True
                except:
                    continue
            
            return False
            
        except Exception:
            return False
    
    def _apply_advanced_scroll(self):
        """Í≥†Í∏â Ïä§ÌÅ¨Î°§ Ìå®ÌÑ¥ Ï†ÅÏö© (10Í∞ÄÏßÄ Ìå®ÌÑ¥ Ï§ë ÎûúÎç§ ÏÑ†ÌÉù)"""
        if not SELENIUM_AVAILABLE:
            return
        
        try:
            print(f"  üåÄ Í≥†Í∏â Ïä§ÌÅ¨Î°§ Ìå®ÌÑ¥ Ï†ÅÏö© Ï§ë...")
            
            # url_collectionÏùò Í≥†Í∏â Ïä§ÌÅ¨Î°§ ÏãúÏä§ÌÖú ÏÇ¨Ïö©
            from .url_collection import smart_scroll_selector
            smart_scroll_selector(self.driver)
            
            # ÌéòÏù¥ÏßÄ ÏÉÅÎã®ÏúºÎ°ú Î∂ÄÎìúÎüΩÍ≤å Î≥µÍ∑Ä
            self.driver.execute_script("window.scrollTo({top: 0, behavior: 'smooth'});")
            time.sleep(1)
            
            print(f"    ‚úÖ Í≥†Í∏â Ïä§ÌÅ¨Î°§ Ìå®ÌÑ¥ ÏôÑÎ£å (ÌÉêÏßÄ Î∞©ÏßÄ Í∞ïÌôî)")
            
        except Exception as e:
            print(f"    ‚ö†Ô∏è Í≥†Í∏â Ïä§ÌÅ¨Î°§ Ïã§Ìñâ Ïã§Ìå®: {e}")
            # Ìè¥Î∞±: Í∏∞Î≥∏ Ïä§ÌÅ¨Î°§ Ïã§Ìñâ
            try:
                self.driver.execute_script("window.scrollBy(0, 500);")
                time.sleep(2)
                self.driver.execute_script("window.scrollTo({top: 0, behavior: 'smooth'});")
                time.sleep(1)
                print(f"    ‚úÖ Ìè¥Î∞± Ïä§ÌÅ¨Î°§ ÏôÑÎ£å")
            except Exception as fallback_e:
                print(f"    ‚ùå Ìè¥Î∞± Ïä§ÌÅ¨Î°§ÎèÑ Ïã§Ìå®: {fallback_e}")
    
    def _smart_page_wait(self):
        """Ïä§ÎßàÌä∏ ÌéòÏù¥ÏßÄ ÎåÄÍ∏∞ (ÎèôÏ†Å Î°úÎî© Í∞êÏßÄ)"""
        if not SELENIUM_AVAILABLE:
            return
        
        try:
            print(f"  ‚è±Ô∏è Ïä§ÎßàÌä∏ ÌéòÏù¥ÏßÄ ÎåÄÍ∏∞ Ï§ë...")
            
            # url_collectionÏùò Í≥†Í∏â ÎåÄÍ∏∞ ÏãúÏä§ÌÖú ÏÇ¨Ïö©
            from .url_collection import wait_for_page_ready, smart_wait_for_page_load
            
            # ÌéòÏù¥ÏßÄ Ï§ÄÎπÑ ÏôÑÎ£å ÎåÄÍ∏∞ (jQuery, DOM ÏôÑÎ£å)
            wait_for_page_ready(self.driver, timeout=8)
            
            # Ï∂îÍ∞Ä Î°úÎìú ÎåÄÍ∏∞ (ÎèôÏ†Å Ïª®ÌÖêÏ∏†)
            smart_wait_for_page_load(self.driver, max_wait=6)
            
            print(f"    ‚úÖ Ïä§ÎßàÌä∏ ÎåÄÍ∏∞ ÏôÑÎ£å")
            
        except Exception as e:
            print(f"    ‚ö†Ô∏è Ïä§ÎßàÌä∏ ÎåÄÍ∏∞ Ïã§Ìå®: {e}")
            # Ìè¥Î∞±: Í∏∞Î≥∏ ÎåÄÍ∏∞
            try:
                wait_time = random.uniform(2, 4)
                time.sleep(wait_time)
                print(f"    ‚úÖ Ìè¥Î∞± ÎåÄÍ∏∞ ÏôÑÎ£å ({wait_time:.1f}Ï¥à)")
            except Exception as fallback_e:
                print(f"    ‚ùå Ìè¥Î∞± ÎåÄÍ∏∞ÎèÑ Ïã§Ìå®: {fallback_e}")
    
    def _check_auto_backup(self, city_name):
        """ÏûêÎèô Î∞±ÏóÖ Ï≤¥ÌÅ¨ (20Í∞úÎßàÎã§ Ïã§Ìñâ)"""
        try:
            # 20Í∞úÎßàÎã§ Î∞±ÏóÖ Ïã§Ìñâ (ÏûêÏ£º Î∞±ÏóÖÌïòÏó¨ Îç∞Ïù¥ÌÑ∞ ÏïàÏ†ÑÏÑ± ÌôïÎ≥¥)
            if self.stats["success_count"] > 0 and self.stats["success_count"] % 20 == 0:
                print(f"  üíæ ÏûêÎèô Î∞±ÏóÖ Ïã§Ìñâ Ï§ë... ({self.stats['success_count']}Í∞ú ÏôÑÎ£å)")
                
                from .data_handler import backup_csv_data
                backup_suffix = f"auto_{self.stats['success_count']}"
                backup_success = backup_csv_data(city_name, backup_suffix)
                
                if backup_success:
                    print(f"  ‚úÖ ÏûêÎèô Î∞±ÏóÖ ÏôÑÎ£å (ÏßÑÌñâÎ•†: {self.stats['success_count']}Í∞ú)")
                else:
                    print(f"  ‚ö†Ô∏è ÏûêÎèô Î∞±ÏóÖ Ïã§Ìå®")
                    
        except Exception as e:
            print(f"  ‚ö†Ô∏è ÏûêÎèô Î∞±ÏóÖ Ï≤¥ÌÅ¨ Ïã§Ìå®: {e}")
    
    def _extract_product_info(self, url, city_name, product_number):
        """ÏÉÅÌíà Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        try:
            print(f"  üìä ÏÉÅÌíà Ï†ïÎ≥¥ ÏàòÏßë Ï§ë...")
            
            # 1. Í∏∞Î≥∏ Ï†ïÎ≥¥ ÏàòÏßë
            product_name = get_product_name(self.driver, "Product")
            price = get_price(self.driver)
            rating = get_rating(self.driver)
            
            # 2. Îç∞Ïù¥ÌÑ∞ Ï†ïÏ†ú
            clean_price_value = clean_price(price)
            clean_rating_value = clean_rating(rating)
            
            # 3. Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ (ÎìÄÏñº Ïù¥ÎØ∏ÏßÄ ÏãúÏä§ÌÖú)
            image_filename = None
            dual_images = None
            if CONFIG.get("SAVE_IMAGES", False):
                try:
                    # Î®ºÏ†Ä ÎìÄÏñº Ïù¥ÎØ∏ÏßÄ ÏãúÏä§ÌÖú ÏãúÎèÑ
                    from .data_handler import get_dual_image_urls_klook, download_dual_images_klook
                    
                    image_urls = get_dual_image_urls_klook(self.driver, "Product")
                    if image_urls and image_urls.get("main"):
                        dual_images = download_dual_images_klook(
                            image_urls, product_number, city_name
                        )
                        print(f"    ‚úÖ ÎìÄÏñº Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨: Î©îÏù∏={bool(dual_images.get('main'))}, Ïç∏ÎÑ§Ïùº={bool(dual_images.get('thumb'))}")
                    else:
                        # Ìè¥Î∞±: Í∏∞Ï°¥ Îã®Ïùº Ïù¥ÎØ∏ÏßÄ ÏãúÏä§ÌÖú
                        img_src = get_image_src_klook(self.driver, "Product") 
                        image_filename = download_and_save_image_klook(
                            img_src, product_number, city_name
                        )
                        print(f"    ‚úÖ Îã®Ïùº Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨: {image_filename}")
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
                    image_filename = None
                    dual_images = None
            
            # 4. Ï∂îÍ∞Ä Ï†ïÎ≥¥ ÏàòÏßë (ÏÑ†ÌÉùÏ†Å)
            additional_data = self._extract_additional_info()
            
            # ÏõêÎ≥∏ Í∞ÄÍ≤©Í≥º ÌèâÏ†ê Ï†ïÎ≥¥ Ï∂îÍ∞Ä
            if price and price != clean_price_value:
                additional_data["Í∞ÄÍ≤©_ÏõêÎ≥∏"] = price
            if rating and rating != clean_rating_value:
                additional_data["ÌèâÏ†ê_ÏõêÎ≥∏"] = rating
            
            # URL Ìï¥Ïãú ÏÉùÏÑ±
            import hashlib
            url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()
            additional_data["URL_Ìï¥Ïãú"] = url_hash
            
            # 5. Îû≠ÌÇπ Ï†ïÎ≥¥ ÏàòÏßë
            tab_info = {}
            try:
                from .ranking_manager import ranking_manager
                url_rankings = ranking_manager.get_url_rankings(url, city_name)
                if url_rankings and url_rankings.get("tab_rankings"):
                    # Í∞ÄÏû• ÎÜíÏùÄ Îû≠ÌÇπ(ÏûëÏùÄ Ïà´Ïûê)ÏùÑ Í∞ÄÏßÑ ÌÉ≠ Ï†ïÎ≥¥ ÏÇ¨Ïö©
                    best_tab = min(url_rankings["tab_rankings"].items(), 
                                 key=lambda x: x[1]["ranking"])
                    tab_info = {
                        "tab_name": best_tab[0],
                        "actual_ranking": best_tab[1]["ranking"],
                        "ranking": best_tab[1]["ranking"],
                        "tab_order": 1,  # Í∏∞Î≥∏Í∞í
                        "is_duplicate": url_rankings.get("is_duplicate", False)
                    }
            except Exception as e:
                print(f"    ‚ö†Ô∏è Îû≠ÌÇπ Ï†ïÎ≥¥ ÏàòÏßë Ïã§Ìå®: {e}")
            
            # 6. Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ ÏÉùÏÑ± (Í∏∞Ï°¥ 32Í∞ú Ïª¨Îüº Íµ¨Ï°∞)
            product_data = create_product_data_structure(
                product_number=product_number,
                product_name=product_name,
                price=clean_price_value,
                image_filename=image_filename,
                url=url,
                city_name=city_name,
                additional_data=additional_data,
                dual_images=dual_images,
                tab_info=tab_info
            )
            
            print(f"    ‚úÖ Ï†ïÎ≥¥ ÏàòÏßë ÏôÑÎ£å: {product_name[:30]}...")
            return product_data
            
        except Exception as e:
            print(f"    ‚ùå Ï†ïÎ≥¥ Ï∂îÏ∂ú Ïã§Ìå®: {e}")
            return None
    
    def _extract_additional_info(self):
        """Ï∂îÍ∞Ä Ï†ïÎ≥¥ Ï∂îÏ∂ú (Ïπ¥ÌÖåÍ≥†Î¶¨, ÌÉúÍ∑∏ Îì±)"""
        additional_data = {}
        
        try:
            # Ïπ¥ÌÖåÍ≥†Î¶¨ Ï†ïÎ≥¥
            category_selectors = [
                ".breadcrumb a",
                "[data-testid='breadcrumb'] a",
                ".category-tag",
                ".tag"
            ]
            
            categories = []
            for selector in category_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        text = element.text.strip()
                        if text and text not in categories:
                            categories.append(text)
                except:
                    continue
            
            if categories:
                additional_data["Ïπ¥ÌÖåÍ≥†Î¶¨"] = " > ".join(categories[:3])  # ÏÉÅÏúÑ 3Í∞úÎßå
            
            # ÏúÑÏπò Ï†ïÎ≥¥ (ÎîîÎ≤ÑÍπÖ Î°úÍ∑∏ Ï∂îÍ∞Ä)
            print(f"    üîç ÏúÑÏπò Ï†ïÎ≥¥ Í≤ÄÏÉâ Ï§ë...")
            try:
                location_selectors = [
                    "[data-testid='location']",
                    ".location",
                    ".address",
                    ".location-info",
                    "[class*='location']",
                    "[class*='address']"
                ]
                
                location_found = False
                for selector in location_selectors:
                    try:
                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        print(f"      üìç ÏÖÄÎ†âÌÑ∞ '{selector}': {len(elements)}Í∞ú ÏöîÏÜå Î∞úÍ≤¨")
                        
                        for i, element in enumerate(elements):
                            location = element.text.strip()
                            print(f"        - ÏöîÏÜå {i+1}: '{location}'")
                            if location and len(location) > 2:
                                additional_data["ÏúÑÏπò"] = location
                                print(f"      ‚úÖ ÏúÑÏπò Ï†ïÎ≥¥ ÏàòÏßë ÏÑ±Í≥µ: '{location}'")
                                location_found = True
                                break
                        
                        if location_found:
                            break
                    except Exception as e:
                        print(f"      ‚ùå ÏÖÄÎ†âÌÑ∞ '{selector}' Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
                        continue
                
                if not location_found:
                    print(f"      ‚ö†Ô∏è ÏúÑÏπò Ï†ïÎ≥¥Î•º Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§")
            except Exception as e:
                print(f"    ‚ùå ÏúÑÏπò Ï†ïÎ≥¥ Í≤ÄÏÉâ Ï†ÑÏ≤¥ Ïã§Ìå®: {e}")
                pass
            
            # ÌïòÏù¥ÎùºÏù¥Ìä∏ Ï†ïÎ≥¥
            try:
                highlight_selectors = [
                    ".highlight",
                    ".description",
                    ".product-description",
                    "[data-testid='description']",
                    ".activity-highlights",
                    ".tour-highlights"
                ]
                
                for selector in highlight_selectors:
                    try:
                        element = self.driver.find_element(By.CSS_SELECTOR, selector)
                        highlight = element.text.strip()
                        if highlight and len(highlight) > 10:
                            # Í∏¥ ÌïòÏù¥ÎùºÏù¥Ìä∏ ÌÖçÏä§Ìä∏Îäî Ï§ÑÏûÑ
                            if len(highlight) > 200:
                                highlight = highlight[:200] + "..."
                            additional_data["ÌïòÏù¥ÎùºÏù¥Ìä∏"] = highlight
                            break
                    except:
                        continue
            except:
                pass
            
            # Î¶¨Î∑∞ Ïàò
            try:
                review_selectors = [
                    "[data-testid='review-count']",
                    ".review-count",
                    "[class*='review'][class*='count']"
                ]
                
                for selector in review_selectors:
                    try:
                        element = self.driver.find_element(By.CSS_SELECTOR, selector)
                        review_text = element.text.strip()
                        if review_text and any(char.isdigit() for char in review_text):
                            additional_data["Î¶¨Î∑∞Ïàò"] = review_text
                            break
                    except:
                        continue
            except:
                pass
            
            # Ïñ∏Ïñ¥ Ï†ïÎ≥¥
            try:
                language_selectors = [
                    ".language",
                    ".guide-language",
                    "[data-testid='language']",
                    "[class*='language']",
                    "[class*='lang']"
                ]
                
                for selector in language_selectors:
                    try:
                        element = self.driver.find_element(By.CSS_SELECTOR, selector)
                        language = element.text.strip()
                        if language and len(language) < 50:
                            additional_data["Ïñ∏Ïñ¥"] = language
                            break
                    except:
                        continue
            except:
                pass
                
        except Exception as e:
            print(f"    ‚ö†Ô∏è Ï∂îÍ∞Ä Ï†ïÎ≥¥ ÏàòÏßë Ïã§Ìå®: {e}")
        
        return additional_data
    
    def get_stats_summary(self):
        """ÌÜµÍ≥Ñ ÏöîÏïΩ Î∞òÌôò"""
        if self.stats["start_time"]:
            elapsed = datetime.now() - self.stats["start_time"]
            elapsed_seconds = elapsed.total_seconds()
        else:
            elapsed_seconds = 0
        
        return {
            "city": self.stats["current_city"],
            "total_processed": self.stats["total_processed"],
            "success_count": self.stats["success_count"],
            "error_count": self.stats["error_count"],
            "skip_count": self.stats["skip_count"],
            "success_rate": (self.stats["success_count"] / max(1, self.stats["total_processed"])) * 100,
            "elapsed_time": elapsed_seconds,
            "avg_time_per_url": elapsed_seconds / max(1, self.stats["total_processed"]),
            "error_log_count": len(self.error_log)
        }

# =============================================================================
# üõ°Ô∏è Í∑∏Î£π 9-B: Í≥†Í∏â ÌÅ¨Î°§ÎßÅ Ï†úÏñ¥ Î∞è ÏóêÎü¨ Î≥µÍµ¨
# =============================================================================

class AdvancedCrawlerController:
    """Í≥†Í∏â ÌÅ¨Î°§ÎßÅ Ï†úÏñ¥ ÏãúÏä§ÌÖú"""
    
    def __init__(self, crawler_engine):
        self.engine = crawler_engine
        self.retry_queue = []
        self.failed_urls = []
        
    def process_url_list_with_recovery(self, urls, city_name, max_retries=2):
        """URL Î¶¨Ïä§Ìä∏ Ï≤òÎ¶¨ (ÏóêÎü¨ Î≥µÍµ¨ Ìè¨Ìï®)"""
        print(f"üõ°Ô∏è Í≥†Í∏â ÌÅ¨Î°§ÎßÅ Ï†úÏñ¥ ÏãúÏûë: {len(urls)}Í∞ú URL")
        
        self.engine.reset_stats(city_name)
        
        # 1Ï∞® Ï≤òÎ¶¨
        remaining_urls = self._process_urls_batch(urls, city_name, "1Ï∞®")
        
        # Ïû¨ÏãúÎèÑ Ï≤òÎ¶¨
        retry_count = 0
        while remaining_urls and retry_count < max_retries:
            retry_count += 1
            print(f"\nüîÑ {retry_count}Ï∞® Ïû¨ÏãúÎèÑ: {len(remaining_urls)}Í∞ú URL")
            
            # Ïû¨ÏãúÎèÑ Ï†Ñ ÎåÄÍ∏∞
            wait_time = min(30, retry_count * 10)
            print(f"‚è±Ô∏è Ïû¨ÏãúÎèÑ Ï†Ñ {wait_time}Ï¥à ÎåÄÍ∏∞...")
            time.sleep(wait_time)
            
            remaining_urls = self._process_urls_batch(remaining_urls, city_name, f"{retry_count}Ï∞® Ïû¨ÏãúÎèÑ")
        
        # ÏµúÏ¢Ö Ïã§Ìå® URL Í∏∞Î°ù
        if remaining_urls:
            self.failed_urls.extend(remaining_urls)
            self._save_failed_urls(city_name)
        
        return self.engine.get_stats_summary()
    
    def _process_urls_batch(self, urls, city_name, batch_name):
        """URL Î∞∞Ïπò Ï≤òÎ¶¨"""
        print(f"\nüì¶ {batch_name} Î∞∞Ïπò Ï≤òÎ¶¨ ÏãúÏûë: {len(urls)}Í∞ú URL")
        
        failed_urls = []
        
        for idx, url in enumerate(urls, 1):
            try:
                print(f"\n[{idx}/{len(urls)}] Ï≤òÎ¶¨ Ï§ë...")
                
                self.engine.stats["total_processed"] += 1
                
                result = self.engine.process_single_url(url, city_name, idx)
                
                if not result.get("success", False):
                    failed_urls.append(url)
                    
                    # Ïó∞ÏÜç Ïã§Ìå® Ïãú Í∏¥Í∏â Ï§ëÎã® Ï≤¥ÌÅ¨
                    if self._should_emergency_stop():
                        print("‚ö†Ô∏è Ïó∞ÏÜç Ïã§Ìå®Î°ú Ïù∏Ìïú Í∏¥Í∏â Ï§ëÎã®")
                        failed_urls.extend(urls[idx:])  # ÎÇòÎ®∏ÏßÄ URLÎì§ÎèÑ Ïã§Ìå® Î™©Î°ùÏóê Ï∂îÍ∞Ä
                        break
                
                # Î∞∞Ïπò Í∞Ñ ÎåÄÍ∏∞
                if idx % 10 == 0:  # 10Í∞úÎßàÎã§ ÏßßÏùÄ Ìú¥Ïãù
                    time.sleep(random.uniform(2, 5))
                
                # ÏßÑÌñâÎ•† ÌëúÏãú
                if idx % 20 == 0:
                    progress = (idx / len(urls)) * 100
                    print(f"üìä ÏßÑÌñâÎ•†: {progress:.1f}% ({idx}/{len(urls)})")
                    
            except KeyboardInterrupt:
                print("\n‚ö†Ô∏è ÏÇ¨Ïö©ÏûêÍ∞Ä Ï§ëÎã®ÌñàÏäµÎãàÎã§")
                failed_urls.extend(urls[idx-1:])  # ÌòÑÏû¨ Î∞è ÎÇòÎ®∏ÏßÄ URLÎì§ Ïã§Ìå® Î™©Î°ùÏóê Ï∂îÍ∞Ä
                break
                
            except Exception as e:
                print(f"‚ùå ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•ò: {e}")
                failed_urls.append(url)
                continue
        
        print(f"üì¶ {batch_name} ÏôÑÎ£å: Ïã§Ìå® {len(failed_urls)}Í∞ú")
        return failed_urls
    
    def _should_emergency_stop(self):
        """Í∏¥Í∏â Ï§ëÎã® Ïó¨Î∂Ä ÌåêÎã®"""
        stats = self.engine.stats
        
        # ÏµúÏÜå Ï≤òÎ¶¨Îüâ Ï≤¥ÌÅ¨
        if stats["total_processed"] < 5:
            return False
        
        # Ïó∞ÏÜç Ïã§Ìå®Ïú® Ï≤¥ÌÅ¨ (ÏµúÍ∑º 10Í∞ú Ï§ë 8Í∞ú Ïù¥ÏÉÅ Ïã§Ìå®)
        recent_success_rate = stats["success_count"] / stats["total_processed"]
        if recent_success_rate < 0.2:  # ÏÑ±Í≥µÎ•† 20% ÎØ∏Îßå
            return True
        
        return False
    
    def _save_failed_urls(self, city_name):
        """Ïã§Ìå®Ìïú URL Ï†ÄÏû•"""
        if not self.failed_urls:
            return
        
        try:
            failed_dir = "failed_urls"
            os.makedirs(failed_dir, exist_ok=True)
            
            city_code = get_city_code(city_name)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{city_code}_failed_urls_{timestamp}.json"
            filepath = os.path.join(failed_dir, filename)
            
            data = {
                "city_name": city_name,
                "city_code": city_code,
                "failed_at": datetime.now().isoformat(),
                "total_failed": len(self.failed_urls),
                "urls": self.failed_urls,
                "error_log": self.engine.error_log[-10:]  # ÏµúÍ∑º 10Í∞ú ÏóêÎü¨Îßå
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"üíæ Ïã§Ìå® URL Ï†ÄÏû•: {filename} ({len(self.failed_urls)}Í∞ú)")
            
        except Exception as e:
            print(f"‚ùå Ïã§Ìå® URL Ï†ÄÏû• Ïã§Ìå®: {e}")

# =============================================================================
# üéÆ ÌÜµÌï© ÌÅ¨Î°§ÎßÅ ÏãúÏä§ÌÖú
# =============================================================================

def execute_klook_crawling_system(driver, urls, city_name, mode="advanced"):
    """KLOOK ÌÅ¨Î°§ÎßÅ ÏãúÏä§ÌÖú Ïã§Ìñâ"""
    print(f"üöÄ KLOOK ÌÅ¨Î°§ÎßÅ ÏãúÏä§ÌÖú ÏãúÏûë!")
    print(f"üèôÔ∏è ÎèÑÏãú: {city_name}")
    print(f"üîó ÎåÄÏÉÅ URL: {len(urls)}Í∞ú")
    print(f"‚öôÔ∏è Î™®Îìú: {mode}")
    print("=" * 80)
    
    # ÎØ∏Ï≤òÎ¶¨ URLÎßå ÌïÑÌÑ∞ÎßÅ
    unprocessed_urls = get_unprocessed_urls(urls, city_name)
    
    if not unprocessed_urls:
        print("‚ÑπÔ∏è Ï≤òÎ¶¨Ìï† ÏÉàÎ°úÏö¥ URLÏù¥ ÏóÜÏäµÎãàÎã§")
        return {"success": True, "message": "no_new_urls"}
    
    print(f"üìã Ï≤òÎ¶¨ ÎåÄÏÉÅ: {len(unprocessed_urls)}Í∞ú URL (Ï§ëÎ≥µ Ï†úÏô∏ ÌõÑ)")
    
    # ÌÅ¨Î°§ÎßÅ ÏóîÏßÑ Ï¥àÍ∏∞Ìôî
    engine = KlookCrawlerEngine(driver)
    
    if mode == "basic":
        # Í∏∞Î≥∏ Î™®Îìú: Îã®Ïàú ÏàúÏ∞® Ï≤òÎ¶¨
        engine.reset_stats(city_name)
        
        for idx, url in enumerate(unprocessed_urls, 1):
            print(f"\n[{idx}/{len(unprocessed_urls)}] Í∏∞Î≥∏ Ï≤òÎ¶¨...")
            engine.stats["total_processed"] += 1
            engine.process_single_url(url, city_name, idx)
        
        final_stats = engine.get_stats_summary()
        
    elif mode == "advanced":
        # Í≥†Í∏â Î™®Îìú: ÏóêÎü¨ Î≥µÍµ¨ Ìè¨Ìï®
        controller = AdvancedCrawlerController(engine)
        final_stats = controller.process_url_list_with_recovery(unprocessed_urls, city_name)
        
    else:
        print(f"‚ùå Ïïå Ïàò ÏóÜÎäî Î™®Îìú: {mode}")
        return {"success": False, "error": "unknown_mode"}
    
    # ÏµúÏ¢Ö Í≤∞Í≥º
    print(f"\nüéâ === KLOOK ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å ===")
    print(f"üèôÔ∏è ÎèÑÏãú: {final_stats['city']}")
    print(f"üìä Ï≤òÎ¶¨ Í≤∞Í≥º:")
    print(f"   üîó Ï¥ù Ï≤òÎ¶¨: {final_stats['total_processed']}Í∞ú")
    print(f"   ‚úÖ ÏÑ±Í≥µ: {final_stats['success_count']}Í∞ú")
    print(f"   ‚ùå Ïã§Ìå®: {final_stats['error_count']}Í∞ú")
    print(f"   ‚è≠Ô∏è Ïä§ÌÇµ: {final_stats['skip_count']}Í∞ú")
    print(f"   üìà ÏÑ±Í≥µÎ•†: {final_stats['success_rate']:.1f}%")
    print(f"   ‚è±Ô∏è Ï¥ù ÏÜåÏöîÏãúÍ∞Ñ: {final_stats['elapsed_time']:.1f}Ï¥à")
    print(f"   ‚ö° ÌèâÍ∑† Ï≤òÎ¶¨ÏãúÍ∞Ñ: {final_stats['avg_time_per_url']:.1f}Ï¥à/URL")
    
    return {
        "success": True,
        "stats": final_stats,
        "mode": mode,
        "city_name": city_name
    }

def quick_crawl_test(driver, test_urls, city_name, max_test=3):
    """Îπ†Î•∏ ÌÅ¨Î°§ÎßÅ ÌÖåÏä§Ìä∏"""
    print(f"üß™ Îπ†Î•∏ ÌÅ¨Î°§ÎßÅ ÌÖåÏä§Ìä∏: {city_name} ({min(max_test, len(test_urls))}Í∞ú URL)")
    
    engine = KlookCrawlerEngine(driver)
    engine.reset_stats(city_name)
    
    test_results = []
    
    for idx, url in enumerate(test_urls[:max_test], 1):
        print(f"\nüß™ ÌÖåÏä§Ìä∏ {idx}/{min(max_test, len(test_urls))}")
        engine.stats["total_processed"] += 1
        
        result = engine.process_single_url(url, city_name, f"test_{idx}")
        test_results.append({
            "url": url,
            "success": result.get("success", False),
            "error": result.get("error")
        })
    
    stats = engine.get_stats_summary()
    
    print(f"\nüß™ ÌÖåÏä§Ìä∏ ÏôÑÎ£å:")
    print(f"   ‚úÖ ÏÑ±Í≥µ: {stats['success_count']}/{stats['total_processed']}")
    print(f"   üìà ÏÑ±Í≥µÎ•†: {stats['success_rate']:.1f}%")
    
    return {
        "test_results": test_results,
        "stats": stats
    }

print("‚úÖ Í∑∏Î£π 9-A,9-B ÏôÑÎ£å: KLOOK ÌÅ¨Î°§Îü¨ ÏóîÏßÑ ÏãúÏä§ÌÖú!")
print("   üöÄ Í∑∏Î£π 9-A (ÌïµÏã¨ ÏóîÏßÑ):")
print("   - KlookCrawlerEngine: ÌïµÏã¨ ÌÅ¨Î°§ÎßÅ Î°úÏßÅ")
print("   - process_single_url(): Îã®Ïùº URL Ï≤òÎ¶¨")
print("   - _extract_product_info(): ÏÉÅÌíà Ï†ïÎ≥¥ Ï∂îÏ∂ú")
print("   üõ°Ô∏è Í∑∏Î£π 9-B (Í≥†Í∏â Ï†úÏñ¥):")
print("   - AdvancedCrawlerController: ÏóêÎü¨ Î≥µÍµ¨ ÏãúÏä§ÌÖú")
print("   - process_url_list_with_recovery(): Î≥µÍµ¨ Í∏∞Îä• Ìè¨Ìï® Ï≤òÎ¶¨")
print("   üéÆ ÌÜµÌï© ÏãúÏä§ÌÖú:")
print("   - execute_klook_crawling_system(): ÌÜµÌï© ÌÅ¨Î°§ÎßÅ Ïã§Ìñâ")
print("   - quick_crawl_test(): Îπ†Î•∏ ÌÖåÏä§Ìä∏")
print("   ‚öôÔ∏è ÏßÄÏõê Î™®Îìú: basic, advanced")